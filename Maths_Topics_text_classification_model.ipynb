{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNm+QtUWCDNtDDza2J8t+8x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ani98622/Emotions-Classifier-Fine-Tuned-Transformer/blob/main/Maths_Topics_text_classification_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install transformers datasets"
      ],
      "metadata": {
        "id": "56MceKz4uZjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, os\n",
        "import pandas as pd\n",
        "from transformers import pipeline, BertForSequenceClassification, BertTokenizerFast\n",
        "from torch.utils.data import Dataset"
      ],
      "metadata": {
        "id": "dhdZRDwKufnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "device"
      ],
      "metadata": {
        "id": "TYx3jQrUuoZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ROOT_DIR = '/content/raw_text.csv'\n",
        "# ROOT_DIR = '/kaggle/input/ttc4900/'\n",
        "\n",
        "df_org= pd.read_csv(ROOT_DIR,engine='python', skipfooter=1)\n",
        "\n",
        "df_org = df_org.sample(frac=1.0, random_state=42)\n",
        "\n",
        "df_org.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "ZReqbTzru1BH",
        "outputId": "c37d7bce-8eec-467a-fdc3-56908f1ee561"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                  text           label\n",
              "714  Let's say we have three\\nmatrices, A, B, and C...  Linear Algebra\n",
              "605  All right. So let's get started. So we're stil...   Math for Eng.\n",
              "120  I've got a function f and it's\\na mapping from...  Linear Algebra\n",
              "208  - [voiceover] So now that\\nwe've spent some ti...              CS\n",
              "380  [MUSIC] Stanford University. &gt;&gt; Happened...      Algorithms"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-cb514f1c-15a1-496b-b4a6-42e775ae82b0\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>714</th>\n",
              "      <td>Let's say we have three\\nmatrices, A, B, and C...</td>\n",
              "      <td>Linear Algebra</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>605</th>\n",
              "      <td>All right. So let's get started. So we're stil...</td>\n",
              "      <td>Math for Eng.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>120</th>\n",
              "      <td>I've got a function f and it's\\na mapping from...</td>\n",
              "      <td>Linear Algebra</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>208</th>\n",
              "      <td>- [voiceover] So now that\\nwe've spent some ti...</td>\n",
              "      <td>CS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>380</th>\n",
              "      <td>[MUSIC] Stanford University. &amp;gt;&amp;gt; Happened...</td>\n",
              "      <td>Algorithms</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cb514f1c-15a1-496b-b4a6-42e775ae82b0')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-cb514f1c-15a1-496b-b4a6-42e775ae82b0 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-cb514f1c-15a1-496b-b4a6-42e775ae82b0');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-cb490bb9-eeac-48c3-84f5-b49f49e7de7c\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-cb490bb9-eeac-48c3-84f5-b49f49e7de7c')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-cb490bb9-eeac-48c3-84f5-b49f49e7de7c button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_org",
              "summary": "{\n  \"name\": \"df_org\",\n  \"rows\": 859,\n  \"fields\": [\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 858,\n        \"samples\": [\n          \"In the last video I was a little\\nformal in defining what Rn is, and what a vector is,\\nand what vector addition or scalar multiplication is. In this video I want to kind of\\ngo back to basics and just give you a lot of examples. And give you a more tangible\\nsense for what vectors are and how we operate with them. So let me define a couple\\nof vectors here. And I'm going to do, most of my\\nvectors I'm going to do in this video are going\\nto be in R2. And that's because they're\\neasy to draw. Remember R2 is the set\\nof all 2-tuples. Ordered 2-tuples where each of\\nthe numbers, so you know you could have x1, my 1 looks like a\\ncomma, x1 and x2, where each of these are real numbers. So you each of them, x1 is a\\nmember of the reals, and x2 is a member of the reals. And just to give you a sense\\nof what that means, if this right here is my coordinate\\naxes, and I wanted a plot all my x1's, x2's. You know you could view this\\nas the first coordinate. We always imagine that\\nas our x-axis. And then our second coordinate\\nwe plotted on the vertical axis. That traditionally is our\\ny-axis, but we'll just call that the second number\\naxis, whatever. You could visually represent all\\nof R2 by literally every single point on this plane if\\nwe were to continue off to infinity in every direction. That's what R2 is. R1 would just be points\\njust along one of these number lines. That would be R1. So you could immediately\\nsee that R2 is kind of a bigger space. But anyway, I said that I\\nwouldn't be too abstract, that I would show you examples. So let's get some vectors\\ngoing in R2. So let me define my vector a. I'll make it nice and bold. My vector a is equal to,\\nI'll make some numbers up, negative 1, 2. And my vector b, make it nice\\nand bold, let me make that, I don't know, 3, 1. Those are my two vectors. Let's just add them up\\nand see what we get. Just based on my definition\\nof vector addition. I'll just stay in one color\\nfor now so I don't have to keep switching back and forth. So a, nice deep a, plus bolded\\nb is equal to, I just add up each of those terms.\\nNegative 1 plus 3. And then 2 plus 1. That was my definition\\nof vector addition. So that is going to be\\nequal to 2 and 3. Fair enough that just\\ncame out of my definition of vector addition. But how can we represent\\nthis vector? So we already know that if we\\nhave coordinates, you know, if I have the coordinate, and this\\nis just a convention. It's just the way\\nthat we do it. The way we visualize things. If I wanted to plot the\\npoint 1, 1, I go to my coordinate axes. The first point I go along\\nthe horizontal, what we traditionally call our x-axis. And I go 1 in that direction. And then convention is, the\\nsecond point I go 1 in the vertical direction. So the point 1, 1. Oh, sorry, let me\\nbe very clear. This is 2 and 2, so one\\nis right here, and one is right there. So the point 1, 1 would\\nbe right there. That's just the standard\\nconvention. Now our convention for\\nrepresenting vectors are, you might be tempted to say, oh,\\nmaybe I just represent this vector at the point\\nminus 1, 2. And on some level\\nyou can do that. I'll show you in a second. But the convention for vectors\\nis that you can start at any point. Let's say we're dealing with\\ntwo dimensional vectors. You can start at any\\npoint in R2. So let's say that you're\\nstarting at the point x1, and x2. This could be any point in R2. To represent the vector, what\\nwe do is we draw a line from that point to the point x1. And let me call this, let's say\\nthat we wanted to draw a. So x1 minus 1. So this is, I'm representing\\na. So this is, I want to represent\\nthe vector a. x1 minus 1, and then\\nx1 plus 2. Now if that seems confusing to\\nyou, when I draw it, it'll be very obvious. So let's say I just want to\\nstart at the point, let's just say for quirky reasons, I just\\npick a random point here. I just pick a point. That one right there. That's my starting point. So minus 4, 4. Now if I want to represent my\\nvector a, what I just said is that I add the first term\\nin vector a to my first coordinate. So x1 plus minus 1\\nor x1 minus 1. So my new one is going to be,\\nso this is my x1 minus 4. So now it's going to be, let's\\nsee, I'm starting at the point minus 4 comma 4. If I want to represent a, what\\nI do is, I draw an arrow to minus 4 plus this first\\nterm, minus 1. And then 4 plus the\\nsecond term. 4 plus 2. And so this is what? This is minus 5 comma 6. So I go to minus 5 comma 6. So I go to that point right\\nthere and I just draw a line. So my vector will\\nlook like this. I draw a line from\\nthere to there. And I draw an arrow\\nat the end point. So that's one representation\\nof the vector minus 1, 2. Actually let me do it\\na little bit better. Because minus 5 is actually\\nmore, a little closer to right here. Minus 5 comma 6 Is right\\nthere, so I draw my vector like that. But remember this point minus\\n4 comma 4 was an arbitrary place to draw my vector. I could have started\\nat this point here. I could have started at the\\npoint 4 comma 6 and done the same thing. I could have gone minus 1 in\\nthe horizontal direction, that's my movement in the\\nhorizontal direction. And then plus 2 in the\\nvertical direction. So I could have drawn, so minus\\n1 in the horizontal and plus 2 in the vertical\\ngets me right there. So I could have just as easily\\ndrawn my vector like that. These are both interpretations\\nof the same vector a. I should draw them in the\\ncolor of vector a. So vector a was this light\\nblue color right there. So this is vector a. This is vector a. Sometimes there'll\\nbe a little arrow notation over the vector. But either of those vectors. I could draw an infinite\\nnumber of vector a's. I could draw vector a here. I could draw it like that. Vector a, it goes\\nback 1 and up 2. So vector a could\\nbe right there. Similarly vector b. What does vector b do? I could pick some arbitrary\\npoint for vector b. It goes to the right 3, so it\\ngoes to the right 1, 2, 3 and then it goes up 1. So vector b, one representation\\nof vector b, looks like this. Another represention. I can start it right here. I could go to the right 3,\\n1, 2, 3, and then up 1. This would be another\\nrepresentation of my vector b. There's an infinite number of\\nrepresentations of them. But the convention is to often\\nput them in what's called the standard position. And that's to start\\nthem off at 0, 0. So your initial point, let\\nme write this down. Standard position is just to\\nstart the vectors at 0, 0 and then draw them. So vector a in standard\\nposition, I'd start at 0, 0 like that and I would go\\nback 1 and then up 2. So this is vector a in standard\\nposition right there. And then vector b in\\nstandard position. Let me write that. That's a. And then vector b in standard\\nposition is 3, go to the 3 right and then up 1. These are the vectors in\\nstandard position, but any of these other things we drew\\nare just as valid. Now let's see if we can get\\nan interpretation of what happened when we\\nadded a plus b. Well if I draw that vector in\\nstandard position, I just calculated, it's 2, 3. So I go to the right\\n2 and I go up 3. So if I just draw it in\\nstandard position it looks like this. This vector right there. And at first when you look at\\nit, this vector right here is the vector a plus b in\\nstandard position. When you draw it like that,\\nit's not clear what the relationship is when\\nwe added a and b. But to see the relationship what\\nyou do is, you put a and b head to tails. What that means is, you put\\nthe tail end of b to the front end of a. Because remember, all\\nof these are valid representations of b. All of the representations\\nof the vector b. They all have, they're all\\nparallel to each other, but they can start from anywhere. So another equally valid\\nrepresentation of vector b is to start at this point right\\nhere, kind of the end point of vector a in standard position,\\nand then draw vector b starting from there. So you go 3 to the right. So you go 1, 2, 3. And then you go up 1. So vector b could also be\\ndrawn just like that. And then you should\\nsee something interesting had happened. And remember, this vector b\\nrepresentation is not in standard position, but it's just\\nan equally valid way to represent my vector. Now what do you see? When I add a, which is right\\nhere, to b what do I get if I connect the starting point of\\na with the end point of b? I get the addition. I have added the two vectors. And I could have done\\nthat anywhere. I could have started\\nwith a here. And then I could have\\ndone the end point. I could have started b here and\\ngone 3 to the right, 1, 2, 3 and then up 1. And I could have drawn b\\nright there like that. And then if I were to add a plus\\nb, I go to the starting point of a, and then\\nthe end point of b. And that should also\\nbe the visual representation of a plus b. Just to make sure it confirms\\nwith this number, what I did here was I went 2 to\\nthe right, 1, 2 and then I went 3 up. 1, 2, 3 and I got a plus b. Now let's think about\\nwhat happens when we scale our vectors. When we multiply it times\\nsome scalar factor. So let me pick new vectors. Those have gotten monotonous. Let me define vector v. v for vector. Let's say that it is\\nequal to 1, 2. So if I just wanted to draw\\nvector v in standard position, I would just go 1 to\\nthe horizontal and then 2 to the vertical. That's it. That's the vector in\\nstandard position. If I wanted to do it in a non\\nstandard position, I could do it right here. 1 to the right up 2,\\njust like that. Equally valid way of\\ndrawing vector v. Equally valid way of doing it. Now what happens if I\\nmultiply vector v. What if I have, I don't know,\\nwhat if I have 2 times v? 2 times my vector v is now going\\nto be equal to 2 times each of these terms. So it's\\ngoing to be 2 times 1 which is 2, and then 2 times\\n2 which is 4. Now what does 2 times\\nvector v look like? Well let me just start from\\nan arbitrary position. Let me just start\\nright over here. So I'm going to go 2\\nto the right, 1, 2. And I go up 4. 1, 2, 3, 4. So this is what 2 times\\nvector v looks like. This is 2 times my vector v. And if you look at it, it's\\npointing in the exact same direction but now it's\\ntwice as long. And that makes sense because we\\nscaled it by a factor of 2. When you multiply it by a\\nscalar, or you're not changing its direction. Its direction is the exact same\\nthing as it was before. You're just scaling\\nit by that amount. And I could draw\\nthis anywhere. I could have drawn\\nit right here. I could have drawn 2v\\nright on top of v. Then you would have seen it,\\nI don't want to cover it. You would have seen that it\\ngoes, it's exactly, in this case when I draw it in standard position, it's colinear. It's along the same line,\\nit's just twice as far. it's just twice as long\\nbut they have the exact same direction. Now what happens if I were\\nto multiply minus 4 times our vector v? Well then that will be equal\\nto minus 4 times 1, which is minus 4. And then minus 4 times\\n2, which is minus 8. So this is on my new vector. Minus 4, minus 8. This is minus 4 times\\nour vector v. So let's just start at\\nsome arbitrary point. Let's just do it in\\nstandard position. So you go to the right 4. Or you go to the left 4. So so you go to the left\\n4, 1, 2, 3, 4. And then down 8. Looks like that. So this new vector is going\\nto look like this. Let me try and draw a relatively\\nstraight line. There you go. So this is minus 4 times\\nour vector v. I'll draw a little arrow\\non it to make sure you know it's a vector. Now what happened? Well we're kind of in\\nthe same direction. Actually we're in the exact\\nopposite direction. But we're still along the\\nsame line, right? But we're just in the exact\\nopposite direction. And it's this negative right\\nthere that flipped us around. If we just multiplied negative\\n1 times this, we would have just flipped around to\\nright there, right? But we multiplied it\\nby negative 4. So we scaled it by 4, so you\\nmake it 4 times as long, and then it's negative, so\\nthen it flips around. It flips backwards. So now that we have that notion,\\nwe can kind of start understanding the idea of\\nsubtracting vectors. Let me make up 2 new\\nvectors right now. Let's say my vector x, nice and\\nbold x, is equal to, and I'm doing everything in R2, but\\nin the last part of this video I'll make a few examples\\nin R3 or R4. Let's say my vector x\\nis equal to 2, 4. And let's say I have\\na vector y. y, make it nice and bold. And then that is equal to\\nnegative 1, minus 2. And I want to think about\\nthe notion of what x minus y is equal to. Well we can say that this is the\\nsame thing as x plus minus 1 times our vector y. Right? So x plus minus 1 times\\nour vector y. Now we can use our\\ndefinitions. We know how to multiply\\nby a scalar. So we'll say that this\\nis equal to, let me switch colors. I don't like this color. This is equal to our\\nx vector is 2, 4. And then what's minus\\n1 times y? So minus 1 times y is minus\\n1 times minus 1 is 1. And then minus 1 times\\nminus 2 is 2. So x minus y is going to be\\nthese two vectors added to each other, right? I'm just adding the\\nminus of y. This is minus vector y. So this x minus y is going to\\nbe equal to 3 and 3 and 6. So let's see what that looks\\nlike when we visually represent them. Our vector x was 2, 4. So 2, 4 in standard position\\nit looks like this. That's my vector x. And then vector y in standard\\nposition, let me do it in a different color, I'll\\ndo y in green. Vector y is minus 1, minus 2. It looks just like this. And actually I ended up\\ninadvertently doing collinear vectors, but, hey, this\\nis interesting too. So this is vector y. So then what's their\\ndifference? This is 3, 6. So it's the vector 3, 6. So it's this vector. Let me draw it someplace else. If I start here I go 1, 2, 3. And then I go up 6. So then up 6. It's a vector that\\nlooks like this. That's the difference between\\nthe two vectors. So at first you say,\\nthis is x minus y. Hey, how is this the difference\\nof these two? Well if you overlay this. If you just shift this over\\nthis, you could actually just start here and go straight up. And you'll see that it's really\\nthe difference between the end points. You're kind of connecting\\nthe end points. I actually didn't want to\\ndraw collinear vectors. Let me do another example. Although that one's kind\\nof interesting. You often don't see that\\none in a book. Let me to define vector x\\nin this case to be 2, 3. And let me define vector y\\nto be minus 4, minus 2. So what would be x in\\nstandard position? It would be 2, 3. It'd look like that. That is our vector x if we\\nstart at the origin. So this is x. And then what does vector\\ny look like? I'll do y in orange. Minus 4, minus 2. So vector y looks like this. Now what is x minus y? Well you know, we could\\nview this, 2 plus minus 1 times this. We could just say\\n2 minus minus 4. I think you get the idea now. But we just did it the first\\nway the last time because I wanted to go from my basic\\ndefinitions of scalar multiplication. So x minus y is just going to\\nbe equal to 2 plus minus 1 times minus 4, or\\n2 minus minus 4. That's the same thing as\\n2 plus 4, so it's 6. And then it's 3 minus\\nminus 2, so it's 5. Right? So the difference between the\\ntwo is the vector 6, 5. So you could draw it\\nout here again. So you could go, add 6 to 4, go\\nup there, then to 5, you'd go like that. So the vector would look\\nsomething like this. It shouldn't curve like that,\\nso that's x minus y. But if we drew them between,\\nlike in the last example, I showed that you could draw it\\nbetween their two heads. So if you do it here, what\\ndoes it look like? Well if you start at this point\\nright there and you go 6 to the right and then up 5,\\nyou end up right there. So the difference between the\\ntwo vectors, let me make sure I get it, the difference\\nbetween the two vectors looks like that. It looks just like that. Which kind of should make\\nsense intuitively. x minus y. That's the difference between\\nthe two vectors. You can view the difference as,\\nhow do you get from one vector to another\\nvector, right? Like if, you know, let's go\\nback to our kind of second grade world of just scalars. If I say what 7 minus 5 is, and\\nyou say it's equal to 2, well that just tells you that\\n5 plus 2 is equal to 7. Or the difference between\\n5 and 7 is 2. And here you're saying, look the\\ndifference between x and y is this vector right there. It's equal to that vector\\nright there. Or you could say look, if I\\ntake 5 and add 2 I get 7. Or you could say, look, if I\\ntake vector y, and I add vector x minus y, then\\nI get vector x. Now let's do something else\\nthat's interesting. Let's do what y minus\\nx is equal to. y minus x. What is that equal to? Do it in another color\\nright here. Well we'll take minus 4, minus\\n2 which is minus 6. And then you have minus\\n2, minus 3. It's minus 5. So y minus x is going to be,\\nlet's see, if we start here we're going to go down 6. 1, 2, 3, 4, 5, 6. And then back 5. So back 2, 4, 5. So y minus x looks like this. It's really the exact\\nsame vector. Remember, it doesn't matter\\nwhere we start. It's just pointing in the\\nopposite direction. So if we shifted it here. I could draw it right\\non top of this. It would be the exact as x\\nminus y, but just in the opposite direction. Which is just a general\\ngood thing to know. So you can kind of do them as\\nthe negatives of each other. And actually let me make\\nthat point very clear. You know we drew y. Actually let me draw x, x\\nwe could draw as 2, 3. So you go to the right\\n2 and then up 3. I've done this before. This is x in non standard\\nposition. That's x as well. What is negative x? Negative x is minus 2 minus 3. So if I were to start here,\\nI'd go to minus 2, then I'd go minus 3. So minus x would look\\njust like this. Minus x. It looks just like x. It's parallel. It has the same magnitude. It's just pointing in the exact\\nopposite direction. And this is just a good thing\\nto kind of really get seared into your brain is to have an\\nintuition for these things. Now just to kind of finish up\\nthis kind of idea of adding and subtracting vectors. Everything I did so\\nfar was in R2. But I want to show you that\\nwe can generalize them. And we can even generalize them\\nto vector spaces that aren't normally intuitive for\\nus to actually visualize. So let me define a couple\\nof vectors. Let me define vector a to be\\nequal to 0, minus 1, 2, and 3. Let me define vector b to be\\nequal to 4, minus 2, 0, 5. We can do the same addition\\nand subtraction operations with them. It's just it'll be hard\\nto visualize. We can keep them in\\njust vector form. So that it's still useful to\\nthink in four dimensions. So if I were to say 4 times a. This is the vector a\\nminus 2 times b. What is this going\\nto be equal to? This is a vector. What is this going\\nto be equal to? Well we could rewrite this as\\n4 times this whole column vector, 0, minus 1, 2, and 3. Minus 2 times b. Minus 2 times 4,\\nminus 2, 0, 5. And what is this going\\nto be equal to? This term right here, 4 times\\nthis, you're going to get, the pen tablet seems to not work\\nwell there, so I'm going to do it right here. 4 times this, you're going to\\nget 4 times 0, 0, minus 4, 8. 4 times 2 is 8. 4 times 3 is 12. And then minus, I'll do it in\\nyellow, minus 2 times 4 is 8. 2 times minus 2 is minus 4. 2 times 0 is 0. 2 times 5 is 10. This isn't a good part of my\\nboard, so let me just. It doesn't write well\\nright over there. I haven't figured out the\\nproblem, but if I were just right it over here,\\nwhat do we get? With 0 minus 8? Minus 8. Minus 4, minus 4. Minus negative 4. So that's minus 4 plus\\n4, so that's 0. 8 minus 0 is 8. 12 minus, what was this? I can't even read it,\\nwhat it says. Oh, this is a 10. Now you can see it again. Something is very bizarre. 2 times 5 is 10. So it's 12 minus\\n10, so it's 2. So when we take this vector\\nand multiply it by 4, and subtract 2 times this vector,\\nwe just get this vector. And even though you can't\\nrepresent this in kind of an easy kind of graph-able format, this is a useful concept. And we're going to see this\\nlater when we apply some of these vectors to\\nmulti-dimensional spaces.\",\n          \"The following content is\\nprovided under a Creative Commons license. Your support will help MIT\\nOpenCourseWare continue to offer high quality educational\\nresources for free. To make a donation or to view\\nadditional materials from hundreds of MIT courses, visit\\nMIT OpenCourseWare at ocw.mit.edu. PROFESSOR: Today we're going to\\ncontinue our discussion of methods of integration. The method that I'm going to\\ndescribe today handles a whole class of functions of\\nthe following form. You take P (x) / Q (x)\\nand this is known as a rational function. And all that means is that\\nit's a ratio off two polynomials, which are these\\nfunctions P ( x) and Q ( x). We'll handle all such functions\\nby a method which is known as partial fractions. And what this does is, it splits\\nP / Q into what you could call easier pieces. So that's going to be some\\nkind of algebra. And that's what we're going\\nto spend most of our time doing today. I'll start with an example. And all of my examples\\nwill be illustrating more general methods. The example is to integrate the\\nfunction 1 / x - 1 +, say, 3 / x + 2 dx. That's easy to do. It's just, we already\\nknow the answer. It's ln x - 1 + 3 ln x + 3. Plus a constant. So that's done. So now, here's the difficulty\\nthat is going to arise. The difficulty is that I can\\nstart with this function, which is perfectly manageable. And than I can add these\\ntwo functions together. The way I add fractions. So that's getting a common\\ndenominator. And so that gives me x +\\n2 here + 3 ( x - 1). And now if I combine together\\nall of these terms, then altogether I have 4x +\\n2 - 3, that's - 1. And if I multiply out the\\ndenominator that's x ^2 + that 2 turned into a 3, that's\\ninteresting. Hope there aren't too many more\\nof those transformations. Is there another one here? STUDENT: [INAUDIBLE] PROFESSOR: Oh, it happened\\nearlier on. Wow that's an interesting\\nvibration there. OK. Thank you. So, I guess my 3's were\\nspeaking to my 2's. Somewhere in my past.\\nOK, anyway, I think this is now correct. So the problem is\\nthe following. This is the problem with this. This integral was easy. I'm calling it easy, we already\\nknow how to do it. Over here. But now over here,\\nit's disguised. It's the same function, but it's\\nno longer clear how to integrate it. If you're faced with this\\none, you say what am I supposed to do. And we have to get around\\nthat difficulty. And so what we're going\\nto do is we're going to unwind this disguise. So we have the algebra problem\\nthat we have. Oh, wow. There must be something\\nin the water. Impressive. Wow. OK, let's see. Is 2/3 = 3/2? Holy cow. Well that's good. Well, I'll keep you awake\\ntoday with several other transpositions here. So our algebra problem is to\\ndetect the easy pieces which are inside. And the method that we're going\\nto use, the one that we'll emphasize anyway, is one\\nalgebraic trick which is a shortcut, which is called\\nthe cover-up method. But we're going to talk\\nabout even more general things than that. But anyway, this is where\\nwe're headed. Is something called the\\ncover-up method. Alright. So that's our intro. And I'll just have to remember\\nthat 2 is not 3. I'll keep on repeating that. So now here I'm going to\\ndescribe to you how we unwind this disguise. The first step is,\\nwe write down the function we want to integrate. Which was this. And now we have to undo the\\nfirst damage that we did. So the first step is to factor\\nthe denominator. And that factors, we happen to\\nknow the factors, so I'm not going to carry this out. But this can be a rather\\ndifficult step. But we're going to assume\\nthat it's done. For the purposes of\\nillustration here. So I factor the denominator. And now, the second thing that\\nI'm going to do is what I'm going to call the setup here. How I'm going to\\nset things up. And I'll tell you what these\\nthings are more systematically in a second. And the setup is that\\nI want to somehow detect what I did before. And I'm going to write\\nsome unknowns here. What I expect is that this will\\nbreak up into two pieces. One with the denominator x -\\n1, and the other with the denominator x + 2. So now, my third step is going\\nto be to solve for A and B. And then I'm done,\\nif I do that. That's the complete unwinding\\nof this disguise. And this is where the cover-up\\nmethod comes in handy. This is this method that\\nI'm about to describe. Now, you can do the algebra in a\\nclumsy way, or you can do it in a quick way. And we'd like to get efficient\\nabout the algebra involved. And so let me show you\\nwhat the first step in the trick is. We're going to solve for A by\\nmultiplying by (x - 1). Now, notice if you multiply by\\n(x - 1) in that equation 2, what you get is this. You got 4x - 2 / the\\nx - 1's cancel. You get this on the\\nleft-hand side. And on the right-hand side\\nyou get A. The x - 1's cancel again. And then we get this\\nextra term. Which is B/ ( x + 2)( x - 1). Now, the trick here, and we're\\ngoing to get even better trick in just a second. The trick here is that I\\ndidn't try to clear the denominators completely. I was very efficient about\\nthe way I did it. It just cleared one factor. And the result here\\nis very useful. Namely, if I plug in now x =\\n1, this term drops out too. So what I'm going to do now is\\nI'm going to plug in x = 1. And what I get on the left-hand\\nside here is 4 - 1 and 1 + 2, and on the\\nleft-hand side I get A. That's the end. This is my formula for A. A\\nhappens to be equal to 1. And that's, of course,\\nwhat I expect. A had better be 1, because the\\nthing broke up into 1 / x - 1 + 3 / x + 2. So this is the correct answer. There was a question out\\nhere, which I missed. STUDENT: Aren't polynomials\\ndefined as functions with whole powers, or could\\nthey be square roots? PROFESSOR: Are polynomials\\ndefined as functions with whole powers, or can they\\nbe square roots? That's the question. The answer is, they only\\nhave whole powers. So for instance here I only\\nhave the power 1 and 0. Here I have the powers 2, 1\\nand 0 in the denominator. Square roots are no good\\nfor this method. Another question. STUDENT: [INAUDIBLE] PROFESSOR: Why did\\nI say x = 1? The reason why I said x = 1 was\\nthat it works really fast. You can't know this in advance, that's part of the method. It just turns out to be\\nthe best thing to do. The fastest way of getting at\\nthe coefficient A. Now the curious thing, let me\\njust pause for a second before I do it. If I had plugged x = 1 into the\\noriginal equation, I would have gotten nonsense. Because I would've gotten\\n0 in the denominator. And that seems like the most\\nhorrible thing to do. The worst possible thing\\nto do, is to set x = 1. On the other hand, what\\nwe did is a trick. We multiplied by x - 1. And that turned the equation\\ninto this. So now, in disguise,\\nI multiplied by 0. But that turns out\\nto be legitimate. Because really this equation\\nis true for all x except 1. And then instead of taking\\nx = 1, I can really take x tends to 1. That's really what I need. The limit is x goes to one. The equation is still\\nvalid then. So I'm using the worst case, the\\ncase that looks like it's dividing by 0. And it's helping me because it's\\ncancelling out all the information in terms of B. So\\nthe advantage here is this cancellation that occurs\\nin this part. So that's the method. We're going to shorten it much,\\nmuch more in a second. But let me carry it out for the\\nother coefficient as well. So the other coefficient I'm\\ngoing to solve for B, I'm going to multiply by x + 2. And when I do that, I get 4x\\n- 1 / x - 1, that's the left-hand side, the very\\ntop expression there. And then down below I get\\nA/ ( x - 1)( x + 2). And then again the\\nx + 2's cancel. So I get B sitting alone. And now I'm going to\\ndo the same trick. I'm going to set x = - 2. That's the value which\\nis going to knock out this A term here. So that cancels this\\nterm completely. And what we get here all told\\nis - 8 - 1 / - 2 - 1 = B. In other words, B = 3, which\\nwas also what it was supposed to be. B was this number\\n3, right here. Which I'm now not going\\nto change to 2. Because I know that\\nit's not 2. There was a question. STUDENT: [INAUDIBLE] PROFESSOR: All right. Now, this is the method which\\nis called cover-up. But it's really carried out\\nmuch, much faster than this. So I'm going to review the\\nmethod and I'm going to show you what it is in general. So the first step is to factor\\nthe denominator, Q. That's what I labeled 1 over there. That was the factorization of\\nthe denominator up top. The second step is what I'm\\ngoing to call the setup. That's step 2. And that's where I knew what I\\nwas aiming for in advance. And I'm going to have to\\nexplain to you in every instance exactly what this\\nsetup should be. That is, what the unknowns\\nshould be and what target, simplified expression,\\nwe're aiming for. So that's the setup. And then the third step is what\\nI'll now call cover-up. Which is just a very fast way\\nof doing what I did on this last board, which is solving for\\nthe unknown coefficients. So now, let me perform\\nit for you again. Over here. So it's 4x - 1 divided\\nby, so this is to eliminate writing here. Handwriting it makes\\nit much faster. So this part just factoring the\\ndenominator, that was 1, that was step 1. And then step 2, again,\\nis the setup, which is setting it up like this. Alright, that's the setup. And now I claim that without\\nwriting very much, I can figure out what A and B are. Just by staring at this. So now what I'm going to do is\\nI'm just going to think what I did over there. And I'm just going to\\ndo it directly. So let me show you what the\\nmethod consists of visually. I'm going to cover up, that is,\\nknock out this factor, and focus on this number here. And I'm going to plug in the\\nthing that makes the 0, which is x = 1. So I'm plugging in x = 1. To this left-hand side. And what I get is 4 - 1 / 1 +\\n2 = A. Now, that's the same thing I did over there. I just did it by skipping the\\nintermediate algebra step, which is a lot of writing. So the cover-up method\\nreally amounts to the following thing. I'm thinking of multiplying\\nthis over here. It cancels this and it gets\\nrid of everything else. And it just leaves me with\\nA on the right-hand side. And I have to get rid\\nof it on this side. So in other words, by\\neliminating this, I'm isolating a on the\\nright-hand side. So the cover-up is that\\nI'm covering this and getting A out of it. Similarly, I can do the same\\nthing with B. It's focused on the value x = - 2. And b is what I'm getting\\non the right-hand side. And then I have to\\ncover up this. So if I cover up that, then\\nwhat's left over with x = - 2 is again, - 8 - 1 / - 2 - 1. So this is the way the method\\ngets carried out in practice. Writing, essentially,\\nthe least you can. Now, when you get to several\\nvariables, it becomes just way more convenient to do this. So now, let me just review\\nwhen cover-up works. So this cover-up method\\nworks if Q ( x) has distinct linear factors. And, so you need two\\nthings here. It has to factor completely, the\\ndenominator has to factor completely. And the degree of the numerator\\nhas to be strictly less than the degree\\nof the denominator. I'm going to give you\\nan example here. So, for instance, and this tells\\nyou the general pattern of the setup also. Say you had x ^2 + 3x\\n+ 8, let's say. Over (x - 1) ( x - 2)( x + 5). So here I'm going to\\ntell you the setup. The setup is going to be\\nA / (x - 1) + B / (x - 2) + c / x + 5. And it will always break\\nup into something. So however many factors you\\nhave, you'll have to put in a term for each of those. And then you can find\\neach number here by this cover-up method. Now we're done with that. And now we have to go on to the\\nalgebraic complications. So would the first typical\\nalgebraic complication be. It would be repeated roots\\nor repeated factors. Let me get one that doesn't\\ncome out to be extremely ugly here. So this is what we'll\\ncall Example 2. And this is going to work when\\nthe degree, you always need that the degree of the numerator\\nis less than the degree of the denominator. And Q has now repeated\\nlinear factors. So let's see which example\\nI wanted to show you. So let's just give this here. I'll just repeat the\\ndenominator. With an extra factor\\nlike this. Now, the main thing you need\\nto know, since I've already performed the factorization\\nfor you. Already performed Step 1. This is Step 1 here. You have to factor things all\\nthe way, and that's already been done for you. And here's what this setup is. The setup is that it's of\\nthe form A / (x - 1) + B / (x - 1) ^2. We need another term for the\\nsquare here. + C / (x + 2). In general, if you have more\\npowers you just need to keep on putting in those powers. You need one for each\\nof the powers. Why does it have to be squared? OK. Good question. So why in the world\\nam I doing this? Let me just give you one hint\\nas to why I'm doing this. It's very, very much like the\\ndecimal expansion of a number or, say, the base 2 expansion\\nof a number. So, for, example the number 7/16\\nis 0 / 2 + 1 / 2 ^2 + 1/2 ^3 +, is that right? So it's 4/16 + 1/2 ^4. It's this sort of thing. And I'm getting this power\\nand this power. If I have higher powers,\\nI'm going to have to have more and more. So this is what happens\\nwhen I have a 2 ^ 4. I have to represent\\nthings like this. That's what's coming out\\nof this piece with the repetitious here. Of the powers. This is just an analogy. Of what we're doing. Yeah, another question\\nover here. STUDENT: [INAUDIBLE] PROFESSOR: Yes. So this is an example, but it's\\nmeant to represent the general case and I will also\\ngive you a general picture. For sure, once you have the\\nsecond power here, you'll need both the first and the second\\npower mentioned over here. And since there's only a first\\npower over here I only have to mention a first power\\nover there. If this were a 3 here, there\\nwould be one more term which would be the one for x - 1\\n^2 in the denominator. That's what you just said. OK, now, what's different about\\nthis setup is that the cover-up method, although\\nit works, it doesn't work so well. It doesn't work quite as well. The cover-up works for the\\ncoefficients B and C, not A. We'll have a quick method for\\nthe numbers B and C. To figure out what they are. But it will be a little slower\\nto get to A, which we will do last. Let me show you\\nhow it works. First of all, I'm going to do\\nthe ordinary cover-up with C. So for C, I just want\\nto do the same old thing that I did before. I cover up this, and that's\\ngoing to get rid of all the junk except for the C term. So I have to plug x = - 2. And I get x -- sorry, I get (-2\\n) ^2 + 2 in the numerator. And I get (- 2 - 1)^2\\nin the denominator. Remember I'm covering this up. So that's all there is on\\nthe left-hand side. And on the right-hand side all\\nthere is C. Everything else got killed off, because it\\nwas x - 2 times that. That's 0 times all\\nthat other stuff. And the x - 2 over\\nhere canceled. This is the shortcut that I just\\ndescribed, and this is much faster than doing\\nall that arithmetic. And algebra. So all together this\\nis a 6/9, right? So it's C = 6/9, which is 2/3. Now, the other one which is easy\\nto do, I'm going to do by the slow method first.\\nBut you omit a term. The idea is to cover up\\nthe other bad factor. Cover ups, I'll do it both\\nthe way and the slow way. I'll do it the fast way first,\\nand then I'll show you the slow way. The fast way is to\\ncover this up. And then I have to cover\\nup everything else. That gets eliminated. And that includes everything\\nbut B. So I get B on this side. And I get 1 on that side. So that's 1 ^2 + 2 / 1 + 2. So in other words, B = 1. That was pretty fast, so let\\nme show you what arithmetic was hiding behind that. What algebra was hiding\\nbehind it. What I was really\\ndoing is this. In multiplying through by\\nx - 1 ^2, so I got this. So this canceled here, so this\\nC just stands alone. And then I have here C\\n/x + 2 (x - 1) ^2. Notice again, I cleared out\\nthat 1, this term from the denominator and sent it over\\nto the other side. Now, what's happening is that\\nwhen I set x = 1 here, this term is dying. This term is going away, because\\nthere's more powers in the numerator than in\\nthe denominator. This is still 0. And this one is gone also. So all that's left is B. Now, I\\ncannot pull that off with a single power of x - 1. I can't expose the A term. It's the B term that\\nI can expose. Because I can multiply through\\nby this thing squared. If I multiply through by just x\\n- 1, what'll happen here is I won't have canceled\\nthis (x - 1 )^2. It's useless. I still have a 0 in\\nthe denominator. I'll have B / 0 when\\nI plug in x = 1. Which I can't use. Again, the cover-up method is\\ngiving us B and C, not A. Now, for the last term, for A,\\nI'm going to just have to be straightforward about it. And so I'll just suggest\\nfor A, plug in your favorite number. So plug in my favorite number. Which is x = 0. And you won't be able to\\nplug in x = 0 if you've already used it. Here the two numbers we've\\nalready used are x = 1 and x = - 2. But we haven't used x =\\n0 yet, so that's good. I'm going to plug in now x\\n= 0 into the equation. What do I get? I get 0 ^2 + 2 / (- 1) ^2 *\\n2 is equal to, let's see. A is the thing that\\nI don't know. So it's A / - 1 +, B /\\nx - 1 ^2 so B = 1, so that's 1 / (- 1) ^2. And then C was 2/3. 2/3 / x + 2. So that's 0 + 2. Don't give up at this point. This is a lot of algebra. You really have to plug\\nin all these numbers. You make one arithmetic mistake\\nand you're always going to get the wrong answer. This is very arithmetically\\nintensive. However, it does simplify\\nat this point. We have 2 / 2, that's 1. Is equal to - A + 1 + 1/3. So let's see. A on the other side, this\\nbecomes A = 1/3. And that's it. This is the end. We've we've simplified\\nour function. And now it's easy to integrate. Question. Another question. STUDENT: [INAUDIBLE] PROFESSOR: So the question is,\\nif x = 0 has already been used, what do I do? And the answer is, pick\\nsomething else. And you said pick\\na random number. And that's right, except that if\\nyou really picked a random number it would be 4.12567843,\\nwhich would be difficult. What you want to pick is the\\neasiest possible number you can think of. Yeah. STUDENT: [INAUDIBLE] PROFESSOR: If you had, as in\\nthis sort of situation here. More powers. Wouldn't you have\\nmore variables. Very good question. That's absolutely right. This was a 3 by 3 system in\\ndisguise, for these three unknowns, A, B and C. What we\\nstarted with in the previous problem was two variables. It's over here, the variables A\\nand B. And as the degree of the denominator goes up, the\\nnumber of variables goes up. It gets more and more and\\nmore complicated. More and more arithmetically\\nintensive. STUDENT: [INAUDIBLE] PROFESSOR: Well, so. The question is, how would\\nyou solve it if you have two unknowns. That's exactly the point here. This is a system\\nof simultaneous equations for unknowns. And we have little tricks for\\nisolating single variables. Otherwise we're stuck with\\nsolving the whole system. And you'd have to solve the\\nwhole system by elimination, various other tricks. I'll say a little more\\nabout that later. Now, I have to get one\\nstep more complicated with my next example. My next example is going to\\nhave a quadratic factor. So still I'm sticking to the\\ndegree of the polynomial and the numerator is less than the\\ndegree of the polynomial in the denominator. And I'm going to take\\nthe case where Q has a quadratic factor. Let me just again illustrate\\nthis by example. I have here (x - 1) (x^2 + 1). I'll make it about as\\neasy as they come. Now, the setup will be slightly\\ndifferent here. Here's the setup. It's already factored. I've already done as\\nmuch as I can do. I can't factor this x^2 + 1 into\\nlinear factors unless you know about complex numbers. If you know about complex\\nnumbers this method becomes much easier. And it comes back to the\\ncover-up method. Which is the way that the\\ncover-up method was originally conceived by heavy side. But you won't get to\\nthat until 18.03. So we'll wait. This, by the way, is a method\\nwhich is used for integration. But it was invented to do\\nsomething with Laplace transforms and inversion\\nof certain kinds of differential equations. By heavy side. And so it came much later\\nthan integration. But anyway, it's a very\\nconvenient method. So here's the set up\\nwith this one. Again, we want a term for\\nthis (x - 1) factor. And now we're going to also\\nhave a term with the denominator x squared plus 1. But this is the difference. It's now going to be a first\\ndegree polynomial. One degree down from\\nthe quadratic here. So this is what I keep\\non calling the setup, this is number 2. You have to know that in advance\\nbased on the pattern that you see on the\\nleft-hand side. Yes. STUDENT: [INAUDIBLE] PROFESSOR: The question is, if\\nthe degree of the numerator. So in this case, if this were\\ncubed, and this is matching with the denominator, which\\nis total of degree 3. The answer is that this\\ndoes not work. STUDENT: [INAUDIBLE] PROFESSOR: It definitely\\ndoesn't work. And we're going to have to\\ndo something totally different to handle it. Which turns out, fortunately,\\nto be much easier than this. But we'll deal with\\nthat at the end. Keep this in mind. This is an easy way to make a\\nmistake if you start with a higher degree numerator. You'll never get the\\nright answer. So now, so I have\\nmy setup now. And now what can I do? Well, I claim that I can still\\ndo cover-up for A. It's the same idea. I cover this guy up. And if I really multiply by it\\nit would knock everything out but A. So I cover this up\\nand I plug in x = 1. So I get here 1 ^2 / 1 ^2 + 1 =\\nA. In other words, A = 1/2. Again cover-up is pretty\\nfast, as you can see. It's not too bad. Now, at this next stage, I want\\nto find B and C. And the best idea is the slow way. Here, it's not too terrible. But it's just what we're\\ngoing to do. Which is to clear the\\ndenominators completely. So for B and C, just clear\\nthe denominator. That means multiply through\\nby that whole business. Now, when you do that on the\\nleft-hand side you're going to get x ^2. Because I got rid of the\\nwhole denominator. On the right-hand side when I\\nbring this up, the x - 1 will cancel with this. So the a term will\\nbe A ( x ^2 + 1). And the Bx + C term will have\\na remaining factor of x - 1. Because the x ^2 +\\n1 will cancel. Again, the arithmetic here\\nis not too terrible. Now I'm going to do\\nthe following. I'm going to look at\\nthe x ^2 term. On the left-hand side and\\nthe right-hand side. And that will give me one\\nequation for B and C. And then I'm going to do the same thing\\nwith another term. The x^2 term on the left-hand\\nside, the coefficient is 1. It's 1 ( x ^2). On the other side, it's A.\\nremember I actually have A. So I'm going to put it\\nin, it's 1/2. So this is the A term. And so I get 1/2 ( x ^2). And then the only other\\nx^2 is when this Bx multiplies this x. So Bx * x is Bx ^2, so this is\\nthe other coefficient on x ^2 is B. And that forces\\nB to be 1/2. And last of all, I'm going to\\ndo the x^ 0 power term. Or, otherwise known as\\nthe constant term. And on the left-hand side,\\nthe constant term is 0. There is no constant term. On the right-hand side there's\\na constant term, 1/2 * 1. That's 1/2 here. And then there's another\\nconstant term, which is this constant times this - 1. Which is - C. And so\\nthe conclusion here is that C = 1/2. Another question. Yeah. STUDENT: [INAUDIBLE] PROFESSOR: There's also an x^\\n0 power hidden in here. Sorry, an x ^ 1 , that's what\\nyou were asking about, sorry. There's also an x ^ 1 . The only reason why I didn't\\ngo to the x^ 1 is that it turns out with these two\\nI didn't need it. The other thing is that by\\nexperience, I know that the extreme ends of the\\nmultiplication are the easiest ends. And the middle terms have tons\\nof cross terms. And so I don't like the middle term as much\\nbecause it always involves more arithmetic. So I stick to the lowest and\\nthe highest terms if I can. So that was really\\na sneaky thing. I did that without\\nsaying anything. Yes. STUDENT: [INAUDIBLE] PROFESSOR: Another\\ngood question. Could I just set equals 0? Absolutely. In fact, that's equivalent to\\npicking out the x^ 0 term. And you could plug in numbers. If you wanted. That's another way of doing\\nthis besides doing that. So you can also plug\\nin numbers. Can plug in numbers. x = 0. Actually, not x = 1,\\nright? - 1, 2, etc. Not 1 just because we've\\nalready used it. We won't get interesting\\ninformation out. Yes. STUDENT: [INAUDIBLE] PROFESSOR: So the question\\nis, could I have done it this other way. With the polynomial, with\\nthis other one. Yes, absolutely. So in other words what I've\\ntaught you now is two choices which are equally reasonable. The one that I picked was the\\none that was the fastest for this guy and the one that was\\nfastest for this one, but I could've done the other\\nway around. There are a lot of\\nways of solving simultaneous equations. Yeah, another question. STUDENT: [INAUDIBLE] PROFESSOR: The question\\nis the following. So now everybody can understand\\nthe question. If this, instead of being x\\n^2 + 1, this were x^3 + 1. So that's an important\\ncase to understand. That's a case in which\\nthis denominator is not fully factored. So it's an x^3 + 1, you would\\nhave to factor out an x + 1. So that would be a situation\\nlike this, you have an x^3 + 1, but that's (x + 1)( x^2 +\\nx + 1), this kind of thing. If that's the right, there\\nmust be a minus sign in here maybe. OK, something like this. Right? Isn't that what it is? STUDENT: [INAUDIBLE] PROFESSOR: I think it's right. But anyway, the point is that\\nyou have to factor it. And then you have a linear\\nand a quadratic. So you're always going to be\\nfaced eventually with linear factors and quadratic factors. If you have a cubic, that means\\nyou haven't factored sufficiently. So you're still back\\nin Step 1. STUDENT: [INAUDIBLE] PROFESSOR: In the\\nx^3 + 1 case? STUDENT: [INAUDIBLE] PROFESSOR: In the x^3 + 1 case,\\nwe are out of luck until we've completed the\\nfactorization. Once we've completed the\\nfactorization, we're going to have to deal with these two\\nfactors as denominators. So it'll be this plus something\\nover x + 1 + a Bx + C type of thing over\\nthis thing here. That's what's eventually\\ngoing to happen. But hold on to that idea. Let me carry out one\\nmore example here. So I've figured out what\\nall the values are. But I think it's also worth it\\nto remember now that we also have to carry out\\nthe integration. What I've just shown you is that\\nthe integral of x ^2 dx over (x - 1)( x ^2 + 1) is equal\\nto, and I've split up into these pieces. So what are the pieces? The pieces are, 1/2, x -\\n1 + 1/2 x / x ^2 + 1. This is the A term. This is the B term. And then there's the C term. So we'd better remember\\nthat we know how to antidifferentiate\\nthese things. In other words, I want to\\nfinish the problem. The others were pretty easy, so\\nI didn't bother to finish my sentence, but here I want\\nto be careful and have you realize that there's something\\na little more to do. First of all we have the, the\\nfirst one is no problem. That's this. The second one actually\\nis not too bad either. This is, by the advanced\\nguessing method, my favorite method, something like the\\nlogarithm, because that's what's going to appear\\nin the denominator. And then, if you differentiate\\nthis, you're going to get 2x over this. But here we have 1/2. So altogether it's\\n1/4 of this. So I fixed the coefficient\\nhere. And then the last one, you have\\nto think back to some level of memorization here and\\nremember that this is 1/2 the arc tangent. STUDENT: [INAUDIBLE] PROFESSOR: Why did\\nI go to 1/4? Because in disguise, for this\\nguy, I was thinking d / dx of ln (x ^2 + 1) = 2x / x^2 + 1. Because it's the derivative\\nof this divided by itself. This is the derivative\\nof ln u is u ' / u. Ln u' = u' / u. That was what I applied. And what I had was 1/2, so I\\nneed a total of 1/4 to cancel. So 2/4 is 1/2. Now I've got to get you out\\nof one more deep hole. And I'm going to save the\\ngeneral pattern for next time. But I do want to clarify\\none thing. So let's handle this thing. What if the degree of P is\\nbigger than or equal to the degree of Q. That's the thing\\nthat I claimed was easier. And I'm going to describe\\nto you how it's done. Now, in analogy, with numbers\\nyou might call this an improper fraction. That's the thing that should\\necho in your mind when you're thinking about this. And I'm just going to do\\nit by example here. Let's see., I cooked up an\\nexample so that I don't make an arithmetistic mistake\\nalong the way. So there are two or three steps\\nthat I need to explain. So here's an example. The denominator's degree 2,\\nthe numerator is degree 3. It well exceeds, so there's\\na problem here. Our method is not\\ngoing to work. And the first step that I\\nwant to carry out is to reverse Step 1. That is, I don't want the\\nfactorization for what I'm going to do next. I want it multiplied out. That means I have to multiply\\nthrough, so I get x ^2 + x - 2. I'm back to the starting\\nplace here. And now, the next thing that I'm\\ngoing to do is, I'm going to use long division. That's how you convert\\nan improper fraction to a proper fraction. This is something you were\\nsupposed to learn in, I don't know, Grade 4, I know. Grade 3, Grade 4, Grade\\n5, Grade 6, etc. So here's how it works in\\nthe case of polynomials. It's kind of amusing. So we're dividing this\\npolynomial into that one. And so the quotient to first\\norder here is x. That is, that's going to match\\nthe top terms. So I get x^3 + x ^2 - 2x. That's the product. And now I subtract. And it cancels. So we get here - x ^2 + 2x. That's the difference. And now I need to divide\\nthis next term in. And I need a - 1. So - 1 times this is\\n- x ^2 - x + 2. And I subtract. And the x^2's cancel. And here I get + 3x - 2. Now, this thing has a name. This is called the quotient. And this thing also\\nhas a name. This is called the remainder. And now I'll show you how it\\nworks by sticking it into the answer here. The quotient is x - 1. And the remainder is, let's\\nget down there. 3x - 2 / x ^2 + x - 2. So the punchline here\\nis that this thing is easy to integrate. This is easy. And this one, you can use, now\\nyou can use cover-up, The method that we had before. Because the degree of the\\nnumerator is now below the degree of the denominator. It's now first degree and\\nthis is second degree. What you can't do is\\nuse cover-up to start out with here. That will give you\\nthe wrong answer. So that's the end for today,\\nand see you next time.\",\n          \"The following content is\\nprovided under a Creative Commons license. Your support will help\\nMIT OpenCourseWare continue to offer high quality\\neducational resources for free. To make a donation or to\\nview additional materials from hundreds of MIT courses,\\nvisit MIT OpenCourseWare at ocw.mit.edu. PHILIPPE RIGOLLET:\\nWe keep on talking about principal component\\nanalysis, which we essentially introduced as a way to\\nwork with a bunch of data. So the data that's given to\\nus when we want to do PCA is a bunch of vectors X1 to Xn. So they are random vectors. in Rd. And what we mentioned\\nis that we're going to be using linear\\nalgebra-- in particular, the spectral theorem-- that\\nguarantees to us that if I look at the convenience\\nmatrix of this guy, or its empirical\\ncovariance matrix, since they're\\nsymmetric real matrices and they are positive\\nsemidefinite, there exists a diagonalization\\ninto non-negative eigenvalues. And so here, those\\nthings live in Rd, so it's a really large space. And what we want to\\ndo is to map it down into a space that\\nwe can visualize, hopefully a space\\nof size 2 or 3. Or if not, then we're just going\\nto take more and start looking at subspaces altogether. So think of the case where d\\nis large but not larger than n. So let's say, you have a\\nlarge number of points. The question is, is it possible\\nto project those things onto a lower dimensional\\nspace, d prime, which is much less than d-- so\\nthink of d prime equals, say, 2 or 3-- and so that you keep\\nas much information about the cloud of points\\nthat you had originally. So again, the example\\nthat we could have is that X1 to Xn for, say,\\nXi for patient i's recording a bunch of body measurements\\nand maybe blood pressure, some symptoms, et cetera. And then we have a\\ncloud of n patients. And we're trying to\\nvisualize maybe to see if-- If I could see, for\\nexample, that there's two groups of\\npatients, maybe I would know that I have two\\ngroups of different disease or maybe two groups\\nof different patients that respond differently\\nto a particular disease or drug et cetera. So visualizing is\\ngoing to give us quite a bit of insight about\\nwhat the spatial arrangement of those vectors are. And so PCA says, well, here,\\nof course, in this question, one thing that's not defined\\nis what is information. And we said that\\none thing we might want to do when we project\\nis that points do not collide with each other. And so that means we're\\ntrying to find directions, where after I project, the\\npoints are still pretty spread out. And so I can see\\nwhat's going on. And PCA says-- OK,\\nso there's many ways to answer this question. And PCA says, let's just\\nfind a subspace of dimension d prime that keeps as much\\ncovariance structure as possible. And the reason is\\nthat those directions are the ones that maximize\\nthe variance, which is a proxy for the spread. There's many, many\\nways to do this. There's actually a\\nGoogle video that was released maybe last week\\nabout the data visualization team of Google that shows\\nyou something called t-SNE, which is\\nessentially something that tries to do that. It takes points in\\nvery high dimensions and tries to map them\\nin lower dimensions, so that you can\\nactually visualize them. And t-SNE is some\\nalternative to PCA that gives an other definition\\nfor the word information. I'll talk about this towards\\nthe end, how you can actually somewhat automatically\\nextend everything we've said for PCA to an\\ninfinite family of procedures. So how do we do this? Well, the way we do\\nthis is as follows. So remember, given\\nthose guys, we can form something which is\\ncalled S, which is the sample, or the empirical\\ncovariance matrix. And since from\\ncouple of slides ago, we know that S has a\\neigenvalue decomposition, S is equal to PDP transpose,\\nwhere P is orthogonal. So that's where we use our\\nlinear algebra results. So that means that P transpose P\\nis equal to PP transpose, which is the identity. So remember, S is\\na d by d matrix. And so P is also d by d. And d is diagonal. And I'm actually going to take,\\nwithout loss of generality, I'm going to assume that d-- so it's going to be\\ndiagonal-- and I'm going to have something\\nthat looks like lambda 1 to lambda d. Those are called the\\neigenvalues of S. What we know is that lambda\\nj's are non-negative. And actually, what I'm\\ngoing to assume without loss of generalities is lambda 1\\nis larger than lambda 2, which is larger than lambda d. Because in particular,\\nthis decomposition-- the spectrum decomposition--\\nis not entirely unique. I could permute\\nthe columns of P, and I would still have\\nan orthogonal matrix. And to balance that,\\nI would also have to permute the entries of d. So there's as many\\ndecompositions as there are permutations. So there's actually quite a bit. But the bag of\\neigenvalues is unique. The set of\\neigenvalues is unique. The ordering is\\ncertainly not unique. So here, I'm just\\ngoing to pick-- I'm going to nail down one\\nparticular permutation-- actually, maybe two in\\ncase I have equalities. But let's say, I pick\\none that satisfies this. And the reason why I do this\\nis really not very important. It's just to say,\\nI'm going to want to talk about the largest\\nof those eigenvalues. So this is just\\ngoing to be easier for me to say that\\nthis one is lambda 1, rather than say it's lambda 7. So this is just to say that\\nthe largest eigenvalue of S is lambda 1. If I didn't do that, I would\\njust call it maybe lambda max, and you would just know\\nwhich one I'm talking about. So what's happening now\\nis that if I look at d, then it turns out\\nthat if I start-- so if I do P transpose Xi, I am\\nactually projecting my Xi's-- I'm basically changing\\nthe basis for my Xi's. And now, D is the\\nempirical covariance matrix of those guys. So let's check that. So what it means is\\nthat if I look at-- so what I claim is\\nthat P transpose Xi-- that's a new vector, let's\\ncall it Yi, it's also in Rd-- and what I claim is that the\\ncovariance matrix of this guy is actually now this\\ndiagonal matrix, which means in particular that\\nif they were Gaussian, then they would be independent. But I also know now that\\nthere's no correlation across coordinates of Yi. So to prove this, let me assume\\nthat X bar is equal to 0. And the reason why I do\\nthis is because it's just annoying to carry out all\\nthis censuring constantly and I talk about S. So\\nwhen X bar is equal to 0, that implies that S\\nhas a very simple form. It's of the form\\nsum from i equal 1 to n of Xi Xi transpose. So that's my S. But what I want is the S of Y-- So OK, that implies\\nalso that P times X bar, which is equal to P times\\nX bar is also equal to 0. So that means that Y bar-- Y has mean 0, if this is 0. So if I look at the sample\\ncovariance matrix of Y, it's just going to\\nbe something that looks like the sum of the\\nouter products or the Yi Yi transpose. And again, the reason why\\nI make this assumption is so that I don't have to write\\nminus X bar X bar transpose. But you can do it. And it's going to\\nwork exactly the same. So now, I look at this S prime. And so what is this S prime? Well, I'm just going\\nto replace Yi with PXi. So it's the sum from i equal\\n1 to n of PXi PXi transpose, which is equal to the sum from-- sorry there's a 1/n. So it's equal to 1/n\\nsum from i equal 1 to n of PXi Xi\\ntranspose P transpose. Agree? I just said that the transpose\\nof AB is the transpose of B times the transpose of A. And so now, I can\\npush the sum in. P does not depend on i. So this thing here is\\nequal to PS P transpose, because the sum of the Xi Xi\\ntranspose divided by n is S. But what is PS P transpose? Well, we know that\\nS is equal to-- sorry that's P transpose. So this was with a P transpose. I'm sorry, I made an\\nimportant mistake here. So Yi is P transpose Xi. So this is P transpose\\nand P transpose here, which means that\\nthis is P transpose and this is double transpose,\\nwhich is just nothing and that transpose and nothing. So now, I write S\\nas PD P transpose. That's the spectral\\ndecomposition that I had before. That's my eigenvalue\\ndecomposition, which means that now,\\nif I look at S prime, it's P transpose times\\nPD P transpose P. But now, P transpose\\nP is the identity, P transpose P is the identity. So this is actually\\njust equal to D. And again, you can\\ncheck that this also works if you have to center\\nall those guys as you go. But if you think about\\nit, this is the same thing as saying that I just\\nreplaced Xi by Xi minus X bar. And then it's true that Y bar\\nis also P times Xi minus X bar. So now, we have that D is\\nthe empirical covariance matrix of those guys-- the Yi's, which are\\nP transpose Xi's. And so in particular,\\nwhat it means is that if I look at the\\ncovariance of Yj Yk-- So that's the covariance\\nof the j-th coordinate of Y and the k-th coordinate of Y.\\nI'm just not putting an index. But maybe, let's say the\\nfirst one or something like this-- any of\\nthem, their IID. Then what is this covariance? It's actually 0 if j\\nis different from k. And the covariance\\nbetween Yj and Yj, which is just the variance\\nof Yj, is equal to lambda j-- the j-th largest eigenvalue. So the eigenvalues capture the\\nvariance of my observations in this new coordinate system. And they're\\ncompletely orthogonal. So what does that mean? Well, again, remember,\\nif I chop off the head of my Gaussian\\nin multi dimensions, we said that what\\nwe started from was something that\\nlooked like this. And we said, well, there's one\\ndirection that's important, that's this guy, and one\\nimportant that's this guy. When I applied a transformation\\nP transpose, what I'm doing is that I'm realigning this\\nthing with the new axes. Or in a way, rather\\nto be fair, I'm not actually realigning\\nthe ellipses with the axes. I'm really realigning the\\naxes with the ellipses. So really, what I'm doing is\\nI'm saying, after I apply P, I'm just rotating this\\ncoordinate system. So now, it becomes this guy. And now, my ellipses\\nactually completely align. And what happens here is\\nthat this coordinate is independent of that coordinate. And that's what we write\\nhere, if they are Gaussian. I didn't really tell this-- I'm only making statements\\nabout covariances. If they are Gaussians,\\nthose implied statements about independence. So as I said, the\\nvariance now, lambda 1, is actually the variance\\nof P transpose Xi. But if I look now at\\nthe-- so this is a vector, so I need to look at the\\nfirst coordinate of this guy. So it turns out that\\ndoing this is actually the same thing as looking\\nat the variance of what? Well, the first\\ncolumn of P times Xi. So that's the variance of-- I'm going to call it v1\\ntranspose Xi, where P-- So the v1 vd in Rd\\nare eigenvectors. And each vi is\\nassociated to lambda i. So that's what we saw when\\nwe talked about this eigen decomposition a\\ncouple of slides back. That's the one here. So if I call the\\ncolumns of P v1 to vd, this is what's happening. So when I look at lambda\\n1, it's just the variance of Xi inner product with v1. And we made this picture\\nwhen we said, well, let's say v1 is here\\nand then x1 is here. And if vi has a unique\\nnorm, then the inner product between Xi and v1 is just\\nthe length of this guy here. So that's the variance of the\\nXi says the length of Xi-- so this is 0-- that's the\\nlength of Xi when I project it onto the direction\\nthat span by v1. If v1 has length 2, this is\\nreally just twice this length. If vi has length 3,\\nit's three times this. But it turns out that since\\nP satisfies P transpose P is equal to the identity-- that's an orthogonal\\nmatrix, that's right here-- then this is actually\\nsaying the same thing as vj transpose vj, which is\\nreally the norm squared of vj, is equal to 1. And vj transpose vk is equal\\nto 0, if j is different from k. The eigenvectors are\\northogonal to each other. And they're actually\\nall of norm 1. So now, I know that this\\nis indeed a direction. And so when I look\\nat v1 transpose Xi, I'm really measuring\\nexactly this length. And what is this length? It's the length of\\nthe projection of Xi onto this line. That's the line\\nthat's spanned by v1. So if I had a very high\\ndimensional problem and I started to look\\nat the direction v1-- let's say v1 now is\\nnot a eigenvector, it's any direction-- then\\nif I want to do this lower dimensional projection, then\\nI have to understand how those Xi's project onto the\\nline that's spanned by v1, because this is all that I'm\\ngoing to be keeping at the end of the day about Xi's. So what we want is\\nto find the direction where those Xi's,\\nthose projections, have a lot of variance. And we know that the variance\\nof Xi on this direction is actually exactly\\ngiven by lambda 1. Sorry, that's the\\nempirical var-- yeah, I should\\ncall variance hat. That's the empirical variance. Everything is in empirical here. We're talking about the\\nempirical covariance matrix. And so I also have that lambda\\n2 is the empirical variance of when I project Xi onto\\nv2, which is the second one, just for exactly this reason. Any question? So lambda j's are going\\nto be important for us. Lambda j measure the\\nspread of the points when I project them onto a\\nline which is a one dimensional space. And so I'm going to have-- let's\\nsay I want to pick only one, I'm going to have to find the\\none dimensional space that carries the most variance. And I claim that\\nv1 is the one that actually maximizes the spread. So the claim-- so for\\nany direction, u in Rd-- and by direction, I really\\njust mean that the norm of u is equal to 1. I need to play fair-- I'm going to compare myself to\\nother things of lengths one, so I need to play fair and\\nlook at directions of length 1. Now, if I'm interested\\nin the empirical variance of X1 transpose-- sorry, u transpose X1 u\\ntranspose Xn, then this thing is maximized for\\nu equals v1, where v1 is the eigenvector\\nassociated to lambda 1 and lambda 1 is not\\nany eigenvalues, it's the largest of all those. So it's the largest eigenvalue. So why is that true? Well, there's also a claim\\nthat for any direction u-- so that's 1 and 2-- the variance of u\\ntranspose X-- now, this is just a random variable,\\nand I'm looking about the true variance-- this is maximized for u\\nequals, let's call it w1, where w1 is the\\neigenvector of sigma-- Now, I'm talking about\\nthe true variance. Whereas, here, I was talking\\nabout the empirical variance. So the true variance\\nis the eigenvectors of the true sigma\\nassociated to the largest eigenvalue of sigma. So I did not give it a name. Here, that was lambda 1\\nfor the empirical one. For the true one,\\nyou can give it another name, mu 1 if you want. But that's just the same thing. All it's saying is like,\\nwherever I see empirical, I can remove it. So why is this claim true? Well, let's look at the\\nsecond one, for example. So what is the variance\\nof u transpose X? So that's what I want to know. So that's the expectation--\\nso let's assume that X is 0, again, for same\\nreasons as before. So what is the variance? It's just the expectation\\nof the square? I don't need to remove\\nthe expectation. And the expedition\\nof the square is just the expectation\\nof u transpose X. And then I'm going to write\\nthe other one X transpose u. And we know that this\\nis deterministic. So I'm just going to take\\nthat this is just u transpose expectation of X X transpose u. And what is this guy? That's covariance sigma. That's just what sigma is. So the variance I can write\\nas u transpose sigma u. We've made this\\ncomputation before. And now what I want to claim\\nis that this thing is actually less than the largest\\neigenvalue, which I actually called lambda 1 here. I should probably not. And the P is-- well, OK. Let's just pretend\\neverything is not empirical. So now, I'm going to write\\nsigma as P lambda 1 lambda n P transpose. That's just the\\neigendecomposition, where I admittedly reuse the\\nsame notation as I did for S. So I should really put\\nsome primes everywhere, so you know those are\\nthings that are actually different in practice. So this is just that the\\ndecomposition of sigma. You seem confused, Helen. You have a question? Yeah? AUDIENCE: What is-- when you\\ntalked about the empirical data and-- PHILIPPE RIGOLLET: So OK-- so I can make\\neverything I'm saying, I can talk about\\neither the variance or the empirical variance. And you can just add the\\nword empirical in front of it whenever you want. The same thing works. But just for the sake of\\nremoving the confusion, let's just do it again\\nwith S. So I'm just going to do everything\\nwith S. So I'm going to assume that\\nX bar is equal to 0. And here, I'm going to talk\\nabout the empirical variance, which is just 1/n\\nsum from i equal 1 to n of u transpose Xi squared. So it's the same thing. Everywhere you see\\nan expectation, you just put in average. And then I get 1/n\\nsum from i equal 1 to n of Xi Xi transpose. And now, I'm going\\nto call this guy S, because that's what it is. So this is u transpose Su. But just defined that I could\\njust replace the expectation by averages everywhere,\\nyou can tell that the thing is going to work\\nfor either one or the other. So now, this thing\\nwas actually-- so now, I don't have any problem\\nwith my notation. This is actually the\\ndecomposition of S. That's just the\\nspectral decomposition and it's to its eigenvalues. And so now, what I have is that\\nwhen I look at u transpose Su, this is actually equal\\nto P u transpose S Pu. OK. There's a transpose somewhere. That's this guy. And that's this guy. Now-- sorry, that's\\nnot P, that's D. That's D, that's\\nthis diagonal matrix. Let's look at this thing. And let's call P transpose\\nu, let's call it b. So that's also a vector in Rd. What is it? It's just, I take a\\nunit vector, and then I apply P transpose to it. So that's basically what\\nhappens to a unit vector when I apply the same\\nchange of basis that I did. So I'm just changing my\\northogonal system the same way I did for the other ones. So what's happening\\nwhen I write this? Well, now I have that u\\ntranspose Su is b transpose Db. But now, doing b transpose\\nDb when D is diagonal and b is a vector is\\na very simple thing. I can expand it. This is what? This is just the\\nsum from j equal 1 to d of lambda j bj squared. So that's just like matrix\\nvector multiplication. And in particular, I know\\nthat the largest of those guys is lambda 1 and those\\nguys are all non-negative. So this thing is actually\\nless than lambda 1 times the sum from j equal 1 to\\nd of lambda j squared-- sorry, bj squared. And this is just the\\nnorm of b squared. So if I want to prove what's on\\nthe slide, all I need to check is that b has norm, which is-- AUDIENCE: 1. PHILIPPE RIGOLLET: At most, 1. It's going to be at most 1. Why? Well, because b is really\\njust a change of basis for u. And so if I take a vector,\\nI'm just changing its basis. I'm certainly not\\nchanging its length-- think of a rotation,\\nand I can also flip it, but think of a rotation-- well, actually, for vector, it's\\njust going to be a rotation. And so now, what\\nI have I just have to check that the norm of\\nb squared is equal to what? Well, it's equal to the norm\\nof P transpose u squared, which is equal to u\\ntranspose P P transpose u. But P is orthogonal. So this thing is actually\\njust the identity. So that's just u\\ntranspose u, which is equal to the norm u\\nsquared, which is equal to 1, because I took u to have\\nnorm 1 in the first place. And so this-- you're right--\\nwas actually of norm equal to 1. I just needed to have\\nit less, but it's equal. And so what I'm left with is\\nthat this thing is actually equal to lambda 1. So I know that for\\nevery u that I pick-- that has norm-- So I'm just reminding\\nyou that u here has norm squared equal to 1. For every u that I\\npick, this u transpose Su is at mostly lambda 1. So that's the u transpose\\nSu is at most lambda 1. And we know that that's\\nthe variance, that's the empirical variance,\\nwhen I project my points onto direction spanned by u. So now, I have an\\nempirical variance, which is at most lambda 1. But I also know that if I take u\\nto be something very specific-- I mean, it was on\\nthe previous board-- if I take u to be\\nequal to v1, then this thing is actually\\nnot an inequality, this is an equality. And the reason is, when I\\nactually take u to be v1, all of these bj's are going to\\nbe 0, except for the one that's b1, which is itself equal to 1. So I mean, we can\\nbriefly check this. But if I take v-- if u is equal to v1, what\\nI have is that u transpose Su is equal to P transpose\\nv1 D P transpose v1. But what is P transpose v1? Well, remember P\\ntranspose is just the matrix that has\\nvectors v1 transpose here, v2 transpose here, all the\\nway to vd transpose here. And we know that when I take\\nvj transpose vk, I get 0, if j is different from k. And if j is equal to k, I get 1. So P transpose v1\\nis equal to what? Take v1 here and multiply it. So the first coordinate\\nis going to be v1 transpose v1, which is 1. The second coordinate\\nis going to be v2 transpose v1, which is 0. And so I get 0's\\nall the way, right? So that means that this\\nthing here is really just the vector 1, 0, 0. And here, this is just\\nthe vector 1, 0, 0. So when I multiply\\nit with this guy, I am only picking up\\nthe top left element of D, which is lambda 1. So for every one,\\nit's less lambda 1. And for v1, it's\\nequal to lambda 1, which means that it's\\nmaximized for a equals v1. And that's where\\nI said that this is the fanciest non-convex\\nproblem we know how to solve. This was a problem that\\nwas definitely non-convex. We were maximizing a convex\\nfunction over a sphere. But we know that v1,\\nwhich is something-- I mean, of course,\\nyou still have to believe me that\\nyou can compute the spectral decomposition\\nefficiently-- but essentially, if you've\\ntaken linear algebra, you know that you can\\ndiagonalize a matrix. And so you get that v1\\nis just the maximum. So you can find your\\nmaximum just by looking at the spectral decomposition. You don't have to\\ndo any optimization or anything like this. So let's recap. Where are we? We've established\\nthat if I start with my empirical covariance\\nmatrix, I can diagonalize it and PD P transpose. And then if I take the\\neigenvector associated to the largest eigenvalues-- so\\nif I permute the columns of P and of D's in such\\na way that they are ordered from the\\nlargest to the smallest when I look at the diagonal\\nelements of D, then if I pick the first\\ncolumn of P, it's v1. And v1 is the direction on\\nwhich, if I project my points, they are going to carry the\\nmost empirical variance. Well, that's a good way. If I told you,\\npick one direction along which if you were\\nto project your points they would be as spread out\\nas possible, that's probably the one you would pick. And so that's exactly\\nwhat PCA is doing for us. It says, OK, if you ask me\\nto take d prime equal to 1, I will take v1. I will just take the direction\\nthat's spanned by v1. And that's just when I come\\nback to this picture that was here before, this is v1. Of course, here, I\\nonly have two of them. So v2 has to be this\\nguy, or this guy, or I mean or this thing. I mean, I don't know\\nthem up to sine. But then if I have three-- think of like an olive\\nin three dimensions-- then maybe I have one\\ndirection that's slightly more elongated than the other one. And so I'm going to\\npick the second one. And so the procedure is\\nto say, well, first, I'm going to pick v1 the same way\\nI pick v1 in the first place. So the first\\ndirection I am taking is the leading eigenvector. And then I'm looking\\nfor a direction. Well, if I found\\none-- the one I'm going to want to find-- if you\\nsay you can take d equal 2, you're going to need\\nthe basis for this guy. So the second one has to be\\northogonal to the first one you've already picked. And so the second\\none you pick is the one that's just,\\namong all those that are orthogonal to v1, maximized\\nthe empirical variance when you project onto it. And it turns out that this\\nis actually exactly v2. You don't have to\\nredo anything again. You're eigendecomposition,\\nthis is just the second column\\nof P. Clearly, v2 is orthogonal to v1. We just used it here. This 0 here just says this\\nv2 is orthogonal to v1. So they're like this. And now, what I said-- what this slide\\ntells you extra-- is that v2 among all\\nthose directions that are orthogonal-- I mean, there's still\\nd minus 1 of them-- this is the one that\\nmaximizes the, say, residual empirical\\nvariance-- the one that was not explained by the first\\ndirection that you picked. And you can check that. I mean, it's becoming a bit\\nmore cumbersome to write down, but you can check that. If you're not convinced,\\nplease raise your concern. I mean, basically, one\\nway you view this to-- I mean, you're not really\\ndropping a coordinate, because v1 is not a coordinate. But let's assume actually for\\nsimplicity that v1 was actually equal to e1, that the direction\\nthat carries the most variance is the one that\\njust says, just look at the first coordinate of X.\\nSo if that was the case, then clearly the orthogonal\\ndirections are the ones that comprise only\\nof the coordinates 2 to d. So you could actually just\\ndrop the first coordinate and do the same thing on\\na slightly shorter vector of length d minus 1. And then you would just look\\nat the largest eigenvector of these guys, et\\ncetera, et cetera. So in a way, that's\\nwhat's happening, except that you rotate it\\nbefore you actually do this. And that's exactly\\nwhat's happening. So what we put together here\\nis essentially three things. One was statistics. Statistics says, if\\nyou won't spread, if you want information, you\\nshould be looking at variance. The second one was optimization. Optimization said, well, if you\\nwant to maximize spread, well, you have to maximize variance\\nin a certain direction. And that means maximizing\\nover the sphere of vectors that have unique norm. And that's an optimization\\nproblem, which actually turned out to be difficult. But then the third thing that\\nwe use to solve this problem was linear algebra. Linear algebra\\nsaid, well, it looks like it's a difficult\\noptimization problem. But it turns out that the\\nanswer comes in almost-- I mean, it's not a closed form,\\nbut those things are so used, that it's almost a closed form-- says, just pick the\\neigenvectors in order of their associated eigenvalues\\nfrom largest to smallest. And that's why principal\\ncomponent analysis has been so popular and has\\ngained huge amount of traction since we had computers that were\\nallowed to compute eigenvalues and eigenvectors for\\nmatrices of gigantic sizes. You can actually do that. If I give you-- I don't know, this Google\\nvideo, for example, is talking about words. They want to do just the,\\nsay, principal component analysis of words. So I give you all the\\nwords in the dictionary. And-- sorry, well,\\nyou would have to have a representation\\nfor words, so it's a little more\\ndifficult. But how do I do this? Let's say, for example,\\npages of a book. I want to understand\\nthe pages of a book. And I need to turn\\nit into a number. And a page of a book is\\nbasically the word count. So I just count the number\\nof times \\\"the\\\" shows up, the number of times \\\"and\\\"\\nshows up, number of times \\\"dog\\\" shows up. And so that gives me a vector. It's in pretty high dimensions. It's as many dimensions as there\\nare words in the dictionary. And now, I want to visualize\\nhow those pages get together-- are two pages very\\nsimilar or not. And so what you would\\ndo is essentially just compute the largest\\neigenvector of this matrix-- maybe the two largest-- and\\nthen project this into a plane. Yeah. AUDIENCE: Can we assume\\nthe number of points was far larger\\nthan the dimension? PHILIPPE RIGOLLET:\\nYeah, but there's many pages in the world. There's probably more\\npages in the world than there's words\\nin the dictionary. Yeah, so of course, if\\nyou are in high dimensions and you don't have\\nenough points, it's going to be\\nclearly an issue. If you have two points,\\nthen the leading eigenvector is going to be\\njust the line that goes through those\\ntwo points, regardless of what the dimension is. And clearly, you're\\nnot learning anything. So you have to pick,\\nsay, the k largest one. If you go all the way, you're\\njust reordering your thing, and you're not actually\\ngaining anything. You start from d\\nand you go too d. So at some point, this\\nprocedure has to stop. And let's say it stops at k. Now, of course, you\\nshould ask me a question, which is, how do you choose k? So that's, of course,\\na natural question. Probably the basic answer\\nis just pick k equals 3, because you can\\nactually visualize it. But what happens if I\\ntake k is equal to 4? If I take is equal\\nto 4, I'm not going to be able to plot points\\nin four dimensions. Well, I could, I\\ncould add color, or I could try to be a\\nlittle smart about it. But it's actually\\nquite difficult. And so what people tend to do,\\nif you have four dimensions, they actually do a bunch\\nof two dimensional plots. And that's what a computer--\\na computer is not very good-- I mean, by default,\\nthey don't spit out three dimensional plots. So let's say they want to plot\\nonly two dimensional things. So they're going to take the\\nfirst directions of, say, v1, v2. Let's say you have\\nthree, but you want to have only two\\ndimensional plots. And then it's going to do\\nv1, v3; and then v2, v3. So really, you take\\nall three of them, but it's really just\\nshowing you all choices of pairs of those guys. So if you were to\\nkeep k is equal to 5, you would have five,\\nchoose two different plots. So this is the actual\\nprincipal component algorithm, how it's implemented. And it's actually fairly simple. I mean, it looks like\\nthere's lots of steps. But really, there's only\\none that's important. So the first one is the input. I give you a bunch of points,\\nx1 to xn in d dimensions. And step two is, well, compute\\ntheir empirical covariance matrix S. The points themselves,\\nwe don't really care. We care about their\\nempirical covariance matrix. So it's a d by d matrix. Now, I'm going to feed that. And that's where the actual\\ncomputation starts happening. I'm going to feed that\\nto something that knows how to diagonalize this matrix. And you have to\\ntrust me, if I want to compute the k\\nlargest eigenvalues and my matrix is\\nd by d, it's going to take me about k times\\nd squared operations. So if I want only three,\\nit's 3 times d squared, which is about-- d squared is the time for me\\nit takes to just even read the matrix sigma. So that's not too bad. So what it's going to\\nspit out, of course, is the diagonal matrix\\nD. And those are nice, because they allow\\nme to tell me what is the order in which I should\\nbe taking the columns of P. But what's really important\\nto me is v1 to vd, because those are going to be\\nthe ones I'm going to be using to draw those plots. And now, I'm going\\nto say, OK, I need to actually choose some set k. And I'm going to basically\\ntruncate and look only at the first\\nk columns of P. Once I have those\\ncolumns, what I want to do is to project\\nonto the linear span of those columns. And there's actually\\na simple way to do this, which is just take\\nthis matrix P, which is really the matrix of projection onto\\nthe linear span of those k columns. And you just take Pk transpose. And then you apply this to\\nevery single one of your points. Now Pk transpose, what is\\nthe size of the matrix Pk? Yeah, [INAUDIBLE]? AUDIENCE: [INAUDIBLE] PHILIPPE RIGOLLET: So\\nPk is just this matrix. I take the v1 and I stop at vk-- well-- AUDIENCE: [INAUDIBLE] PHILIPPE RIGOLLET:\\nd by k, right? Each of the column\\nis an eigenvector. It's of dimension d. I mean, that's a vector\\nin the original space. So I have this d by k matrix. So all it is is if I had my-- well, I'm going to talk in\\na second about Pk transpose. Pk transpose is\\njust this guy, where I stop at the k-th vector. So Pk transpose is k by d. So now, when I take Yi,\\nwhich is Pk transpose Xi, I end up with a point\\nwhich is in k dimensions. I have only k coordinates. So I took every single one\\nof my original points Xi, which had d coordinates, and\\nI turned it into a point that has only k coordinates. Particularly, I could\\nhave k is equal to 2. This matrix is exactly\\nthe one that projects. If you think about\\nit for one second, this is just the\\nmatrix that says-- well, we actually did\\nthat several times. The matrix, so that\\nwas this P transpose u that showed up somewhere. And so that's just\\nthe matrix that take your point X in,\\nsay, three dimensions, and then just project it\\ndown to two dimensions. And that's just-- it goes to the\\nclosest point in the subspace. Now, here, the floor is flat. But we can pick any\\nsubspace we want, depending on what\\nthe lambdas are. So the lambdas were\\nimportant for us to be able to identify\\nwhich columns to pick. The fact that we assumed\\nthat they were ordered tells us that we can\\npick the first ones. If they were not\\nordered, it would be just a subset of the\\ncolumns, depending on what the size of the eigenvalue is. So each column is labeled. And so then, of course, we\\nstill have this question of, how do I pick k? So there's definitely the\\nmatter of convenience. Maybe 2 is convenient. If it works for 2, you don't\\nhave to go any farther. But you might want\\nto say, well-- originally, I did\\nthat to actually keep as much information as possible. I know that the\\nultimate thing is to keep as much information,\\nwhich would be to k is equal d-- that's as much\\ninformation as you want. But it's essentially the\\nsame question about, well, if I want to compress\\na JPEG image, how much information should\\nI keep so it's still visible? And so there's some\\nrules for that. But none of them is\\nactually really a science. So it's really a\\nmatter of what you think is actually tolerable. And we're just going to start\\nreplacing this choice by maybe another parameter. So here, we're going to\\nbasically replace k by alpha, and so we just do stuff. So the first one that\\npeople do that is probably the most popular one-- OK, the most popular\\none is definitely take k is equal to 2\\nor 3, because it's just convenient to visualize. The second most popular\\none is the scree plot. So the scree plot-- remember, I have my\\nvalues, lambda j's. And I've chosen the\\nlambda j's to decrease. So the indices are\\nchosen in such a way that lambda is a\\ndecreasing function. So I have lambda 1, and\\nlet's say it's this guy here. And then I have lambda 2, and\\nlet's say it's this guy here. And then I have lambda 3, and\\nlet's say it's this guy here, lambda 4, lambda 5, lambda 6. And all I care about is\\nthat this thing decreases. The scree plot says\\nsomething like this-- if there's an inflection point,\\nmeaning that you can sort of do something like this and\\nthen something like this, you should stop at 3. That's what the\\nscree plot tells you. What it's saying in a way\\nis that the percentage of the marginal\\nincrement of explained variance that you get\\nstarts to decrease after you pass this inflection point. So let's see why I way this. Well, here, what I\\nhave-- so this ratio that you see there is\\nactually the percentage of explained variance. So what it means is that, if I\\nlook at lambda 1 plus lambda k, and then I divide by lambda\\n1 plus lambda d, well, what is this? Well, this lambda\\n1 plus lambda d is the total amount of variance\\nthat I get in my points. That's the trace of sigma. So that's the variance\\nin the first direction plus the variance in\\nthe second direction plus the variance in\\nthe third direction. That's basically all the\\nvariance that I have possible. Now, this is the variance that\\nI kept in the first direction. This is the variance that I\\nkept in the second direction, all the way to the variance that\\nI kept in the k-th direction. So I know that this number is\\nalways less than or equal to 1. And it's larger than 1. And this is just\\nthe proportion, say, of variance explained\\nby v1 to vk, or simply, the proportion of\\nexplained variance by my PCA, say. So now, what this\\nthing is telling me, its says, well, if\\nI look at this thing and I start seeing this\\ninflection point, it's saying, oh, here, you're gaining\\na lot and lot of variance. And then at some point,\\nyou stop gaining a lot in your proportion of\\nexplained variance. So this will\\ntranslate in something where when I look at this ratio,\\nlambda 1 plus lambda k divided by lambda 1 plus\\nlambda d, this would translate into a function\\nthat would look like this. And what it's telling you,\\nit says, well, maybe you should stop here, because\\nhere every time you add one, you don't get as much\\nas you did before. You actually get like\\nsmaller marginal returns. So explained variance is\\nthe numerator of this ratio. And the total variance\\nis the denominator. Those are pretty\\nstraightforward terms that you would want\\nto use for this. So if your goal is to\\ndo data visualization-- so why would you\\ntake k larger than 2? Let's say, if you\\ntake k larger than 6, you can start to\\nimagine that you're going to have six, choose two,\\nwhich starts to be annoying. And if you have k\\nis equal to 10-- because you could start\\nin dimension 50,000-- and then k equal to\\n10 would be the place where you have this thing\\nthat's a lot of plots that you would have to show. So it's not always for\\ndata visualization. Once I've actually\\ndone this, I've actually effectively reduced\\nthe dimension of my problem. And what I could do\\nwith what I have is do a regression on those guys. The v1-- so I\\nforgot to tell you-- why is that called principal\\ncomponent analysis? Well, the vj's that\\nI keep, v1 to vk are called principal components. And they effectively act\\nas the summary of my Xi's. When I mentioned\\nimage compression, I started with a point\\nXi that was d numbers-- let's say 50,000 numbers. And now, I'm saying,\\nactually, you can throw out those\\n50,000 numbers. If you actually know only\\nthe k numbers that you need-- the 6 numbers that you need-- you're going to\\nhave something that was pretty close to getting\\nwhat information you had. So in a way, there is\\nsome form of compression that's going on here. And what you can do is that\\nthose principal components, you can actually use\\nnow for regression. If I want to regress\\nY onto X that's very high dimensional,\\nbefore I do this, if I don't have enough points,\\nmaybe what I can actually do is to do principal\\ncomponent analysis throughout my\\nexercise, replace them by those compressed versions,\\nand do linear aggression on those guys. And that's called principal\\ncomponent regression, not surprisingly. And that's something\\nthat's pretty popular. And you can do with k is\\nequal to 10, for example. So for data visualization, I did\\nnot find a Thanksgiving themed picture. But I found one that\\nhas turkey in it. Get it? So this is actually a\\ngene data set that was-- so when you see\\nsomething like this, you can imagine that someone\\nhas been preprocessing the hell out of this thing. This is not like, oh, I\\ncollect data on 23andMe and I'm just going\\nto run PCA on this. It just doesn't\\nhappen like that. And so what happened is that--\\nso let's assume that this was a bunch of preprocessed data,\\nwhich are gene expression levels-- so 500,000 genes\\namong 1,400 Europeans. So here, I actually\\nhave less observations than I have samples. And that's when you use\\nprincipal component regression most of the time, so\\nit doesn't stop you. And then what you do is you say,\\nOK, have those 500,000 genes among-- so here, that means that\\nthere's 1,400 points here. And I actually take\\nthose 500,000 directions. So each person has a vector\\nof, say, 500,000 genes that are attached to them. And I project them onto\\ntwo dimensions, which should be extremely lossy. I lose a lot of information. And indeed, I do, because\\nI'm one of these guys. And I'm pretty sure I'm very\\ndifferent from this guy, even though probably from\\nan American perspective, we're all the same. But I think we have like\\nslightly different genomes. And so the thing is\\nnow we have this-- so you see there's lots of\\nSwiss that participate in this. But actually, those two\\nprincipal components recover sort of\\nthe map of Europe. I mean, OK, again, this is\\nactually maybe fine-grained for you guys. But right here, there's\\nPortugal and Spain, which are those colors. So here is color-coded. And here is Turkey, of\\ncourse, which we know has very different genomes. So Turks are very\\nat the boundary. So you can see all the greens. They stay very far apart\\nfrom everything else. And then the rest\\nhere is pretty mixed. But it sort of recovers--\\nif you look at the colors, it sort of recovers that. So in a way, those two\\nprincipal components are just the geographic feature. So if you insist to compress\\nall the genomic information of these people into two\\nnumbers, what you're actually going to get is\\nlongitude and latitude, which is somewhat\\nsurprising, but not so much if you think that's\\nit's been preprocessed. So what do you do\\nbeyond practice? Well, you could try to\\nactually study those things. If you think about\\nit for a second, we did not do any statistics. I talked to you about\\nIID observations, but we never used the fact\\nthat they were independent. The way we typically\\nuse independence is to have central\\nlimit theorem, maybe. I mentioned the fact that\\nthe covariances of the word Gaussian would actually give me\\nsomething which is independent. We didn't care. This was a data analysis, data\\nmining process that we did. I give you points, and you just\\nput them through the crank. There was an algorithm\\nin six steps. And you just put it through\\nand that's what you got. Now, of course, there's some\\nwork which studies says, OK, if my data is actually generated\\nfrom some process-- maybe, my points are multivariate\\nGaussian with some structure on the covariance-- how well am I recovering\\nthe covariance structure? And that's where\\nstatistics kicks in. And that's where we stop. So this is actually a bit\\nmore difficult to study. But in a way, it's not\\nentirely satisfactory, because we could work\\nfor a couple of boards and I would just basically\\nsort of reverse engineer this and find some models under which\\nit's a good idea to do that. And what are those models? Well, those are the models\\nthat sort of give you sort of prominent directions\\nthat you want to find. And it will say, yes, if you\\nhave enough observations, you will find those\\ndirections along which your data is elongated. So that's essentially\\nwhat you want to do. So that's exactly what\\nthis thing is telling you. So where does the\\nstatistics lie from? Well, everything, remember--\\nso actually that's where Alana was confused--\\nthe idea was to say, well, if I have a true\\ncovariance matrix sigma and I never really\\nhave access to it, I'm just running PCA on the\\nempirical covariance matrix, how do those results relate? And this is something\\nthat you can study. So for example, if\\nn goes to infinity and the number of points,\\nyour dimension, is fixed, then S goes to sigma\\nin any sense you want. Maybe each entry is going\\nto each entry of sigma, for example. So S is a good estimator. We know that the\\nempirical covariance is a consistent as the mater. And if d is fixed, this\\nis actually not an issue. So in particular, if you run\\nPCA on the sample covariance matrix, you look\\nat, say, v1, then v1 is going to converge to the\\nlargest eigenvector of sigma as n goes to infinity,\\nbut for d fixed. And that's a story that\\nwe know since the '60s. More recently, people have\\nstarted challenging this. Because what's happening\\nwhen you fix the dimension and let the sample\\nsize go to infinity, you're certainly not\\nallowing for this. It's certainly not explaining\\nto you anything about the fact when d is equal to 500,000\\nand n is equal to 1,400. Because when d is fixed\\nand n goes to infinity, in particular, n is\\nmuch larger than d, which is not the case here. And so when n is much larger\\nthan d, things go well. But if d is less than n,\\nit's not clear what happens. And particularly, if d is of the\\norder of n, what's happening? So there's an entire theory\\nin mathematics that's called random matrix theory that\\nstudies the behavior of exactly this question-- what is the\\nbehavior of the spectrum-- the eigenvalues\\nand eigenvectors-- of a matrix in which I put\\nrandom numbers and I let-- so the matrix I'm interested\\nin here is the matrix of X's. When I stack all my\\nX's next to each other, so that's a matrix of size,\\nsay, d by n, so each column is of size d, it's one person. And so I put them. And when I let the\\nmatrix go to infinity, I let both d and n to infinity. But I want the aspect ratio,\\nd/n, to go to some constant. That's what they do. And what's nice is that in the\\nend, you have this constant-- let's call it gamma-- that shows up in\\nall the asymptotics. And then you can\\nreplace it by d/n. And you know that you still have\\na handle of both the dimension and the sample size. Whereas, usually the dimension\\ngoes away, as you let n go to infinity without having\\ndimension going to infinity. And so now, when\\nthis happens, as soon as d/n goes to a\\nconstant, you can show that essentially there's\\nan angle between the largest eigenvector of sigma and the\\nlargest eigenvector of S, as n and d go to infinity. There is always an\\nangle-- you can actually write it explicitly. And it's an angle that\\ndepends on this ratio, gamma-- the asymptotic ratio of d/n. And so there's been a lot of\\nunderstanding how to correct, how to pay attention to this. This creates some biases that\\nwere sort of overlooked before. In particular, when\\nI do this, this is not the proportion\\nof explained variance, when n and d are similar. This is an estimated\\nnumber computed from S. This is computed from S. All\\nthese guys are computed from S. So those are\\nactually not exactly where you want them to be. And there's some nice work that\\nallows you to recalibrate what this ratio should be, how\\nthis ratio should be computed, so it's a better\\nrepresentative of what the proportion of explained\\nvariance actually is. So then, of course,\\nthere's the question of-- so that's when d/n\\ngoes to some constant. So the best case--\\nso that was '60s-- d is fixed and it's\\nmuch larger than d. And then random matrix theory\\ntells you, well, d and n are sort of the same\\norder of magnitude. When they go to infinity, the\\nratio goes to some constant. Think of it as being order 1. To be fair, if d is 100 times\\nlarger than n, it still works. And it depends on\\nwhat you think what the infinity is at this point. But I think the random matrix\\ntheory results are very useful. But then even in\\nthis case, I told you that the leading\\neigenvector of S is actually an angle of the\\nleading eigenvector of-- So what's happening is that-- so let's say that d/n\\ngoes to some gamma. And what I claim is\\nthat, if you look at-- so that's v1, that's the v1 of\\nS. And then there's the v1 of-- so this should be of size 1. So that's the v1 of sigma. Then those things are going\\nto have an angle, which is some function of gamma. It's complicated, but\\nthere's a function of gamma that you can see there. And there's some models. When gamma goes\\nto infinity, which means that d is now\\nmuch larger than n, this angle is 90\\ndegrees, which means that you're getting nothing. Yeah. AUDIENCE: If d is not\\non your lower plane, so like gamma is 0,\\nis there still angle? PHILIPPE RIGOLLET: No,\\nbut that's consistent-- the fact that it's\\nconsistent when-- so the angle is a function-- AUDIENCE: d is not a\\nconstant [INAUDIBLE]?? PHILIPPE RIGOLLET:\\nd is not a constant? So if d is little of n? Then gamma goes to 0 and\\nf of gamma goes to 0. So f of gamma is\\na function that-- so for example, if f of gamma-- this is the sine of the\\nangle, for example-- then it's a function that starts\\nat 0, and that goes like this. But as soon as gamma is\\npositive, it goes away from 0. So now when gamma\\ngoes to infinity, then this thing goes\\nto a right angle, which means I'm getting just junk. So this is not my\\nleading eigenvector. So how do you do this? Well, just like\\neverywhere in statistics, you have to just make\\nmore assumptions. You have to assume\\nthat you're not looking for the leading\\neigenvector or the direction that carries the most variance. But you're looking, maybe,\\nfor a special direction. And that's what\\nsparse PCA is doing. Sparse PCA is saying, I'm not\\nlooking for any direction new that carries the most variance. I'm only looking for a\\ndirection new that is sparse. Think of it, for example, as\\nhaving 10 non-zero coordinates. So that's a lot of\\ndirections still to look for. But once you do this,\\nthen you actually have not only--\\nthere's a few things that actually you\\nget from doing this. The first one is you\\nactually essentially replace d by k, which means\\nthat n now just-- I'm sorry, let's say S\\nnon-zero coefficients. You replace d by S,\\nwhich means that n only has to be much larger than S\\nfor this thing to actually work. Now, of course, you've\\nset your goal weaker. Your goal is not to\\nfind any direction, only a sparse direction. But there's something\\nvery valuable about sparse directions,\\nis that they actually are interpretable. When I found the v-- let's say that the v\\nthat I found before was 0.2, and then 0.9, and\\nthen 1.1 minus 3, et cetera. So that was the coordinates\\nof my leading eigenvector in the original\\ncoordinate system. What does it mean? Well, it means that if\\nI see a large number, that means that this\\nv is very close-- so that's my original\\ncoordinate system. Let's call it e1 and e2. So that's just 1,\\n0; and then 0, 1. Then clearly, from\\nthe coordinates of v, I can tell if my v is like\\nthis, or it's like this, or it's like this. Well, I mean, they should\\nall be of the same size. So I can tell if\\nit's here or here or here, depending\\non-- like here, that means I'm going\\nto see something where the Y-coordinate it much\\nlarger than the X-coordinate. Here, I'm going to see something\\nwhere the X-coordinate is much larger than the Y-coordinate. And here, I'm going\\nto see something where the X-coordinate\\nis about the same size of the Y-coordinate. So when things\\nstarts to be bigger, you're going to have\\nto make choices. What does it mean to be bigger-- when d is 100,000,\\nI mean, the sum of the squares of those\\nguys have to be equal to 1. So they're all\\nvery small numbers. And so it's hard for you to\\ntell which one is a big number and which ones is\\na small number. Why would you want to know this? Because it's\\nactually telling you that if v is very close to\\ne1, then that means that e1-- in the case of the\\ngene example, that would mean that e1 is the\\ngene that's very important. Maybe there's actually\\njust two genes that explain those two things. And those are the genes\\nthat have been picked up. There's two genes that I\\nencode geographic location, and that's it. And so it's very\\nimportant for you to be able to\\ninterpret what v means. Where it has large\\nvalues, it means that maybe it has large\\nvalues for e1, e2, and e3. And it means that it's a\\ncombination of e1, e2, and e3. And now, you can\\ninterpret, because you have only three variables to find. And so sparse PCA\\nbuilds that in. Sparse PCA says,\\nlisten, I'm going to want to have at most\\n10 non-zero coefficients. And the rest, I want to be 0. I want to be able to be a\\ncombination of at most 10 of my original variables. And now, I can do\\ninterpretation. So the problem\\nwith sparse PCA is that it becomes very\\ndifficult numerically to solve this problem. I can write it. So the problem is simply\\nmaximize the variance u transpose, say, Su\\nsubject to-- well, I wanted to have u2 equal to 1. So that's the original PCA. But now, I also\\nwant that the sum of the indicators of the\\nuj that are not equal to 0 is at most, say, 10. This constraint is\\nvery non-convex. So I can relax it\\nto a convex one like we did for\\nlinear aggression. But now, I've totally\\nmessed up with the fact that I could use linear\\nalgebra to solve this problem. And so now, you have to go\\nthrough much more complicated optimization techniques,\\nwhich are called semidefinite\\nprograms, which do not scale well in high dimensions. And so you have to do\\na bunch of tricks-- numerical tricks. But there are some packages\\nthat implements some heuristics or some other things-- iterative\\nthresholding, all sorts of various numerical\\ntricks that you can do. But the problem they are trying\\nto solve is exactly this. Among all directions that\\nI have norm 1, of course, because it's the direction\\nthat have at most, say, 10 non-zero coordinates, I want\\nto find the one that maximizes the empirical variance. Actually, let me let\\nme just so you this. I wanted to show\\nyou an output of PCA where people are actually\\ntrying to do directly-- maybe-- there you go. So right here, you\\nsee this is SPSS. That's a statistical software. And this is an output\\nthat was preprocessed by a professional-- not preprocessed,\\npost-processed. So that's something\\nwhere they read PCA. So what is the data? This is raw data\\nabout you ask doctors what they think of the\\nbehavior of a particular sales representative for\\npharmaceutical companies. So pharmaceutical\\ncompanies are trying to improve their sales force. And they're asking\\ndoctors how would they rate-- what do they value\\nabout their interaction with a sales representative. So basically, there's\\na bunch of questions. One offers credible point\\nof view on something trends, provides valuable\\nnetworking opportunities. This is one question. Rate this on a\\nscale from 1 to 5. That was the question. And they had a bunch\\nof questions like this. And then they asked 1,000\\ndoctors to make those ratings. And what they want--\\nso each doctor now is a vector of ratings. And they want to know if there's\\ndifferent groups of doctors, what do doctors respond to. If there's different\\ngroups, then maybe they know that they\\ncan actually address them separately, et cetera. And so to do that, of course,\\nthere's lots of questions. And so what you want is\\nto just first project into lower dimensions,\\nso you can actually visualize what's going on. And this is what\\nwas done for this. So these are the three\\nfirst principal component that came out. And even though we ordered\\nthe values of the lambdas, there's no reason why the\\nentries of v should be ordered. And if you look at\\nthe values of v here, they look like they're\\npretty much ordered. It starts at 0.784, and then\\nyou're at 0.3 around here. There's something that goes up\\nagain, and then you go down. Actually, it's marked in red\\nevery time it goes up again. And so now, what they\\ndid is they said, OK, I need to\\ninterpret those guys. I need to tell you what this is. If you tell me, we found\\nthe principal component that really discriminates\\nthe doctors in two groups, the drug company is\\ngoing to come back to you and say, OK, what is\\nthis characteristic? And you say, oh, it's\\nactually a linear combination of 40 characteristics. And they say, well, we\\ndon't need you to do that. I mean, it cannot be a linear\\ncombination of anything you didn't ask. And so for that,\\nfirst of all, there's a post-processing of PCA, which\\nsays, OK, once I actually, say, found three\\nprincipal components, that means that I found the\\ndimension three space on which I want to project my points. In this base, I can pick\\nany direction I want. So the first thing\\nis that you do some sort of local arrangements,\\nso that those things look like they are increasing\\nand then decreasing. So you just change, you\\nrotate your coordinate system in this three dimensional space\\nthat you've actually isolated. And so once you do\\nthis, the reason to do that is that\\nit sort of makes them big, sharp differences\\nbetween large and small values of the coordinates\\nof the thing you had. And why do you want this? Because now, you\\ncan say, well, I'm going to start looking at the\\nones that have large values. And what do they say? They say in-depth knowledge,\\nin-depth knowledge, in-depth knowledge,\\nknowledge about. This thing is clearly\\nsomething that actually characterizes\\nthe knowledge of my sales representative. And so that's something that\\ndoctors are sensitive to. That's something that\\nreally discriminates the doctors in a way. There's lots of variance\\nalong those things, or at least a lot of variance-- I mean, doctors are separate\\nin terms of their experience with respect to this. And so what they\\ndid is said, OK, all these guys, some of\\nthose they have large values, but I don't know how\\nto interpret them. And so I'm just going\\nto put the first block, and I'm going to call\\nit medical knowledge, because all those things are\\nknowledge about medical stuff. Then here, I didn't know\\nhow to interpret those guys. But those guys, there's a big\\nclump of large coordinates, and they're about respectful\\nof my time, listens, friendly but courteous. This is all about the\\nquality of interaction. So this block was actually\\ncalled quality of interaction. And then there\\nwas a third block, which you can tell starts to\\nbe spreading a little thin. There's just much less of them. But this thing was\\nactually called fair and critical opinion. And so now, you have three\\ndiscriminating directions. And you can actually\\ngive them a name. Wouldn't it be beautiful if\\nall the numbers in the gray box came non-zero and\\nall the other numbers came zero-- there\\nwas no ad hoc choice. I mean, this is probably\\nan afternoon of work to like scratch out\\nall these numbers and put all these\\ncolor codes, et cetera. Whereas, you could just have\\nsomething that tells you, OK, here are the non-zeros. If you can actually make a story\\naround why this group of thing actually makes sense, such\\nas it is medical knowledge, then good for you. Otherwise, you could\\njust say, I can't. And that's what sparse\\nPCA does for you. Sparse PCA outputs something\\nwhere all those numbers would be zero. And there would be exactly,\\nsay, 10 non-zero coordinates. And you can turn\\nthis knob off 10. You can make it 9. Depending on what\\nyour major is, maybe you can actually go\\non with 20 of them and have the ability to\\ntell the story about 20 different variables and how\\nthey fit in the same group. And depending on\\nhow you feel, it's easy to rerun the PCA\\ndepending on the value that you want here. And so you could actually\\njust come up with the one you prefer. And so that's the\\nsparse PCA thing which I'm trying to promote. I mean, this is not\\nsuper well-spread. It's a fairly new idea,\\nmaybe at most 10 years old. And it's not\\ncompletely well-spread in statistical packages. But that's clearly\\nwhat people are trying to emulate currently. Yes? AUDIENCE: So what\\nexactly does it mean that the doctors\\nhave a lot of variance in medical knowledge,\\nquality of interaction, and fair and critical opinion? Like, it was saying that\\nthese are like the main things that doctors vary on,\\nsome doctors care. Like we could sort of\\ncharacterize a doctor by, oh, he cares this much about\\nmedical knowledge, this much about the quality\\nof interaction, and this much about\\ncritical opinion. And that says most of the story\\nabout what this doctor wants from a drug representative? PHILIPPE RIGOLLET: Not really. I mean, OK, let's say\\nyou pick only one. So that means that you\\nwould take all your doctors, and you would have\\none direction, which is quality of interaction. And there would be just\\nspread out points here. So there are two\\nthings that can happen. The first one is that\\nthere's a clump here, and then there's a clump here. That still represents\\na lot of variance. And if this happens,\\nyou probably want to go back in\\nyour data and see were these people visited\\nby a different group than these people,\\nor maybe these people have a different specialty. I mean, you have to\\nlook back at your data and try to understand\\nwhy you would have different groups of people. And if it's like completely\\nevenly spread out, then all it's saying\\nis that, if you want to have a uniform\\nquality of interaction, you need to take\\nmeasures on this. You need to have this to\\nnot be discrimination. But I think really when it's\\nbecoming interesting it's not when it's complete spread out. It's when there's\\na big group here. And then there's\\nalmost no one here, and then there's\\na big group here. And then maybe there's\\nsomething you can do. And so those two things actually\\ngive you a lot of variance. So actually, maybe\\nI'll talk about this. Here, this is sort of a mixture. You have a mixture of\\ntwo different populations of doctors. And it turns out that\\nprincipal component analysis-- so a mixture is when you\\nhave different populations-- think of like two\\nGaussians that are just centered at two\\ndifferent points, and maybe they're\\nin high dimensions. And those are\\nclusters of people, and you want to be able to\\ndifferentiate those guys. If you're in very\\nhigh dimensions, it's going to be very\\ndifficult. But one of the first processing tools\\nthat people do is to do PCA. Because if you have one big\\ngroup here and one big group here, it means that\\nthere's a lot of variance along the direction that\\ngoes through the centers of those groups. And that's essentially\\nwhat happened here. You could think of this as being\\ntwo blobs in high dimensions. But you're really\\njust projecting them into one dimension. And this dimension, hopefully,\\ngoes through the center. And so as preprocessing--\\nso I'm going to stop here. But PCA is not just made\\nfor dimension reduction. It's used for\\nmixtures, for example. It's also used when you\\nhave graphical data. What is the idea of PCA? It just says, if you have a\\nmatrix that seems to have low rank-- meaning that there's a\\nlot of those lambda i's that are very small-- and then I see that\\nplus noise, then it's a good idea to\\ndo PCA on this thing. And in particular, people\\nuse that in networks a lot. So you take the adjacency\\nmatrix of a graph-- well, you sort of preprocess it\\na little bit, so it looks nice. And then if you have, for\\nexample, two communities in there, it should\\nlook like something that is low rank plus some noise. And low rank means that there's\\njust very few non-zero-- well, low rank means this. Low rank means that if\\nyou do the scree plot, you will see\\nsomething like this, which means that if you throw\\nout all the smaller ones, it should not really matter\\nin the overall structure. And so you can use all-- these techniques are used\\neverywhere these days, not just in PCA. So we call it PCA\\nas statisticians. But people call it the\\nspectral methods or SVD. So everyone--\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 11,\n        \"samples\": [\n          \"Data Structures\",\n          \"Linear Algebra\",\n          \"Diff. Eq.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_org.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDOsTVplwjpZ",
        "outputId": "3557444c-a174-4b96-c2c9-d6d3d893c0f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(859, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels = df_org['label'].unique().tolist()\n",
        "labels = [s.strip() for s in labels ]\n",
        "labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QqFCarUOxZHH",
        "outputId": "22f78832-8bf3-4462-8616-29ea8dbcb37f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Linear Algebra',\n",
              " 'Math for Eng.',\n",
              " 'CS',\n",
              " 'Algorithms',\n",
              " 'AI',\n",
              " 'Data Structures',\n",
              " 'Calculus',\n",
              " 'Probability',\n",
              " 'Statistics',\n",
              " 'Diff. Eq.',\n",
              " 'NLP']"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for key, value in enumerate(labels):\n",
        "    print(value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HiI96SdXxkQe",
        "outputId": "910129ee-9f6a-4a0c-9ac0-18b27686c305"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Algebra\n",
            "Math for Eng.\n",
            "CS\n",
            "Algorithms\n",
            "AI\n",
            "Data Structures\n",
            "Calculus\n",
            "Probability\n",
            "Statistics\n",
            "Diff. Eq.\n",
            "NLP\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_LABELS= len(labels)\n",
        "\n",
        "id2label={id:label for id,label in enumerate(labels)}\n",
        "\n",
        "label2id={label:id for id,label in enumerate(labels)}"
      ],
      "metadata": {
        "id": "esrtefFlxuDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label2id, id2label"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ic2LJ_8rx12_",
        "outputId": "140e99e3-8ee1-47ad-9d52-d570d2c66b3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'Linear Algebra': 0,\n",
              "  'Math for Eng.': 1,\n",
              "  'CS': 2,\n",
              "  'Algorithms': 3,\n",
              "  'AI': 4,\n",
              "  'Data Structures': 5,\n",
              "  'Calculus': 6,\n",
              "  'Probability': 7,\n",
              "  'Statistics': 8,\n",
              "  'Diff. Eq.': 9,\n",
              "  'NLP': 10},\n",
              " {0: 'Linear Algebra',\n",
              "  1: 'Math for Eng.',\n",
              "  2: 'CS',\n",
              "  3: 'Algorithms',\n",
              "  4: 'AI',\n",
              "  5: 'Data Structures',\n",
              "  6: 'Calculus',\n",
              "  7: 'Probability',\n",
              "  8: 'Statistics',\n",
              "  9: 'Diff. Eq.',\n",
              "  10: 'NLP'})"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_org[\"labels\"]=df_org.label.map(lambda x: label2id[x.strip()])"
      ],
      "metadata": {
        "id": "_nseG6PSyB3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_org.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "uP1qvy1YyNLe",
        "outputId": "78e0c794-5ffa-425f-f577-095479ad9e15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                  text           label  labels\n",
              "714  Let's say we have three\\nmatrices, A, B, and C...  Linear Algebra       0\n",
              "605  All right. So let's get started. So we're stil...   Math for Eng.       1\n",
              "120  I've got a function f and it's\\na mapping from...  Linear Algebra       0\n",
              "208  - [voiceover] So now that\\nwe've spent some ti...              CS       2\n",
              "380  [MUSIC] Stanford University. &gt;&gt; Happened...      Algorithms       3"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6416990f-3a00-41df-855b-347954aa1ade\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>714</th>\n",
              "      <td>Let's say we have three\\nmatrices, A, B, and C...</td>\n",
              "      <td>Linear Algebra</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>605</th>\n",
              "      <td>All right. So let's get started. So we're stil...</td>\n",
              "      <td>Math for Eng.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>120</th>\n",
              "      <td>I've got a function f and it's\\na mapping from...</td>\n",
              "      <td>Linear Algebra</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>208</th>\n",
              "      <td>- [voiceover] So now that\\nwe've spent some ti...</td>\n",
              "      <td>CS</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>380</th>\n",
              "      <td>[MUSIC] Stanford University. &amp;gt;&amp;gt; Happened...</td>\n",
              "      <td>Algorithms</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6416990f-3a00-41df-855b-347954aa1ade')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6416990f-3a00-41df-855b-347954aa1ade button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6416990f-3a00-41df-855b-347954aa1ade');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-fc2fd98c-4d98-4d3c-82fc-e1119ba9ce36\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-fc2fd98c-4d98-4d3c-82fc-e1119ba9ce36')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-fc2fd98c-4d98-4d3c-82fc-e1119ba9ce36 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_org",
              "summary": "{\n  \"name\": \"df_org\",\n  \"rows\": 859,\n  \"fields\": [\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 858,\n        \"samples\": [\n          \"In the last video I was a little\\nformal in defining what Rn is, and what a vector is,\\nand what vector addition or scalar multiplication is. In this video I want to kind of\\ngo back to basics and just give you a lot of examples. And give you a more tangible\\nsense for what vectors are and how we operate with them. So let me define a couple\\nof vectors here. And I'm going to do, most of my\\nvectors I'm going to do in this video are going\\nto be in R2. And that's because they're\\neasy to draw. Remember R2 is the set\\nof all 2-tuples. Ordered 2-tuples where each of\\nthe numbers, so you know you could have x1, my 1 looks like a\\ncomma, x1 and x2, where each of these are real numbers. So you each of them, x1 is a\\nmember of the reals, and x2 is a member of the reals. And just to give you a sense\\nof what that means, if this right here is my coordinate\\naxes, and I wanted a plot all my x1's, x2's. You know you could view this\\nas the first coordinate. We always imagine that\\nas our x-axis. And then our second coordinate\\nwe plotted on the vertical axis. That traditionally is our\\ny-axis, but we'll just call that the second number\\naxis, whatever. You could visually represent all\\nof R2 by literally every single point on this plane if\\nwe were to continue off to infinity in every direction. That's what R2 is. R1 would just be points\\njust along one of these number lines. That would be R1. So you could immediately\\nsee that R2 is kind of a bigger space. But anyway, I said that I\\nwouldn't be too abstract, that I would show you examples. So let's get some vectors\\ngoing in R2. So let me define my vector a. I'll make it nice and bold. My vector a is equal to,\\nI'll make some numbers up, negative 1, 2. And my vector b, make it nice\\nand bold, let me make that, I don't know, 3, 1. Those are my two vectors. Let's just add them up\\nand see what we get. Just based on my definition\\nof vector addition. I'll just stay in one color\\nfor now so I don't have to keep switching back and forth. So a, nice deep a, plus bolded\\nb is equal to, I just add up each of those terms.\\nNegative 1 plus 3. And then 2 plus 1. That was my definition\\nof vector addition. So that is going to be\\nequal to 2 and 3. Fair enough that just\\ncame out of my definition of vector addition. But how can we represent\\nthis vector? So we already know that if we\\nhave coordinates, you know, if I have the coordinate, and this\\nis just a convention. It's just the way\\nthat we do it. The way we visualize things. If I wanted to plot the\\npoint 1, 1, I go to my coordinate axes. The first point I go along\\nthe horizontal, what we traditionally call our x-axis. And I go 1 in that direction. And then convention is, the\\nsecond point I go 1 in the vertical direction. So the point 1, 1. Oh, sorry, let me\\nbe very clear. This is 2 and 2, so one\\nis right here, and one is right there. So the point 1, 1 would\\nbe right there. That's just the standard\\nconvention. Now our convention for\\nrepresenting vectors are, you might be tempted to say, oh,\\nmaybe I just represent this vector at the point\\nminus 1, 2. And on some level\\nyou can do that. I'll show you in a second. But the convention for vectors\\nis that you can start at any point. Let's say we're dealing with\\ntwo dimensional vectors. You can start at any\\npoint in R2. So let's say that you're\\nstarting at the point x1, and x2. This could be any point in R2. To represent the vector, what\\nwe do is we draw a line from that point to the point x1. And let me call this, let's say\\nthat we wanted to draw a. So x1 minus 1. So this is, I'm representing\\na. So this is, I want to represent\\nthe vector a. x1 minus 1, and then\\nx1 plus 2. Now if that seems confusing to\\nyou, when I draw it, it'll be very obvious. So let's say I just want to\\nstart at the point, let's just say for quirky reasons, I just\\npick a random point here. I just pick a point. That one right there. That's my starting point. So minus 4, 4. Now if I want to represent my\\nvector a, what I just said is that I add the first term\\nin vector a to my first coordinate. So x1 plus minus 1\\nor x1 minus 1. So my new one is going to be,\\nso this is my x1 minus 4. So now it's going to be, let's\\nsee, I'm starting at the point minus 4 comma 4. If I want to represent a, what\\nI do is, I draw an arrow to minus 4 plus this first\\nterm, minus 1. And then 4 plus the\\nsecond term. 4 plus 2. And so this is what? This is minus 5 comma 6. So I go to minus 5 comma 6. So I go to that point right\\nthere and I just draw a line. So my vector will\\nlook like this. I draw a line from\\nthere to there. And I draw an arrow\\nat the end point. So that's one representation\\nof the vector minus 1, 2. Actually let me do it\\na little bit better. Because minus 5 is actually\\nmore, a little closer to right here. Minus 5 comma 6 Is right\\nthere, so I draw my vector like that. But remember this point minus\\n4 comma 4 was an arbitrary place to draw my vector. I could have started\\nat this point here. I could have started at the\\npoint 4 comma 6 and done the same thing. I could have gone minus 1 in\\nthe horizontal direction, that's my movement in the\\nhorizontal direction. And then plus 2 in the\\nvertical direction. So I could have drawn, so minus\\n1 in the horizontal and plus 2 in the vertical\\ngets me right there. So I could have just as easily\\ndrawn my vector like that. These are both interpretations\\nof the same vector a. I should draw them in the\\ncolor of vector a. So vector a was this light\\nblue color right there. So this is vector a. This is vector a. Sometimes there'll\\nbe a little arrow notation over the vector. But either of those vectors. I could draw an infinite\\nnumber of vector a's. I could draw vector a here. I could draw it like that. Vector a, it goes\\nback 1 and up 2. So vector a could\\nbe right there. Similarly vector b. What does vector b do? I could pick some arbitrary\\npoint for vector b. It goes to the right 3, so it\\ngoes to the right 1, 2, 3 and then it goes up 1. So vector b, one representation\\nof vector b, looks like this. Another represention. I can start it right here. I could go to the right 3,\\n1, 2, 3, and then up 1. This would be another\\nrepresentation of my vector b. There's an infinite number of\\nrepresentations of them. But the convention is to often\\nput them in what's called the standard position. And that's to start\\nthem off at 0, 0. So your initial point, let\\nme write this down. Standard position is just to\\nstart the vectors at 0, 0 and then draw them. So vector a in standard\\nposition, I'd start at 0, 0 like that and I would go\\nback 1 and then up 2. So this is vector a in standard\\nposition right there. And then vector b in\\nstandard position. Let me write that. That's a. And then vector b in standard\\nposition is 3, go to the 3 right and then up 1. These are the vectors in\\nstandard position, but any of these other things we drew\\nare just as valid. Now let's see if we can get\\nan interpretation of what happened when we\\nadded a plus b. Well if I draw that vector in\\nstandard position, I just calculated, it's 2, 3. So I go to the right\\n2 and I go up 3. So if I just draw it in\\nstandard position it looks like this. This vector right there. And at first when you look at\\nit, this vector right here is the vector a plus b in\\nstandard position. When you draw it like that,\\nit's not clear what the relationship is when\\nwe added a and b. But to see the relationship what\\nyou do is, you put a and b head to tails. What that means is, you put\\nthe tail end of b to the front end of a. Because remember, all\\nof these are valid representations of b. All of the representations\\nof the vector b. They all have, they're all\\nparallel to each other, but they can start from anywhere. So another equally valid\\nrepresentation of vector b is to start at this point right\\nhere, kind of the end point of vector a in standard position,\\nand then draw vector b starting from there. So you go 3 to the right. So you go 1, 2, 3. And then you go up 1. So vector b could also be\\ndrawn just like that. And then you should\\nsee something interesting had happened. And remember, this vector b\\nrepresentation is not in standard position, but it's just\\nan equally valid way to represent my vector. Now what do you see? When I add a, which is right\\nhere, to b what do I get if I connect the starting point of\\na with the end point of b? I get the addition. I have added the two vectors. And I could have done\\nthat anywhere. I could have started\\nwith a here. And then I could have\\ndone the end point. I could have started b here and\\ngone 3 to the right, 1, 2, 3 and then up 1. And I could have drawn b\\nright there like that. And then if I were to add a plus\\nb, I go to the starting point of a, and then\\nthe end point of b. And that should also\\nbe the visual representation of a plus b. Just to make sure it confirms\\nwith this number, what I did here was I went 2 to\\nthe right, 1, 2 and then I went 3 up. 1, 2, 3 and I got a plus b. Now let's think about\\nwhat happens when we scale our vectors. When we multiply it times\\nsome scalar factor. So let me pick new vectors. Those have gotten monotonous. Let me define vector v. v for vector. Let's say that it is\\nequal to 1, 2. So if I just wanted to draw\\nvector v in standard position, I would just go 1 to\\nthe horizontal and then 2 to the vertical. That's it. That's the vector in\\nstandard position. If I wanted to do it in a non\\nstandard position, I could do it right here. 1 to the right up 2,\\njust like that. Equally valid way of\\ndrawing vector v. Equally valid way of doing it. Now what happens if I\\nmultiply vector v. What if I have, I don't know,\\nwhat if I have 2 times v? 2 times my vector v is now going\\nto be equal to 2 times each of these terms. So it's\\ngoing to be 2 times 1 which is 2, and then 2 times\\n2 which is 4. Now what does 2 times\\nvector v look like? Well let me just start from\\nan arbitrary position. Let me just start\\nright over here. So I'm going to go 2\\nto the right, 1, 2. And I go up 4. 1, 2, 3, 4. So this is what 2 times\\nvector v looks like. This is 2 times my vector v. And if you look at it, it's\\npointing in the exact same direction but now it's\\ntwice as long. And that makes sense because we\\nscaled it by a factor of 2. When you multiply it by a\\nscalar, or you're not changing its direction. Its direction is the exact same\\nthing as it was before. You're just scaling\\nit by that amount. And I could draw\\nthis anywhere. I could have drawn\\nit right here. I could have drawn 2v\\nright on top of v. Then you would have seen it,\\nI don't want to cover it. You would have seen that it\\ngoes, it's exactly, in this case when I draw it in standard position, it's colinear. It's along the same line,\\nit's just twice as far. it's just twice as long\\nbut they have the exact same direction. Now what happens if I were\\nto multiply minus 4 times our vector v? Well then that will be equal\\nto minus 4 times 1, which is minus 4. And then minus 4 times\\n2, which is minus 8. So this is on my new vector. Minus 4, minus 8. This is minus 4 times\\nour vector v. So let's just start at\\nsome arbitrary point. Let's just do it in\\nstandard position. So you go to the right 4. Or you go to the left 4. So so you go to the left\\n4, 1, 2, 3, 4. And then down 8. Looks like that. So this new vector is going\\nto look like this. Let me try and draw a relatively\\nstraight line. There you go. So this is minus 4 times\\nour vector v. I'll draw a little arrow\\non it to make sure you know it's a vector. Now what happened? Well we're kind of in\\nthe same direction. Actually we're in the exact\\nopposite direction. But we're still along the\\nsame line, right? But we're just in the exact\\nopposite direction. And it's this negative right\\nthere that flipped us around. If we just multiplied negative\\n1 times this, we would have just flipped around to\\nright there, right? But we multiplied it\\nby negative 4. So we scaled it by 4, so you\\nmake it 4 times as long, and then it's negative, so\\nthen it flips around. It flips backwards. So now that we have that notion,\\nwe can kind of start understanding the idea of\\nsubtracting vectors. Let me make up 2 new\\nvectors right now. Let's say my vector x, nice and\\nbold x, is equal to, and I'm doing everything in R2, but\\nin the last part of this video I'll make a few examples\\nin R3 or R4. Let's say my vector x\\nis equal to 2, 4. And let's say I have\\na vector y. y, make it nice and bold. And then that is equal to\\nnegative 1, minus 2. And I want to think about\\nthe notion of what x minus y is equal to. Well we can say that this is the\\nsame thing as x plus minus 1 times our vector y. Right? So x plus minus 1 times\\nour vector y. Now we can use our\\ndefinitions. We know how to multiply\\nby a scalar. So we'll say that this\\nis equal to, let me switch colors. I don't like this color. This is equal to our\\nx vector is 2, 4. And then what's minus\\n1 times y? So minus 1 times y is minus\\n1 times minus 1 is 1. And then minus 1 times\\nminus 2 is 2. So x minus y is going to be\\nthese two vectors added to each other, right? I'm just adding the\\nminus of y. This is minus vector y. So this x minus y is going to\\nbe equal to 3 and 3 and 6. So let's see what that looks\\nlike when we visually represent them. Our vector x was 2, 4. So 2, 4 in standard position\\nit looks like this. That's my vector x. And then vector y in standard\\nposition, let me do it in a different color, I'll\\ndo y in green. Vector y is minus 1, minus 2. It looks just like this. And actually I ended up\\ninadvertently doing collinear vectors, but, hey, this\\nis interesting too. So this is vector y. So then what's their\\ndifference? This is 3, 6. So it's the vector 3, 6. So it's this vector. Let me draw it someplace else. If I start here I go 1, 2, 3. And then I go up 6. So then up 6. It's a vector that\\nlooks like this. That's the difference between\\nthe two vectors. So at first you say,\\nthis is x minus y. Hey, how is this the difference\\nof these two? Well if you overlay this. If you just shift this over\\nthis, you could actually just start here and go straight up. And you'll see that it's really\\nthe difference between the end points. You're kind of connecting\\nthe end points. I actually didn't want to\\ndraw collinear vectors. Let me do another example. Although that one's kind\\nof interesting. You often don't see that\\none in a book. Let me to define vector x\\nin this case to be 2, 3. And let me define vector y\\nto be minus 4, minus 2. So what would be x in\\nstandard position? It would be 2, 3. It'd look like that. That is our vector x if we\\nstart at the origin. So this is x. And then what does vector\\ny look like? I'll do y in orange. Minus 4, minus 2. So vector y looks like this. Now what is x minus y? Well you know, we could\\nview this, 2 plus minus 1 times this. We could just say\\n2 minus minus 4. I think you get the idea now. But we just did it the first\\nway the last time because I wanted to go from my basic\\ndefinitions of scalar multiplication. So x minus y is just going to\\nbe equal to 2 plus minus 1 times minus 4, or\\n2 minus minus 4. That's the same thing as\\n2 plus 4, so it's 6. And then it's 3 minus\\nminus 2, so it's 5. Right? So the difference between the\\ntwo is the vector 6, 5. So you could draw it\\nout here again. So you could go, add 6 to 4, go\\nup there, then to 5, you'd go like that. So the vector would look\\nsomething like this. It shouldn't curve like that,\\nso that's x minus y. But if we drew them between,\\nlike in the last example, I showed that you could draw it\\nbetween their two heads. So if you do it here, what\\ndoes it look like? Well if you start at this point\\nright there and you go 6 to the right and then up 5,\\nyou end up right there. So the difference between the\\ntwo vectors, let me make sure I get it, the difference\\nbetween the two vectors looks like that. It looks just like that. Which kind of should make\\nsense intuitively. x minus y. That's the difference between\\nthe two vectors. You can view the difference as,\\nhow do you get from one vector to another\\nvector, right? Like if, you know, let's go\\nback to our kind of second grade world of just scalars. If I say what 7 minus 5 is, and\\nyou say it's equal to 2, well that just tells you that\\n5 plus 2 is equal to 7. Or the difference between\\n5 and 7 is 2. And here you're saying, look the\\ndifference between x and y is this vector right there. It's equal to that vector\\nright there. Or you could say look, if I\\ntake 5 and add 2 I get 7. Or you could say, look, if I\\ntake vector y, and I add vector x minus y, then\\nI get vector x. Now let's do something else\\nthat's interesting. Let's do what y minus\\nx is equal to. y minus x. What is that equal to? Do it in another color\\nright here. Well we'll take minus 4, minus\\n2 which is minus 6. And then you have minus\\n2, minus 3. It's minus 5. So y minus x is going to be,\\nlet's see, if we start here we're going to go down 6. 1, 2, 3, 4, 5, 6. And then back 5. So back 2, 4, 5. So y minus x looks like this. It's really the exact\\nsame vector. Remember, it doesn't matter\\nwhere we start. It's just pointing in the\\nopposite direction. So if we shifted it here. I could draw it right\\non top of this. It would be the exact as x\\nminus y, but just in the opposite direction. Which is just a general\\ngood thing to know. So you can kind of do them as\\nthe negatives of each other. And actually let me make\\nthat point very clear. You know we drew y. Actually let me draw x, x\\nwe could draw as 2, 3. So you go to the right\\n2 and then up 3. I've done this before. This is x in non standard\\nposition. That's x as well. What is negative x? Negative x is minus 2 minus 3. So if I were to start here,\\nI'd go to minus 2, then I'd go minus 3. So minus x would look\\njust like this. Minus x. It looks just like x. It's parallel. It has the same magnitude. It's just pointing in the exact\\nopposite direction. And this is just a good thing\\nto kind of really get seared into your brain is to have an\\nintuition for these things. Now just to kind of finish up\\nthis kind of idea of adding and subtracting vectors. Everything I did so\\nfar was in R2. But I want to show you that\\nwe can generalize them. And we can even generalize them\\nto vector spaces that aren't normally intuitive for\\nus to actually visualize. So let me define a couple\\nof vectors. Let me define vector a to be\\nequal to 0, minus 1, 2, and 3. Let me define vector b to be\\nequal to 4, minus 2, 0, 5. We can do the same addition\\nand subtraction operations with them. It's just it'll be hard\\nto visualize. We can keep them in\\njust vector form. So that it's still useful to\\nthink in four dimensions. So if I were to say 4 times a. This is the vector a\\nminus 2 times b. What is this going\\nto be equal to? This is a vector. What is this going\\nto be equal to? Well we could rewrite this as\\n4 times this whole column vector, 0, minus 1, 2, and 3. Minus 2 times b. Minus 2 times 4,\\nminus 2, 0, 5. And what is this going\\nto be equal to? This term right here, 4 times\\nthis, you're going to get, the pen tablet seems to not work\\nwell there, so I'm going to do it right here. 4 times this, you're going to\\nget 4 times 0, 0, minus 4, 8. 4 times 2 is 8. 4 times 3 is 12. And then minus, I'll do it in\\nyellow, minus 2 times 4 is 8. 2 times minus 2 is minus 4. 2 times 0 is 0. 2 times 5 is 10. This isn't a good part of my\\nboard, so let me just. It doesn't write well\\nright over there. I haven't figured out the\\nproblem, but if I were just right it over here,\\nwhat do we get? With 0 minus 8? Minus 8. Minus 4, minus 4. Minus negative 4. So that's minus 4 plus\\n4, so that's 0. 8 minus 0 is 8. 12 minus, what was this? I can't even read it,\\nwhat it says. Oh, this is a 10. Now you can see it again. Something is very bizarre. 2 times 5 is 10. So it's 12 minus\\n10, so it's 2. So when we take this vector\\nand multiply it by 4, and subtract 2 times this vector,\\nwe just get this vector. And even though you can't\\nrepresent this in kind of an easy kind of graph-able format, this is a useful concept. And we're going to see this\\nlater when we apply some of these vectors to\\nmulti-dimensional spaces.\",\n          \"The following content is\\nprovided under a Creative Commons license. Your support will help MIT\\nOpenCourseWare continue to offer high quality educational\\nresources for free. To make a donation or to view\\nadditional materials from hundreds of MIT courses, visit\\nMIT OpenCourseWare at ocw.mit.edu. PROFESSOR: Today we're going to\\ncontinue our discussion of methods of integration. The method that I'm going to\\ndescribe today handles a whole class of functions of\\nthe following form. You take P (x) / Q (x)\\nand this is known as a rational function. And all that means is that\\nit's a ratio off two polynomials, which are these\\nfunctions P ( x) and Q ( x). We'll handle all such functions\\nby a method which is known as partial fractions. And what this does is, it splits\\nP / Q into what you could call easier pieces. So that's going to be some\\nkind of algebra. And that's what we're going\\nto spend most of our time doing today. I'll start with an example. And all of my examples\\nwill be illustrating more general methods. The example is to integrate the\\nfunction 1 / x - 1 +, say, 3 / x + 2 dx. That's easy to do. It's just, we already\\nknow the answer. It's ln x - 1 + 3 ln x + 3. Plus a constant. So that's done. So now, here's the difficulty\\nthat is going to arise. The difficulty is that I can\\nstart with this function, which is perfectly manageable. And than I can add these\\ntwo functions together. The way I add fractions. So that's getting a common\\ndenominator. And so that gives me x +\\n2 here + 3 ( x - 1). And now if I combine together\\nall of these terms, then altogether I have 4x +\\n2 - 3, that's - 1. And if I multiply out the\\ndenominator that's x ^2 + that 2 turned into a 3, that's\\ninteresting. Hope there aren't too many more\\nof those transformations. Is there another one here? STUDENT: [INAUDIBLE] PROFESSOR: Oh, it happened\\nearlier on. Wow that's an interesting\\nvibration there. OK. Thank you. So, I guess my 3's were\\nspeaking to my 2's. Somewhere in my past.\\nOK, anyway, I think this is now correct. So the problem is\\nthe following. This is the problem with this. This integral was easy. I'm calling it easy, we already\\nknow how to do it. Over here. But now over here,\\nit's disguised. It's the same function, but it's\\nno longer clear how to integrate it. If you're faced with this\\none, you say what am I supposed to do. And we have to get around\\nthat difficulty. And so what we're going\\nto do is we're going to unwind this disguise. So we have the algebra problem\\nthat we have. Oh, wow. There must be something\\nin the water. Impressive. Wow. OK, let's see. Is 2/3 = 3/2? Holy cow. Well that's good. Well, I'll keep you awake\\ntoday with several other transpositions here. So our algebra problem is to\\ndetect the easy pieces which are inside. And the method that we're going\\nto use, the one that we'll emphasize anyway, is one\\nalgebraic trick which is a shortcut, which is called\\nthe cover-up method. But we're going to talk\\nabout even more general things than that. But anyway, this is where\\nwe're headed. Is something called the\\ncover-up method. Alright. So that's our intro. And I'll just have to remember\\nthat 2 is not 3. I'll keep on repeating that. So now here I'm going to\\ndescribe to you how we unwind this disguise. The first step is,\\nwe write down the function we want to integrate. Which was this. And now we have to undo the\\nfirst damage that we did. So the first step is to factor\\nthe denominator. And that factors, we happen to\\nknow the factors, so I'm not going to carry this out. But this can be a rather\\ndifficult step. But we're going to assume\\nthat it's done. For the purposes of\\nillustration here. So I factor the denominator. And now, the second thing that\\nI'm going to do is what I'm going to call the setup here. How I'm going to\\nset things up. And I'll tell you what these\\nthings are more systematically in a second. And the setup is that\\nI want to somehow detect what I did before. And I'm going to write\\nsome unknowns here. What I expect is that this will\\nbreak up into two pieces. One with the denominator x -\\n1, and the other with the denominator x + 2. So now, my third step is going\\nto be to solve for A and B. And then I'm done,\\nif I do that. That's the complete unwinding\\nof this disguise. And this is where the cover-up\\nmethod comes in handy. This is this method that\\nI'm about to describe. Now, you can do the algebra in a\\nclumsy way, or you can do it in a quick way. And we'd like to get efficient\\nabout the algebra involved. And so let me show you\\nwhat the first step in the trick is. We're going to solve for A by\\nmultiplying by (x - 1). Now, notice if you multiply by\\n(x - 1) in that equation 2, what you get is this. You got 4x - 2 / the\\nx - 1's cancel. You get this on the\\nleft-hand side. And on the right-hand side\\nyou get A. The x - 1's cancel again. And then we get this\\nextra term. Which is B/ ( x + 2)( x - 1). Now, the trick here, and we're\\ngoing to get even better trick in just a second. The trick here is that I\\ndidn't try to clear the denominators completely. I was very efficient about\\nthe way I did it. It just cleared one factor. And the result here\\nis very useful. Namely, if I plug in now x =\\n1, this term drops out too. So what I'm going to do now is\\nI'm going to plug in x = 1. And what I get on the left-hand\\nside here is 4 - 1 and 1 + 2, and on the\\nleft-hand side I get A. That's the end. This is my formula for A. A\\nhappens to be equal to 1. And that's, of course,\\nwhat I expect. A had better be 1, because the\\nthing broke up into 1 / x - 1 + 3 / x + 2. So this is the correct answer. There was a question out\\nhere, which I missed. STUDENT: Aren't polynomials\\ndefined as functions with whole powers, or could\\nthey be square roots? PROFESSOR: Are polynomials\\ndefined as functions with whole powers, or can they\\nbe square roots? That's the question. The answer is, they only\\nhave whole powers. So for instance here I only\\nhave the power 1 and 0. Here I have the powers 2, 1\\nand 0 in the denominator. Square roots are no good\\nfor this method. Another question. STUDENT: [INAUDIBLE] PROFESSOR: Why did\\nI say x = 1? The reason why I said x = 1 was\\nthat it works really fast. You can't know this in advance, that's part of the method. It just turns out to be\\nthe best thing to do. The fastest way of getting at\\nthe coefficient A. Now the curious thing, let me\\njust pause for a second before I do it. If I had plugged x = 1 into the\\noriginal equation, I would have gotten nonsense. Because I would've gotten\\n0 in the denominator. And that seems like the most\\nhorrible thing to do. The worst possible thing\\nto do, is to set x = 1. On the other hand, what\\nwe did is a trick. We multiplied by x - 1. And that turned the equation\\ninto this. So now, in disguise,\\nI multiplied by 0. But that turns out\\nto be legitimate. Because really this equation\\nis true for all x except 1. And then instead of taking\\nx = 1, I can really take x tends to 1. That's really what I need. The limit is x goes to one. The equation is still\\nvalid then. So I'm using the worst case, the\\ncase that looks like it's dividing by 0. And it's helping me because it's\\ncancelling out all the information in terms of B. So\\nthe advantage here is this cancellation that occurs\\nin this part. So that's the method. We're going to shorten it much,\\nmuch more in a second. But let me carry it out for the\\nother coefficient as well. So the other coefficient I'm\\ngoing to solve for B, I'm going to multiply by x + 2. And when I do that, I get 4x\\n- 1 / x - 1, that's the left-hand side, the very\\ntop expression there. And then down below I get\\nA/ ( x - 1)( x + 2). And then again the\\nx + 2's cancel. So I get B sitting alone. And now I'm going to\\ndo the same trick. I'm going to set x = - 2. That's the value which\\nis going to knock out this A term here. So that cancels this\\nterm completely. And what we get here all told\\nis - 8 - 1 / - 2 - 1 = B. In other words, B = 3, which\\nwas also what it was supposed to be. B was this number\\n3, right here. Which I'm now not going\\nto change to 2. Because I know that\\nit's not 2. There was a question. STUDENT: [INAUDIBLE] PROFESSOR: All right. Now, this is the method which\\nis called cover-up. But it's really carried out\\nmuch, much faster than this. So I'm going to review the\\nmethod and I'm going to show you what it is in general. So the first step is to factor\\nthe denominator, Q. That's what I labeled 1 over there. That was the factorization of\\nthe denominator up top. The second step is what I'm\\ngoing to call the setup. That's step 2. And that's where I knew what I\\nwas aiming for in advance. And I'm going to have to\\nexplain to you in every instance exactly what this\\nsetup should be. That is, what the unknowns\\nshould be and what target, simplified expression,\\nwe're aiming for. So that's the setup. And then the third step is what\\nI'll now call cover-up. Which is just a very fast way\\nof doing what I did on this last board, which is solving for\\nthe unknown coefficients. So now, let me perform\\nit for you again. Over here. So it's 4x - 1 divided\\nby, so this is to eliminate writing here. Handwriting it makes\\nit much faster. So this part just factoring the\\ndenominator, that was 1, that was step 1. And then step 2, again,\\nis the setup, which is setting it up like this. Alright, that's the setup. And now I claim that without\\nwriting very much, I can figure out what A and B are. Just by staring at this. So now what I'm going to do is\\nI'm just going to think what I did over there. And I'm just going to\\ndo it directly. So let me show you what the\\nmethod consists of visually. I'm going to cover up, that is,\\nknock out this factor, and focus on this number here. And I'm going to plug in the\\nthing that makes the 0, which is x = 1. So I'm plugging in x = 1. To this left-hand side. And what I get is 4 - 1 / 1 +\\n2 = A. Now, that's the same thing I did over there. I just did it by skipping the\\nintermediate algebra step, which is a lot of writing. So the cover-up method\\nreally amounts to the following thing. I'm thinking of multiplying\\nthis over here. It cancels this and it gets\\nrid of everything else. And it just leaves me with\\nA on the right-hand side. And I have to get rid\\nof it on this side. So in other words, by\\neliminating this, I'm isolating a on the\\nright-hand side. So the cover-up is that\\nI'm covering this and getting A out of it. Similarly, I can do the same\\nthing with B. It's focused on the value x = - 2. And b is what I'm getting\\non the right-hand side. And then I have to\\ncover up this. So if I cover up that, then\\nwhat's left over with x = - 2 is again, - 8 - 1 / - 2 - 1. So this is the way the method\\ngets carried out in practice. Writing, essentially,\\nthe least you can. Now, when you get to several\\nvariables, it becomes just way more convenient to do this. So now, let me just review\\nwhen cover-up works. So this cover-up method\\nworks if Q ( x) has distinct linear factors. And, so you need two\\nthings here. It has to factor completely, the\\ndenominator has to factor completely. And the degree of the numerator\\nhas to be strictly less than the degree\\nof the denominator. I'm going to give you\\nan example here. So, for instance, and this tells\\nyou the general pattern of the setup also. Say you had x ^2 + 3x\\n+ 8, let's say. Over (x - 1) ( x - 2)( x + 5). So here I'm going to\\ntell you the setup. The setup is going to be\\nA / (x - 1) + B / (x - 2) + c / x + 5. And it will always break\\nup into something. So however many factors you\\nhave, you'll have to put in a term for each of those. And then you can find\\neach number here by this cover-up method. Now we're done with that. And now we have to go on to the\\nalgebraic complications. So would the first typical\\nalgebraic complication be. It would be repeated roots\\nor repeated factors. Let me get one that doesn't\\ncome out to be extremely ugly here. So this is what we'll\\ncall Example 2. And this is going to work when\\nthe degree, you always need that the degree of the numerator\\nis less than the degree of the denominator. And Q has now repeated\\nlinear factors. So let's see which example\\nI wanted to show you. So let's just give this here. I'll just repeat the\\ndenominator. With an extra factor\\nlike this. Now, the main thing you need\\nto know, since I've already performed the factorization\\nfor you. Already performed Step 1. This is Step 1 here. You have to factor things all\\nthe way, and that's already been done for you. And here's what this setup is. The setup is that it's of\\nthe form A / (x - 1) + B / (x - 1) ^2. We need another term for the\\nsquare here. + C / (x + 2). In general, if you have more\\npowers you just need to keep on putting in those powers. You need one for each\\nof the powers. Why does it have to be squared? OK. Good question. So why in the world\\nam I doing this? Let me just give you one hint\\nas to why I'm doing this. It's very, very much like the\\ndecimal expansion of a number or, say, the base 2 expansion\\nof a number. So, for, example the number 7/16\\nis 0 / 2 + 1 / 2 ^2 + 1/2 ^3 +, is that right? So it's 4/16 + 1/2 ^4. It's this sort of thing. And I'm getting this power\\nand this power. If I have higher powers,\\nI'm going to have to have more and more. So this is what happens\\nwhen I have a 2 ^ 4. I have to represent\\nthings like this. That's what's coming out\\nof this piece with the repetitious here. Of the powers. This is just an analogy. Of what we're doing. Yeah, another question\\nover here. STUDENT: [INAUDIBLE] PROFESSOR: Yes. So this is an example, but it's\\nmeant to represent the general case and I will also\\ngive you a general picture. For sure, once you have the\\nsecond power here, you'll need both the first and the second\\npower mentioned over here. And since there's only a first\\npower over here I only have to mention a first power\\nover there. If this were a 3 here, there\\nwould be one more term which would be the one for x - 1\\n^2 in the denominator. That's what you just said. OK, now, what's different about\\nthis setup is that the cover-up method, although\\nit works, it doesn't work so well. It doesn't work quite as well. The cover-up works for the\\ncoefficients B and C, not A. We'll have a quick method for\\nthe numbers B and C. To figure out what they are. But it will be a little slower\\nto get to A, which we will do last. Let me show you\\nhow it works. First of all, I'm going to do\\nthe ordinary cover-up with C. So for C, I just want\\nto do the same old thing that I did before. I cover up this, and that's\\ngoing to get rid of all the junk except for the C term. So I have to plug x = - 2. And I get x -- sorry, I get (-2\\n) ^2 + 2 in the numerator. And I get (- 2 - 1)^2\\nin the denominator. Remember I'm covering this up. So that's all there is on\\nthe left-hand side. And on the right-hand side all\\nthere is C. Everything else got killed off, because it\\nwas x - 2 times that. That's 0 times all\\nthat other stuff. And the x - 2 over\\nhere canceled. This is the shortcut that I just\\ndescribed, and this is much faster than doing\\nall that arithmetic. And algebra. So all together this\\nis a 6/9, right? So it's C = 6/9, which is 2/3. Now, the other one which is easy\\nto do, I'm going to do by the slow method first.\\nBut you omit a term. The idea is to cover up\\nthe other bad factor. Cover ups, I'll do it both\\nthe way and the slow way. I'll do it the fast way first,\\nand then I'll show you the slow way. The fast way is to\\ncover this up. And then I have to cover\\nup everything else. That gets eliminated. And that includes everything\\nbut B. So I get B on this side. And I get 1 on that side. So that's 1 ^2 + 2 / 1 + 2. So in other words, B = 1. That was pretty fast, so let\\nme show you what arithmetic was hiding behind that. What algebra was hiding\\nbehind it. What I was really\\ndoing is this. In multiplying through by\\nx - 1 ^2, so I got this. So this canceled here, so this\\nC just stands alone. And then I have here C\\n/x + 2 (x - 1) ^2. Notice again, I cleared out\\nthat 1, this term from the denominator and sent it over\\nto the other side. Now, what's happening is that\\nwhen I set x = 1 here, this term is dying. This term is going away, because\\nthere's more powers in the numerator than in\\nthe denominator. This is still 0. And this one is gone also. So all that's left is B. Now, I\\ncannot pull that off with a single power of x - 1. I can't expose the A term. It's the B term that\\nI can expose. Because I can multiply through\\nby this thing squared. If I multiply through by just x\\n- 1, what'll happen here is I won't have canceled\\nthis (x - 1 )^2. It's useless. I still have a 0 in\\nthe denominator. I'll have B / 0 when\\nI plug in x = 1. Which I can't use. Again, the cover-up method is\\ngiving us B and C, not A. Now, for the last term, for A,\\nI'm going to just have to be straightforward about it. And so I'll just suggest\\nfor A, plug in your favorite number. So plug in my favorite number. Which is x = 0. And you won't be able to\\nplug in x = 0 if you've already used it. Here the two numbers we've\\nalready used are x = 1 and x = - 2. But we haven't used x =\\n0 yet, so that's good. I'm going to plug in now x\\n= 0 into the equation. What do I get? I get 0 ^2 + 2 / (- 1) ^2 *\\n2 is equal to, let's see. A is the thing that\\nI don't know. So it's A / - 1 +, B /\\nx - 1 ^2 so B = 1, so that's 1 / (- 1) ^2. And then C was 2/3. 2/3 / x + 2. So that's 0 + 2. Don't give up at this point. This is a lot of algebra. You really have to plug\\nin all these numbers. You make one arithmetic mistake\\nand you're always going to get the wrong answer. This is very arithmetically\\nintensive. However, it does simplify\\nat this point. We have 2 / 2, that's 1. Is equal to - A + 1 + 1/3. So let's see. A on the other side, this\\nbecomes A = 1/3. And that's it. This is the end. We've we've simplified\\nour function. And now it's easy to integrate. Question. Another question. STUDENT: [INAUDIBLE] PROFESSOR: So the question is,\\nif x = 0 has already been used, what do I do? And the answer is, pick\\nsomething else. And you said pick\\na random number. And that's right, except that if\\nyou really picked a random number it would be 4.12567843,\\nwhich would be difficult. What you want to pick is the\\neasiest possible number you can think of. Yeah. STUDENT: [INAUDIBLE] PROFESSOR: If you had, as in\\nthis sort of situation here. More powers. Wouldn't you have\\nmore variables. Very good question. That's absolutely right. This was a 3 by 3 system in\\ndisguise, for these three unknowns, A, B and C. What we\\nstarted with in the previous problem was two variables. It's over here, the variables A\\nand B. And as the degree of the denominator goes up, the\\nnumber of variables goes up. It gets more and more and\\nmore complicated. More and more arithmetically\\nintensive. STUDENT: [INAUDIBLE] PROFESSOR: Well, so. The question is, how would\\nyou solve it if you have two unknowns. That's exactly the point here. This is a system\\nof simultaneous equations for unknowns. And we have little tricks for\\nisolating single variables. Otherwise we're stuck with\\nsolving the whole system. And you'd have to solve the\\nwhole system by elimination, various other tricks. I'll say a little more\\nabout that later. Now, I have to get one\\nstep more complicated with my next example. My next example is going to\\nhave a quadratic factor. So still I'm sticking to the\\ndegree of the polynomial and the numerator is less than the\\ndegree of the polynomial in the denominator. And I'm going to take\\nthe case where Q has a quadratic factor. Let me just again illustrate\\nthis by example. I have here (x - 1) (x^2 + 1). I'll make it about as\\neasy as they come. Now, the setup will be slightly\\ndifferent here. Here's the setup. It's already factored. I've already done as\\nmuch as I can do. I can't factor this x^2 + 1 into\\nlinear factors unless you know about complex numbers. If you know about complex\\nnumbers this method becomes much easier. And it comes back to the\\ncover-up method. Which is the way that the\\ncover-up method was originally conceived by heavy side. But you won't get to\\nthat until 18.03. So we'll wait. This, by the way, is a method\\nwhich is used for integration. But it was invented to do\\nsomething with Laplace transforms and inversion\\nof certain kinds of differential equations. By heavy side. And so it came much later\\nthan integration. But anyway, it's a very\\nconvenient method. So here's the set up\\nwith this one. Again, we want a term for\\nthis (x - 1) factor. And now we're going to also\\nhave a term with the denominator x squared plus 1. But this is the difference. It's now going to be a first\\ndegree polynomial. One degree down from\\nthe quadratic here. So this is what I keep\\non calling the setup, this is number 2. You have to know that in advance\\nbased on the pattern that you see on the\\nleft-hand side. Yes. STUDENT: [INAUDIBLE] PROFESSOR: The question is, if\\nthe degree of the numerator. So in this case, if this were\\ncubed, and this is matching with the denominator, which\\nis total of degree 3. The answer is that this\\ndoes not work. STUDENT: [INAUDIBLE] PROFESSOR: It definitely\\ndoesn't work. And we're going to have to\\ndo something totally different to handle it. Which turns out, fortunately,\\nto be much easier than this. But we'll deal with\\nthat at the end. Keep this in mind. This is an easy way to make a\\nmistake if you start with a higher degree numerator. You'll never get the\\nright answer. So now, so I have\\nmy setup now. And now what can I do? Well, I claim that I can still\\ndo cover-up for A. It's the same idea. I cover this guy up. And if I really multiply by it\\nit would knock everything out but A. So I cover this up\\nand I plug in x = 1. So I get here 1 ^2 / 1 ^2 + 1 =\\nA. In other words, A = 1/2. Again cover-up is pretty\\nfast, as you can see. It's not too bad. Now, at this next stage, I want\\nto find B and C. And the best idea is the slow way. Here, it's not too terrible. But it's just what we're\\ngoing to do. Which is to clear the\\ndenominators completely. So for B and C, just clear\\nthe denominator. That means multiply through\\nby that whole business. Now, when you do that on the\\nleft-hand side you're going to get x ^2. Because I got rid of the\\nwhole denominator. On the right-hand side when I\\nbring this up, the x - 1 will cancel with this. So the a term will\\nbe A ( x ^2 + 1). And the Bx + C term will have\\na remaining factor of x - 1. Because the x ^2 +\\n1 will cancel. Again, the arithmetic here\\nis not too terrible. Now I'm going to do\\nthe following. I'm going to look at\\nthe x ^2 term. On the left-hand side and\\nthe right-hand side. And that will give me one\\nequation for B and C. And then I'm going to do the same thing\\nwith another term. The x^2 term on the left-hand\\nside, the coefficient is 1. It's 1 ( x ^2). On the other side, it's A.\\nremember I actually have A. So I'm going to put it\\nin, it's 1/2. So this is the A term. And so I get 1/2 ( x ^2). And then the only other\\nx^2 is when this Bx multiplies this x. So Bx * x is Bx ^2, so this is\\nthe other coefficient on x ^2 is B. And that forces\\nB to be 1/2. And last of all, I'm going to\\ndo the x^ 0 power term. Or, otherwise known as\\nthe constant term. And on the left-hand side,\\nthe constant term is 0. There is no constant term. On the right-hand side there's\\na constant term, 1/2 * 1. That's 1/2 here. And then there's another\\nconstant term, which is this constant times this - 1. Which is - C. And so\\nthe conclusion here is that C = 1/2. Another question. Yeah. STUDENT: [INAUDIBLE] PROFESSOR: There's also an x^\\n0 power hidden in here. Sorry, an x ^ 1 , that's what\\nyou were asking about, sorry. There's also an x ^ 1 . The only reason why I didn't\\ngo to the x^ 1 is that it turns out with these two\\nI didn't need it. The other thing is that by\\nexperience, I know that the extreme ends of the\\nmultiplication are the easiest ends. And the middle terms have tons\\nof cross terms. And so I don't like the middle term as much\\nbecause it always involves more arithmetic. So I stick to the lowest and\\nthe highest terms if I can. So that was really\\na sneaky thing. I did that without\\nsaying anything. Yes. STUDENT: [INAUDIBLE] PROFESSOR: Another\\ngood question. Could I just set equals 0? Absolutely. In fact, that's equivalent to\\npicking out the x^ 0 term. And you could plug in numbers. If you wanted. That's another way of doing\\nthis besides doing that. So you can also plug\\nin numbers. Can plug in numbers. x = 0. Actually, not x = 1,\\nright? - 1, 2, etc. Not 1 just because we've\\nalready used it. We won't get interesting\\ninformation out. Yes. STUDENT: [INAUDIBLE] PROFESSOR: So the question\\nis, could I have done it this other way. With the polynomial, with\\nthis other one. Yes, absolutely. So in other words what I've\\ntaught you now is two choices which are equally reasonable. The one that I picked was the\\none that was the fastest for this guy and the one that was\\nfastest for this one, but I could've done the other\\nway around. There are a lot of\\nways of solving simultaneous equations. Yeah, another question. STUDENT: [INAUDIBLE] PROFESSOR: The question\\nis the following. So now everybody can understand\\nthe question. If this, instead of being x\\n^2 + 1, this were x^3 + 1. So that's an important\\ncase to understand. That's a case in which\\nthis denominator is not fully factored. So it's an x^3 + 1, you would\\nhave to factor out an x + 1. So that would be a situation\\nlike this, you have an x^3 + 1, but that's (x + 1)( x^2 +\\nx + 1), this kind of thing. If that's the right, there\\nmust be a minus sign in here maybe. OK, something like this. Right? Isn't that what it is? STUDENT: [INAUDIBLE] PROFESSOR: I think it's right. But anyway, the point is that\\nyou have to factor it. And then you have a linear\\nand a quadratic. So you're always going to be\\nfaced eventually with linear factors and quadratic factors. If you have a cubic, that means\\nyou haven't factored sufficiently. So you're still back\\nin Step 1. STUDENT: [INAUDIBLE] PROFESSOR: In the\\nx^3 + 1 case? STUDENT: [INAUDIBLE] PROFESSOR: In the x^3 + 1 case,\\nwe are out of luck until we've completed the\\nfactorization. Once we've completed the\\nfactorization, we're going to have to deal with these two\\nfactors as denominators. So it'll be this plus something\\nover x + 1 + a Bx + C type of thing over\\nthis thing here. That's what's eventually\\ngoing to happen. But hold on to that idea. Let me carry out one\\nmore example here. So I've figured out what\\nall the values are. But I think it's also worth it\\nto remember now that we also have to carry out\\nthe integration. What I've just shown you is that\\nthe integral of x ^2 dx over (x - 1)( x ^2 + 1) is equal\\nto, and I've split up into these pieces. So what are the pieces? The pieces are, 1/2, x -\\n1 + 1/2 x / x ^2 + 1. This is the A term. This is the B term. And then there's the C term. So we'd better remember\\nthat we know how to antidifferentiate\\nthese things. In other words, I want to\\nfinish the problem. The others were pretty easy, so\\nI didn't bother to finish my sentence, but here I want\\nto be careful and have you realize that there's something\\na little more to do. First of all we have the, the\\nfirst one is no problem. That's this. The second one actually\\nis not too bad either. This is, by the advanced\\nguessing method, my favorite method, something like the\\nlogarithm, because that's what's going to appear\\nin the denominator. And then, if you differentiate\\nthis, you're going to get 2x over this. But here we have 1/2. So altogether it's\\n1/4 of this. So I fixed the coefficient\\nhere. And then the last one, you have\\nto think back to some level of memorization here and\\nremember that this is 1/2 the arc tangent. STUDENT: [INAUDIBLE] PROFESSOR: Why did\\nI go to 1/4? Because in disguise, for this\\nguy, I was thinking d / dx of ln (x ^2 + 1) = 2x / x^2 + 1. Because it's the derivative\\nof this divided by itself. This is the derivative\\nof ln u is u ' / u. Ln u' = u' / u. That was what I applied. And what I had was 1/2, so I\\nneed a total of 1/4 to cancel. So 2/4 is 1/2. Now I've got to get you out\\nof one more deep hole. And I'm going to save the\\ngeneral pattern for next time. But I do want to clarify\\none thing. So let's handle this thing. What if the degree of P is\\nbigger than or equal to the degree of Q. That's the thing\\nthat I claimed was easier. And I'm going to describe\\nto you how it's done. Now, in analogy, with numbers\\nyou might call this an improper fraction. That's the thing that should\\necho in your mind when you're thinking about this. And I'm just going to do\\nit by example here. Let's see., I cooked up an\\nexample so that I don't make an arithmetistic mistake\\nalong the way. So there are two or three steps\\nthat I need to explain. So here's an example. The denominator's degree 2,\\nthe numerator is degree 3. It well exceeds, so there's\\na problem here. Our method is not\\ngoing to work. And the first step that I\\nwant to carry out is to reverse Step 1. That is, I don't want the\\nfactorization for what I'm going to do next. I want it multiplied out. That means I have to multiply\\nthrough, so I get x ^2 + x - 2. I'm back to the starting\\nplace here. And now, the next thing that I'm\\ngoing to do is, I'm going to use long division. That's how you convert\\nan improper fraction to a proper fraction. This is something you were\\nsupposed to learn in, I don't know, Grade 4, I know. Grade 3, Grade 4, Grade\\n5, Grade 6, etc. So here's how it works in\\nthe case of polynomials. It's kind of amusing. So we're dividing this\\npolynomial into that one. And so the quotient to first\\norder here is x. That is, that's going to match\\nthe top terms. So I get x^3 + x ^2 - 2x. That's the product. And now I subtract. And it cancels. So we get here - x ^2 + 2x. That's the difference. And now I need to divide\\nthis next term in. And I need a - 1. So - 1 times this is\\n- x ^2 - x + 2. And I subtract. And the x^2's cancel. And here I get + 3x - 2. Now, this thing has a name. This is called the quotient. And this thing also\\nhas a name. This is called the remainder. And now I'll show you how it\\nworks by sticking it into the answer here. The quotient is x - 1. And the remainder is, let's\\nget down there. 3x - 2 / x ^2 + x - 2. So the punchline here\\nis that this thing is easy to integrate. This is easy. And this one, you can use, now\\nyou can use cover-up, The method that we had before. Because the degree of the\\nnumerator is now below the degree of the denominator. It's now first degree and\\nthis is second degree. What you can't do is\\nuse cover-up to start out with here. That will give you\\nthe wrong answer. So that's the end for today,\\nand see you next time.\",\n          \"The following content is\\nprovided under a Creative Commons license. Your support will help\\nMIT OpenCourseWare continue to offer high quality\\neducational resources for free. To make a donation or to\\nview additional materials from hundreds of MIT courses,\\nvisit MIT OpenCourseWare at ocw.mit.edu. PHILIPPE RIGOLLET:\\nWe keep on talking about principal component\\nanalysis, which we essentially introduced as a way to\\nwork with a bunch of data. So the data that's given to\\nus when we want to do PCA is a bunch of vectors X1 to Xn. So they are random vectors. in Rd. And what we mentioned\\nis that we're going to be using linear\\nalgebra-- in particular, the spectral theorem-- that\\nguarantees to us that if I look at the convenience\\nmatrix of this guy, or its empirical\\ncovariance matrix, since they're\\nsymmetric real matrices and they are positive\\nsemidefinite, there exists a diagonalization\\ninto non-negative eigenvalues. And so here, those\\nthings live in Rd, so it's a really large space. And what we want to\\ndo is to map it down into a space that\\nwe can visualize, hopefully a space\\nof size 2 or 3. Or if not, then we're just going\\nto take more and start looking at subspaces altogether. So think of the case where d\\nis large but not larger than n. So let's say, you have a\\nlarge number of points. The question is, is it possible\\nto project those things onto a lower dimensional\\nspace, d prime, which is much less than d-- so\\nthink of d prime equals, say, 2 or 3-- and so that you keep\\nas much information about the cloud of points\\nthat you had originally. So again, the example\\nthat we could have is that X1 to Xn for, say,\\nXi for patient i's recording a bunch of body measurements\\nand maybe blood pressure, some symptoms, et cetera. And then we have a\\ncloud of n patients. And we're trying to\\nvisualize maybe to see if-- If I could see, for\\nexample, that there's two groups of\\npatients, maybe I would know that I have two\\ngroups of different disease or maybe two groups\\nof different patients that respond differently\\nto a particular disease or drug et cetera. So visualizing is\\ngoing to give us quite a bit of insight about\\nwhat the spatial arrangement of those vectors are. And so PCA says, well, here,\\nof course, in this question, one thing that's not defined\\nis what is information. And we said that\\none thing we might want to do when we project\\nis that points do not collide with each other. And so that means we're\\ntrying to find directions, where after I project, the\\npoints are still pretty spread out. And so I can see\\nwhat's going on. And PCA says-- OK,\\nso there's many ways to answer this question. And PCA says, let's just\\nfind a subspace of dimension d prime that keeps as much\\ncovariance structure as possible. And the reason is\\nthat those directions are the ones that maximize\\nthe variance, which is a proxy for the spread. There's many, many\\nways to do this. There's actually a\\nGoogle video that was released maybe last week\\nabout the data visualization team of Google that shows\\nyou something called t-SNE, which is\\nessentially something that tries to do that. It takes points in\\nvery high dimensions and tries to map them\\nin lower dimensions, so that you can\\nactually visualize them. And t-SNE is some\\nalternative to PCA that gives an other definition\\nfor the word information. I'll talk about this towards\\nthe end, how you can actually somewhat automatically\\nextend everything we've said for PCA to an\\ninfinite family of procedures. So how do we do this? Well, the way we do\\nthis is as follows. So remember, given\\nthose guys, we can form something which is\\ncalled S, which is the sample, or the empirical\\ncovariance matrix. And since from\\ncouple of slides ago, we know that S has a\\neigenvalue decomposition, S is equal to PDP transpose,\\nwhere P is orthogonal. So that's where we use our\\nlinear algebra results. So that means that P transpose P\\nis equal to PP transpose, which is the identity. So remember, S is\\na d by d matrix. And so P is also d by d. And d is diagonal. And I'm actually going to take,\\nwithout loss of generality, I'm going to assume that d-- so it's going to be\\ndiagonal-- and I'm going to have something\\nthat looks like lambda 1 to lambda d. Those are called the\\neigenvalues of S. What we know is that lambda\\nj's are non-negative. And actually, what I'm\\ngoing to assume without loss of generalities is lambda 1\\nis larger than lambda 2, which is larger than lambda d. Because in particular,\\nthis decomposition-- the spectrum decomposition--\\nis not entirely unique. I could permute\\nthe columns of P, and I would still have\\nan orthogonal matrix. And to balance that,\\nI would also have to permute the entries of d. So there's as many\\ndecompositions as there are permutations. So there's actually quite a bit. But the bag of\\neigenvalues is unique. The set of\\neigenvalues is unique. The ordering is\\ncertainly not unique. So here, I'm just\\ngoing to pick-- I'm going to nail down one\\nparticular permutation-- actually, maybe two in\\ncase I have equalities. But let's say, I pick\\none that satisfies this. And the reason why I do this\\nis really not very important. It's just to say,\\nI'm going to want to talk about the largest\\nof those eigenvalues. So this is just\\ngoing to be easier for me to say that\\nthis one is lambda 1, rather than say it's lambda 7. So this is just to say that\\nthe largest eigenvalue of S is lambda 1. If I didn't do that, I would\\njust call it maybe lambda max, and you would just know\\nwhich one I'm talking about. So what's happening now\\nis that if I look at d, then it turns out\\nthat if I start-- so if I do P transpose Xi, I am\\nactually projecting my Xi's-- I'm basically changing\\nthe basis for my Xi's. And now, D is the\\nempirical covariance matrix of those guys. So let's check that. So what it means is\\nthat if I look at-- so what I claim is\\nthat P transpose Xi-- that's a new vector, let's\\ncall it Yi, it's also in Rd-- and what I claim is that the\\ncovariance matrix of this guy is actually now this\\ndiagonal matrix, which means in particular that\\nif they were Gaussian, then they would be independent. But I also know now that\\nthere's no correlation across coordinates of Yi. So to prove this, let me assume\\nthat X bar is equal to 0. And the reason why I do\\nthis is because it's just annoying to carry out all\\nthis censuring constantly and I talk about S. So\\nwhen X bar is equal to 0, that implies that S\\nhas a very simple form. It's of the form\\nsum from i equal 1 to n of Xi Xi transpose. So that's my S. But what I want is the S of Y-- So OK, that implies\\nalso that P times X bar, which is equal to P times\\nX bar is also equal to 0. So that means that Y bar-- Y has mean 0, if this is 0. So if I look at the sample\\ncovariance matrix of Y, it's just going to\\nbe something that looks like the sum of the\\nouter products or the Yi Yi transpose. And again, the reason why\\nI make this assumption is so that I don't have to write\\nminus X bar X bar transpose. But you can do it. And it's going to\\nwork exactly the same. So now, I look at this S prime. And so what is this S prime? Well, I'm just going\\nto replace Yi with PXi. So it's the sum from i equal\\n1 to n of PXi PXi transpose, which is equal to the sum from-- sorry there's a 1/n. So it's equal to 1/n\\nsum from i equal 1 to n of PXi Xi\\ntranspose P transpose. Agree? I just said that the transpose\\nof AB is the transpose of B times the transpose of A. And so now, I can\\npush the sum in. P does not depend on i. So this thing here is\\nequal to PS P transpose, because the sum of the Xi Xi\\ntranspose divided by n is S. But what is PS P transpose? Well, we know that\\nS is equal to-- sorry that's P transpose. So this was with a P transpose. I'm sorry, I made an\\nimportant mistake here. So Yi is P transpose Xi. So this is P transpose\\nand P transpose here, which means that\\nthis is P transpose and this is double transpose,\\nwhich is just nothing and that transpose and nothing. So now, I write S\\nas PD P transpose. That's the spectral\\ndecomposition that I had before. That's my eigenvalue\\ndecomposition, which means that now,\\nif I look at S prime, it's P transpose times\\nPD P transpose P. But now, P transpose\\nP is the identity, P transpose P is the identity. So this is actually\\njust equal to D. And again, you can\\ncheck that this also works if you have to center\\nall those guys as you go. But if you think about\\nit, this is the same thing as saying that I just\\nreplaced Xi by Xi minus X bar. And then it's true that Y bar\\nis also P times Xi minus X bar. So now, we have that D is\\nthe empirical covariance matrix of those guys-- the Yi's, which are\\nP transpose Xi's. And so in particular,\\nwhat it means is that if I look at the\\ncovariance of Yj Yk-- So that's the covariance\\nof the j-th coordinate of Y and the k-th coordinate of Y.\\nI'm just not putting an index. But maybe, let's say the\\nfirst one or something like this-- any of\\nthem, their IID. Then what is this covariance? It's actually 0 if j\\nis different from k. And the covariance\\nbetween Yj and Yj, which is just the variance\\nof Yj, is equal to lambda j-- the j-th largest eigenvalue. So the eigenvalues capture the\\nvariance of my observations in this new coordinate system. And they're\\ncompletely orthogonal. So what does that mean? Well, again, remember,\\nif I chop off the head of my Gaussian\\nin multi dimensions, we said that what\\nwe started from was something that\\nlooked like this. And we said, well, there's one\\ndirection that's important, that's this guy, and one\\nimportant that's this guy. When I applied a transformation\\nP transpose, what I'm doing is that I'm realigning this\\nthing with the new axes. Or in a way, rather\\nto be fair, I'm not actually realigning\\nthe ellipses with the axes. I'm really realigning the\\naxes with the ellipses. So really, what I'm doing is\\nI'm saying, after I apply P, I'm just rotating this\\ncoordinate system. So now, it becomes this guy. And now, my ellipses\\nactually completely align. And what happens here is\\nthat this coordinate is independent of that coordinate. And that's what we write\\nhere, if they are Gaussian. I didn't really tell this-- I'm only making statements\\nabout covariances. If they are Gaussians,\\nthose implied statements about independence. So as I said, the\\nvariance now, lambda 1, is actually the variance\\nof P transpose Xi. But if I look now at\\nthe-- so this is a vector, so I need to look at the\\nfirst coordinate of this guy. So it turns out that\\ndoing this is actually the same thing as looking\\nat the variance of what? Well, the first\\ncolumn of P times Xi. So that's the variance of-- I'm going to call it v1\\ntranspose Xi, where P-- So the v1 vd in Rd\\nare eigenvectors. And each vi is\\nassociated to lambda i. So that's what we saw when\\nwe talked about this eigen decomposition a\\ncouple of slides back. That's the one here. So if I call the\\ncolumns of P v1 to vd, this is what's happening. So when I look at lambda\\n1, it's just the variance of Xi inner product with v1. And we made this picture\\nwhen we said, well, let's say v1 is here\\nand then x1 is here. And if vi has a unique\\nnorm, then the inner product between Xi and v1 is just\\nthe length of this guy here. So that's the variance of the\\nXi says the length of Xi-- so this is 0-- that's the\\nlength of Xi when I project it onto the direction\\nthat span by v1. If v1 has length 2, this is\\nreally just twice this length. If vi has length 3,\\nit's three times this. But it turns out that since\\nP satisfies P transpose P is equal to the identity-- that's an orthogonal\\nmatrix, that's right here-- then this is actually\\nsaying the same thing as vj transpose vj, which is\\nreally the norm squared of vj, is equal to 1. And vj transpose vk is equal\\nto 0, if j is different from k. The eigenvectors are\\northogonal to each other. And they're actually\\nall of norm 1. So now, I know that this\\nis indeed a direction. And so when I look\\nat v1 transpose Xi, I'm really measuring\\nexactly this length. And what is this length? It's the length of\\nthe projection of Xi onto this line. That's the line\\nthat's spanned by v1. So if I had a very high\\ndimensional problem and I started to look\\nat the direction v1-- let's say v1 now is\\nnot a eigenvector, it's any direction-- then\\nif I want to do this lower dimensional projection, then\\nI have to understand how those Xi's project onto the\\nline that's spanned by v1, because this is all that I'm\\ngoing to be keeping at the end of the day about Xi's. So what we want is\\nto find the direction where those Xi's,\\nthose projections, have a lot of variance. And we know that the variance\\nof Xi on this direction is actually exactly\\ngiven by lambda 1. Sorry, that's the\\nempirical var-- yeah, I should\\ncall variance hat. That's the empirical variance. Everything is in empirical here. We're talking about the\\nempirical covariance matrix. And so I also have that lambda\\n2 is the empirical variance of when I project Xi onto\\nv2, which is the second one, just for exactly this reason. Any question? So lambda j's are going\\nto be important for us. Lambda j measure the\\nspread of the points when I project them onto a\\nline which is a one dimensional space. And so I'm going to have-- let's\\nsay I want to pick only one, I'm going to have to find the\\none dimensional space that carries the most variance. And I claim that\\nv1 is the one that actually maximizes the spread. So the claim-- so for\\nany direction, u in Rd-- and by direction, I really\\njust mean that the norm of u is equal to 1. I need to play fair-- I'm going to compare myself to\\nother things of lengths one, so I need to play fair and\\nlook at directions of length 1. Now, if I'm interested\\nin the empirical variance of X1 transpose-- sorry, u transpose X1 u\\ntranspose Xn, then this thing is maximized for\\nu equals v1, where v1 is the eigenvector\\nassociated to lambda 1 and lambda 1 is not\\nany eigenvalues, it's the largest of all those. So it's the largest eigenvalue. So why is that true? Well, there's also a claim\\nthat for any direction u-- so that's 1 and 2-- the variance of u\\ntranspose X-- now, this is just a random variable,\\nand I'm looking about the true variance-- this is maximized for u\\nequals, let's call it w1, where w1 is the\\neigenvector of sigma-- Now, I'm talking about\\nthe true variance. Whereas, here, I was talking\\nabout the empirical variance. So the true variance\\nis the eigenvectors of the true sigma\\nassociated to the largest eigenvalue of sigma. So I did not give it a name. Here, that was lambda 1\\nfor the empirical one. For the true one,\\nyou can give it another name, mu 1 if you want. But that's just the same thing. All it's saying is like,\\nwherever I see empirical, I can remove it. So why is this claim true? Well, let's look at the\\nsecond one, for example. So what is the variance\\nof u transpose X? So that's what I want to know. So that's the expectation--\\nso let's assume that X is 0, again, for same\\nreasons as before. So what is the variance? It's just the expectation\\nof the square? I don't need to remove\\nthe expectation. And the expedition\\nof the square is just the expectation\\nof u transpose X. And then I'm going to write\\nthe other one X transpose u. And we know that this\\nis deterministic. So I'm just going to take\\nthat this is just u transpose expectation of X X transpose u. And what is this guy? That's covariance sigma. That's just what sigma is. So the variance I can write\\nas u transpose sigma u. We've made this\\ncomputation before. And now what I want to claim\\nis that this thing is actually less than the largest\\neigenvalue, which I actually called lambda 1 here. I should probably not. And the P is-- well, OK. Let's just pretend\\neverything is not empirical. So now, I'm going to write\\nsigma as P lambda 1 lambda n P transpose. That's just the\\neigendecomposition, where I admittedly reuse the\\nsame notation as I did for S. So I should really put\\nsome primes everywhere, so you know those are\\nthings that are actually different in practice. So this is just that the\\ndecomposition of sigma. You seem confused, Helen. You have a question? Yeah? AUDIENCE: What is-- when you\\ntalked about the empirical data and-- PHILIPPE RIGOLLET: So OK-- so I can make\\neverything I'm saying, I can talk about\\neither the variance or the empirical variance. And you can just add the\\nword empirical in front of it whenever you want. The same thing works. But just for the sake of\\nremoving the confusion, let's just do it again\\nwith S. So I'm just going to do everything\\nwith S. So I'm going to assume that\\nX bar is equal to 0. And here, I'm going to talk\\nabout the empirical variance, which is just 1/n\\nsum from i equal 1 to n of u transpose Xi squared. So it's the same thing. Everywhere you see\\nan expectation, you just put in average. And then I get 1/n\\nsum from i equal 1 to n of Xi Xi transpose. And now, I'm going\\nto call this guy S, because that's what it is. So this is u transpose Su. But just defined that I could\\njust replace the expectation by averages everywhere,\\nyou can tell that the thing is going to work\\nfor either one or the other. So now, this thing\\nwas actually-- so now, I don't have any problem\\nwith my notation. This is actually the\\ndecomposition of S. That's just the\\nspectral decomposition and it's to its eigenvalues. And so now, what I have is that\\nwhen I look at u transpose Su, this is actually equal\\nto P u transpose S Pu. OK. There's a transpose somewhere. That's this guy. And that's this guy. Now-- sorry, that's\\nnot P, that's D. That's D, that's\\nthis diagonal matrix. Let's look at this thing. And let's call P transpose\\nu, let's call it b. So that's also a vector in Rd. What is it? It's just, I take a\\nunit vector, and then I apply P transpose to it. So that's basically what\\nhappens to a unit vector when I apply the same\\nchange of basis that I did. So I'm just changing my\\northogonal system the same way I did for the other ones. So what's happening\\nwhen I write this? Well, now I have that u\\ntranspose Su is b transpose Db. But now, doing b transpose\\nDb when D is diagonal and b is a vector is\\na very simple thing. I can expand it. This is what? This is just the\\nsum from j equal 1 to d of lambda j bj squared. So that's just like matrix\\nvector multiplication. And in particular, I know\\nthat the largest of those guys is lambda 1 and those\\nguys are all non-negative. So this thing is actually\\nless than lambda 1 times the sum from j equal 1 to\\nd of lambda j squared-- sorry, bj squared. And this is just the\\nnorm of b squared. So if I want to prove what's on\\nthe slide, all I need to check is that b has norm, which is-- AUDIENCE: 1. PHILIPPE RIGOLLET: At most, 1. It's going to be at most 1. Why? Well, because b is really\\njust a change of basis for u. And so if I take a vector,\\nI'm just changing its basis. I'm certainly not\\nchanging its length-- think of a rotation,\\nand I can also flip it, but think of a rotation-- well, actually, for vector, it's\\njust going to be a rotation. And so now, what\\nI have I just have to check that the norm of\\nb squared is equal to what? Well, it's equal to the norm\\nof P transpose u squared, which is equal to u\\ntranspose P P transpose u. But P is orthogonal. So this thing is actually\\njust the identity. So that's just u\\ntranspose u, which is equal to the norm u\\nsquared, which is equal to 1, because I took u to have\\nnorm 1 in the first place. And so this-- you're right--\\nwas actually of norm equal to 1. I just needed to have\\nit less, but it's equal. And so what I'm left with is\\nthat this thing is actually equal to lambda 1. So I know that for\\nevery u that I pick-- that has norm-- So I'm just reminding\\nyou that u here has norm squared equal to 1. For every u that I\\npick, this u transpose Su is at mostly lambda 1. So that's the u transpose\\nSu is at most lambda 1. And we know that that's\\nthe variance, that's the empirical variance,\\nwhen I project my points onto direction spanned by u. So now, I have an\\nempirical variance, which is at most lambda 1. But I also know that if I take u\\nto be something very specific-- I mean, it was on\\nthe previous board-- if I take u to be\\nequal to v1, then this thing is actually\\nnot an inequality, this is an equality. And the reason is, when I\\nactually take u to be v1, all of these bj's are going to\\nbe 0, except for the one that's b1, which is itself equal to 1. So I mean, we can\\nbriefly check this. But if I take v-- if u is equal to v1, what\\nI have is that u transpose Su is equal to P transpose\\nv1 D P transpose v1. But what is P transpose v1? Well, remember P\\ntranspose is just the matrix that has\\nvectors v1 transpose here, v2 transpose here, all the\\nway to vd transpose here. And we know that when I take\\nvj transpose vk, I get 0, if j is different from k. And if j is equal to k, I get 1. So P transpose v1\\nis equal to what? Take v1 here and multiply it. So the first coordinate\\nis going to be v1 transpose v1, which is 1. The second coordinate\\nis going to be v2 transpose v1, which is 0. And so I get 0's\\nall the way, right? So that means that this\\nthing here is really just the vector 1, 0, 0. And here, this is just\\nthe vector 1, 0, 0. So when I multiply\\nit with this guy, I am only picking up\\nthe top left element of D, which is lambda 1. So for every one,\\nit's less lambda 1. And for v1, it's\\nequal to lambda 1, which means that it's\\nmaximized for a equals v1. And that's where\\nI said that this is the fanciest non-convex\\nproblem we know how to solve. This was a problem that\\nwas definitely non-convex. We were maximizing a convex\\nfunction over a sphere. But we know that v1,\\nwhich is something-- I mean, of course,\\nyou still have to believe me that\\nyou can compute the spectral decomposition\\nefficiently-- but essentially, if you've\\ntaken linear algebra, you know that you can\\ndiagonalize a matrix. And so you get that v1\\nis just the maximum. So you can find your\\nmaximum just by looking at the spectral decomposition. You don't have to\\ndo any optimization or anything like this. So let's recap. Where are we? We've established\\nthat if I start with my empirical covariance\\nmatrix, I can diagonalize it and PD P transpose. And then if I take the\\neigenvector associated to the largest eigenvalues-- so\\nif I permute the columns of P and of D's in such\\na way that they are ordered from the\\nlargest to the smallest when I look at the diagonal\\nelements of D, then if I pick the first\\ncolumn of P, it's v1. And v1 is the direction on\\nwhich, if I project my points, they are going to carry the\\nmost empirical variance. Well, that's a good way. If I told you,\\npick one direction along which if you were\\nto project your points they would be as spread out\\nas possible, that's probably the one you would pick. And so that's exactly\\nwhat PCA is doing for us. It says, OK, if you ask me\\nto take d prime equal to 1, I will take v1. I will just take the direction\\nthat's spanned by v1. And that's just when I come\\nback to this picture that was here before, this is v1. Of course, here, I\\nonly have two of them. So v2 has to be this\\nguy, or this guy, or I mean or this thing. I mean, I don't know\\nthem up to sine. But then if I have three-- think of like an olive\\nin three dimensions-- then maybe I have one\\ndirection that's slightly more elongated than the other one. And so I'm going to\\npick the second one. And so the procedure is\\nto say, well, first, I'm going to pick v1 the same way\\nI pick v1 in the first place. So the first\\ndirection I am taking is the leading eigenvector. And then I'm looking\\nfor a direction. Well, if I found\\none-- the one I'm going to want to find-- if you\\nsay you can take d equal 2, you're going to need\\nthe basis for this guy. So the second one has to be\\northogonal to the first one you've already picked. And so the second\\none you pick is the one that's just,\\namong all those that are orthogonal to v1, maximized\\nthe empirical variance when you project onto it. And it turns out that this\\nis actually exactly v2. You don't have to\\nredo anything again. You're eigendecomposition,\\nthis is just the second column\\nof P. Clearly, v2 is orthogonal to v1. We just used it here. This 0 here just says this\\nv2 is orthogonal to v1. So they're like this. And now, what I said-- what this slide\\ntells you extra-- is that v2 among all\\nthose directions that are orthogonal-- I mean, there's still\\nd minus 1 of them-- this is the one that\\nmaximizes the, say, residual empirical\\nvariance-- the one that was not explained by the first\\ndirection that you picked. And you can check that. I mean, it's becoming a bit\\nmore cumbersome to write down, but you can check that. If you're not convinced,\\nplease raise your concern. I mean, basically, one\\nway you view this to-- I mean, you're not really\\ndropping a coordinate, because v1 is not a coordinate. But let's assume actually for\\nsimplicity that v1 was actually equal to e1, that the direction\\nthat carries the most variance is the one that\\njust says, just look at the first coordinate of X.\\nSo if that was the case, then clearly the orthogonal\\ndirections are the ones that comprise only\\nof the coordinates 2 to d. So you could actually just\\ndrop the first coordinate and do the same thing on\\na slightly shorter vector of length d minus 1. And then you would just look\\nat the largest eigenvector of these guys, et\\ncetera, et cetera. So in a way, that's\\nwhat's happening, except that you rotate it\\nbefore you actually do this. And that's exactly\\nwhat's happening. So what we put together here\\nis essentially three things. One was statistics. Statistics says, if\\nyou won't spread, if you want information, you\\nshould be looking at variance. The second one was optimization. Optimization said, well, if you\\nwant to maximize spread, well, you have to maximize variance\\nin a certain direction. And that means maximizing\\nover the sphere of vectors that have unique norm. And that's an optimization\\nproblem, which actually turned out to be difficult. But then the third thing that\\nwe use to solve this problem was linear algebra. Linear algebra\\nsaid, well, it looks like it's a difficult\\noptimization problem. But it turns out that the\\nanswer comes in almost-- I mean, it's not a closed form,\\nbut those things are so used, that it's almost a closed form-- says, just pick the\\neigenvectors in order of their associated eigenvalues\\nfrom largest to smallest. And that's why principal\\ncomponent analysis has been so popular and has\\ngained huge amount of traction since we had computers that were\\nallowed to compute eigenvalues and eigenvectors for\\nmatrices of gigantic sizes. You can actually do that. If I give you-- I don't know, this Google\\nvideo, for example, is talking about words. They want to do just the,\\nsay, principal component analysis of words. So I give you all the\\nwords in the dictionary. And-- sorry, well,\\nyou would have to have a representation\\nfor words, so it's a little more\\ndifficult. But how do I do this? Let's say, for example,\\npages of a book. I want to understand\\nthe pages of a book. And I need to turn\\nit into a number. And a page of a book is\\nbasically the word count. So I just count the number\\nof times \\\"the\\\" shows up, the number of times \\\"and\\\"\\nshows up, number of times \\\"dog\\\" shows up. And so that gives me a vector. It's in pretty high dimensions. It's as many dimensions as there\\nare words in the dictionary. And now, I want to visualize\\nhow those pages get together-- are two pages very\\nsimilar or not. And so what you would\\ndo is essentially just compute the largest\\neigenvector of this matrix-- maybe the two largest-- and\\nthen project this into a plane. Yeah. AUDIENCE: Can we assume\\nthe number of points was far larger\\nthan the dimension? PHILIPPE RIGOLLET:\\nYeah, but there's many pages in the world. There's probably more\\npages in the world than there's words\\nin the dictionary. Yeah, so of course, if\\nyou are in high dimensions and you don't have\\nenough points, it's going to be\\nclearly an issue. If you have two points,\\nthen the leading eigenvector is going to be\\njust the line that goes through those\\ntwo points, regardless of what the dimension is. And clearly, you're\\nnot learning anything. So you have to pick,\\nsay, the k largest one. If you go all the way, you're\\njust reordering your thing, and you're not actually\\ngaining anything. You start from d\\nand you go too d. So at some point, this\\nprocedure has to stop. And let's say it stops at k. Now, of course, you\\nshould ask me a question, which is, how do you choose k? So that's, of course,\\na natural question. Probably the basic answer\\nis just pick k equals 3, because you can\\nactually visualize it. But what happens if I\\ntake k is equal to 4? If I take is equal\\nto 4, I'm not going to be able to plot points\\nin four dimensions. Well, I could, I\\ncould add color, or I could try to be a\\nlittle smart about it. But it's actually\\nquite difficult. And so what people tend to do,\\nif you have four dimensions, they actually do a bunch\\nof two dimensional plots. And that's what a computer--\\na computer is not very good-- I mean, by default,\\nthey don't spit out three dimensional plots. So let's say they want to plot\\nonly two dimensional things. So they're going to take the\\nfirst directions of, say, v1, v2. Let's say you have\\nthree, but you want to have only two\\ndimensional plots. And then it's going to do\\nv1, v3; and then v2, v3. So really, you take\\nall three of them, but it's really just\\nshowing you all choices of pairs of those guys. So if you were to\\nkeep k is equal to 5, you would have five,\\nchoose two different plots. So this is the actual\\nprincipal component algorithm, how it's implemented. And it's actually fairly simple. I mean, it looks like\\nthere's lots of steps. But really, there's only\\none that's important. So the first one is the input. I give you a bunch of points,\\nx1 to xn in d dimensions. And step two is, well, compute\\ntheir empirical covariance matrix S. The points themselves,\\nwe don't really care. We care about their\\nempirical covariance matrix. So it's a d by d matrix. Now, I'm going to feed that. And that's where the actual\\ncomputation starts happening. I'm going to feed that\\nto something that knows how to diagonalize this matrix. And you have to\\ntrust me, if I want to compute the k\\nlargest eigenvalues and my matrix is\\nd by d, it's going to take me about k times\\nd squared operations. So if I want only three,\\nit's 3 times d squared, which is about-- d squared is the time for me\\nit takes to just even read the matrix sigma. So that's not too bad. So what it's going to\\nspit out, of course, is the diagonal matrix\\nD. And those are nice, because they allow\\nme to tell me what is the order in which I should\\nbe taking the columns of P. But what's really important\\nto me is v1 to vd, because those are going to be\\nthe ones I'm going to be using to draw those plots. And now, I'm going\\nto say, OK, I need to actually choose some set k. And I'm going to basically\\ntruncate and look only at the first\\nk columns of P. Once I have those\\ncolumns, what I want to do is to project\\nonto the linear span of those columns. And there's actually\\na simple way to do this, which is just take\\nthis matrix P, which is really the matrix of projection onto\\nthe linear span of those k columns. And you just take Pk transpose. And then you apply this to\\nevery single one of your points. Now Pk transpose, what is\\nthe size of the matrix Pk? Yeah, [INAUDIBLE]? AUDIENCE: [INAUDIBLE] PHILIPPE RIGOLLET: So\\nPk is just this matrix. I take the v1 and I stop at vk-- well-- AUDIENCE: [INAUDIBLE] PHILIPPE RIGOLLET:\\nd by k, right? Each of the column\\nis an eigenvector. It's of dimension d. I mean, that's a vector\\nin the original space. So I have this d by k matrix. So all it is is if I had my-- well, I'm going to talk in\\na second about Pk transpose. Pk transpose is\\njust this guy, where I stop at the k-th vector. So Pk transpose is k by d. So now, when I take Yi,\\nwhich is Pk transpose Xi, I end up with a point\\nwhich is in k dimensions. I have only k coordinates. So I took every single one\\nof my original points Xi, which had d coordinates, and\\nI turned it into a point that has only k coordinates. Particularly, I could\\nhave k is equal to 2. This matrix is exactly\\nthe one that projects. If you think about\\nit for one second, this is just the\\nmatrix that says-- well, we actually did\\nthat several times. The matrix, so that\\nwas this P transpose u that showed up somewhere. And so that's just\\nthe matrix that take your point X in,\\nsay, three dimensions, and then just project it\\ndown to two dimensions. And that's just-- it goes to the\\nclosest point in the subspace. Now, here, the floor is flat. But we can pick any\\nsubspace we want, depending on what\\nthe lambdas are. So the lambdas were\\nimportant for us to be able to identify\\nwhich columns to pick. The fact that we assumed\\nthat they were ordered tells us that we can\\npick the first ones. If they were not\\nordered, it would be just a subset of the\\ncolumns, depending on what the size of the eigenvalue is. So each column is labeled. And so then, of course, we\\nstill have this question of, how do I pick k? So there's definitely the\\nmatter of convenience. Maybe 2 is convenient. If it works for 2, you don't\\nhave to go any farther. But you might want\\nto say, well-- originally, I did\\nthat to actually keep as much information as possible. I know that the\\nultimate thing is to keep as much information,\\nwhich would be to k is equal d-- that's as much\\ninformation as you want. But it's essentially the\\nsame question about, well, if I want to compress\\na JPEG image, how much information should\\nI keep so it's still visible? And so there's some\\nrules for that. But none of them is\\nactually really a science. So it's really a\\nmatter of what you think is actually tolerable. And we're just going to start\\nreplacing this choice by maybe another parameter. So here, we're going to\\nbasically replace k by alpha, and so we just do stuff. So the first one that\\npeople do that is probably the most popular one-- OK, the most popular\\none is definitely take k is equal to 2\\nor 3, because it's just convenient to visualize. The second most popular\\none is the scree plot. So the scree plot-- remember, I have my\\nvalues, lambda j's. And I've chosen the\\nlambda j's to decrease. So the indices are\\nchosen in such a way that lambda is a\\ndecreasing function. So I have lambda 1, and\\nlet's say it's this guy here. And then I have lambda 2, and\\nlet's say it's this guy here. And then I have lambda 3, and\\nlet's say it's this guy here, lambda 4, lambda 5, lambda 6. And all I care about is\\nthat this thing decreases. The scree plot says\\nsomething like this-- if there's an inflection point,\\nmeaning that you can sort of do something like this and\\nthen something like this, you should stop at 3. That's what the\\nscree plot tells you. What it's saying in a way\\nis that the percentage of the marginal\\nincrement of explained variance that you get\\nstarts to decrease after you pass this inflection point. So let's see why I way this. Well, here, what I\\nhave-- so this ratio that you see there is\\nactually the percentage of explained variance. So what it means is that, if I\\nlook at lambda 1 plus lambda k, and then I divide by lambda\\n1 plus lambda d, well, what is this? Well, this lambda\\n1 plus lambda d is the total amount of variance\\nthat I get in my points. That's the trace of sigma. So that's the variance\\nin the first direction plus the variance in\\nthe second direction plus the variance in\\nthe third direction. That's basically all the\\nvariance that I have possible. Now, this is the variance that\\nI kept in the first direction. This is the variance that I\\nkept in the second direction, all the way to the variance that\\nI kept in the k-th direction. So I know that this number is\\nalways less than or equal to 1. And it's larger than 1. And this is just\\nthe proportion, say, of variance explained\\nby v1 to vk, or simply, the proportion of\\nexplained variance by my PCA, say. So now, what this\\nthing is telling me, its says, well, if\\nI look at this thing and I start seeing this\\ninflection point, it's saying, oh, here, you're gaining\\na lot and lot of variance. And then at some point,\\nyou stop gaining a lot in your proportion of\\nexplained variance. So this will\\ntranslate in something where when I look at this ratio,\\nlambda 1 plus lambda k divided by lambda 1 plus\\nlambda d, this would translate into a function\\nthat would look like this. And what it's telling you,\\nit says, well, maybe you should stop here, because\\nhere every time you add one, you don't get as much\\nas you did before. You actually get like\\nsmaller marginal returns. So explained variance is\\nthe numerator of this ratio. And the total variance\\nis the denominator. Those are pretty\\nstraightforward terms that you would want\\nto use for this. So if your goal is to\\ndo data visualization-- so why would you\\ntake k larger than 2? Let's say, if you\\ntake k larger than 6, you can start to\\nimagine that you're going to have six, choose two,\\nwhich starts to be annoying. And if you have k\\nis equal to 10-- because you could start\\nin dimension 50,000-- and then k equal to\\n10 would be the place where you have this thing\\nthat's a lot of plots that you would have to show. So it's not always for\\ndata visualization. Once I've actually\\ndone this, I've actually effectively reduced\\nthe dimension of my problem. And what I could do\\nwith what I have is do a regression on those guys. The v1-- so I\\nforgot to tell you-- why is that called principal\\ncomponent analysis? Well, the vj's that\\nI keep, v1 to vk are called principal components. And they effectively act\\nas the summary of my Xi's. When I mentioned\\nimage compression, I started with a point\\nXi that was d numbers-- let's say 50,000 numbers. And now, I'm saying,\\nactually, you can throw out those\\n50,000 numbers. If you actually know only\\nthe k numbers that you need-- the 6 numbers that you need-- you're going to\\nhave something that was pretty close to getting\\nwhat information you had. So in a way, there is\\nsome form of compression that's going on here. And what you can do is that\\nthose principal components, you can actually use\\nnow for regression. If I want to regress\\nY onto X that's very high dimensional,\\nbefore I do this, if I don't have enough points,\\nmaybe what I can actually do is to do principal\\ncomponent analysis throughout my\\nexercise, replace them by those compressed versions,\\nand do linear aggression on those guys. And that's called principal\\ncomponent regression, not surprisingly. And that's something\\nthat's pretty popular. And you can do with k is\\nequal to 10, for example. So for data visualization, I did\\nnot find a Thanksgiving themed picture. But I found one that\\nhas turkey in it. Get it? So this is actually a\\ngene data set that was-- so when you see\\nsomething like this, you can imagine that someone\\nhas been preprocessing the hell out of this thing. This is not like, oh, I\\ncollect data on 23andMe and I'm just going\\nto run PCA on this. It just doesn't\\nhappen like that. And so what happened is that--\\nso let's assume that this was a bunch of preprocessed data,\\nwhich are gene expression levels-- so 500,000 genes\\namong 1,400 Europeans. So here, I actually\\nhave less observations than I have samples. And that's when you use\\nprincipal component regression most of the time, so\\nit doesn't stop you. And then what you do is you say,\\nOK, have those 500,000 genes among-- so here, that means that\\nthere's 1,400 points here. And I actually take\\nthose 500,000 directions. So each person has a vector\\nof, say, 500,000 genes that are attached to them. And I project them onto\\ntwo dimensions, which should be extremely lossy. I lose a lot of information. And indeed, I do, because\\nI'm one of these guys. And I'm pretty sure I'm very\\ndifferent from this guy, even though probably from\\nan American perspective, we're all the same. But I think we have like\\nslightly different genomes. And so the thing is\\nnow we have this-- so you see there's lots of\\nSwiss that participate in this. But actually, those two\\nprincipal components recover sort of\\nthe map of Europe. I mean, OK, again, this is\\nactually maybe fine-grained for you guys. But right here, there's\\nPortugal and Spain, which are those colors. So here is color-coded. And here is Turkey, of\\ncourse, which we know has very different genomes. So Turks are very\\nat the boundary. So you can see all the greens. They stay very far apart\\nfrom everything else. And then the rest\\nhere is pretty mixed. But it sort of recovers--\\nif you look at the colors, it sort of recovers that. So in a way, those two\\nprincipal components are just the geographic feature. So if you insist to compress\\nall the genomic information of these people into two\\nnumbers, what you're actually going to get is\\nlongitude and latitude, which is somewhat\\nsurprising, but not so much if you think that's\\nit's been preprocessed. So what do you do\\nbeyond practice? Well, you could try to\\nactually study those things. If you think about\\nit for a second, we did not do any statistics. I talked to you about\\nIID observations, but we never used the fact\\nthat they were independent. The way we typically\\nuse independence is to have central\\nlimit theorem, maybe. I mentioned the fact that\\nthe covariances of the word Gaussian would actually give me\\nsomething which is independent. We didn't care. This was a data analysis, data\\nmining process that we did. I give you points, and you just\\nput them through the crank. There was an algorithm\\nin six steps. And you just put it through\\nand that's what you got. Now, of course, there's some\\nwork which studies says, OK, if my data is actually generated\\nfrom some process-- maybe, my points are multivariate\\nGaussian with some structure on the covariance-- how well am I recovering\\nthe covariance structure? And that's where\\nstatistics kicks in. And that's where we stop. So this is actually a bit\\nmore difficult to study. But in a way, it's not\\nentirely satisfactory, because we could work\\nfor a couple of boards and I would just basically\\nsort of reverse engineer this and find some models under which\\nit's a good idea to do that. And what are those models? Well, those are the models\\nthat sort of give you sort of prominent directions\\nthat you want to find. And it will say, yes, if you\\nhave enough observations, you will find those\\ndirections along which your data is elongated. So that's essentially\\nwhat you want to do. So that's exactly what\\nthis thing is telling you. So where does the\\nstatistics lie from? Well, everything, remember--\\nso actually that's where Alana was confused--\\nthe idea was to say, well, if I have a true\\ncovariance matrix sigma and I never really\\nhave access to it, I'm just running PCA on the\\nempirical covariance matrix, how do those results relate? And this is something\\nthat you can study. So for example, if\\nn goes to infinity and the number of points,\\nyour dimension, is fixed, then S goes to sigma\\nin any sense you want. Maybe each entry is going\\nto each entry of sigma, for example. So S is a good estimator. We know that the\\nempirical covariance is a consistent as the mater. And if d is fixed, this\\nis actually not an issue. So in particular, if you run\\nPCA on the sample covariance matrix, you look\\nat, say, v1, then v1 is going to converge to the\\nlargest eigenvector of sigma as n goes to infinity,\\nbut for d fixed. And that's a story that\\nwe know since the '60s. More recently, people have\\nstarted challenging this. Because what's happening\\nwhen you fix the dimension and let the sample\\nsize go to infinity, you're certainly not\\nallowing for this. It's certainly not explaining\\nto you anything about the fact when d is equal to 500,000\\nand n is equal to 1,400. Because when d is fixed\\nand n goes to infinity, in particular, n is\\nmuch larger than d, which is not the case here. And so when n is much larger\\nthan d, things go well. But if d is less than n,\\nit's not clear what happens. And particularly, if d is of the\\norder of n, what's happening? So there's an entire theory\\nin mathematics that's called random matrix theory that\\nstudies the behavior of exactly this question-- what is the\\nbehavior of the spectrum-- the eigenvalues\\nand eigenvectors-- of a matrix in which I put\\nrandom numbers and I let-- so the matrix I'm interested\\nin here is the matrix of X's. When I stack all my\\nX's next to each other, so that's a matrix of size,\\nsay, d by n, so each column is of size d, it's one person. And so I put them. And when I let the\\nmatrix go to infinity, I let both d and n to infinity. But I want the aspect ratio,\\nd/n, to go to some constant. That's what they do. And what's nice is that in the\\nend, you have this constant-- let's call it gamma-- that shows up in\\nall the asymptotics. And then you can\\nreplace it by d/n. And you know that you still have\\na handle of both the dimension and the sample size. Whereas, usually the dimension\\ngoes away, as you let n go to infinity without having\\ndimension going to infinity. And so now, when\\nthis happens, as soon as d/n goes to a\\nconstant, you can show that essentially there's\\nan angle between the largest eigenvector of sigma and the\\nlargest eigenvector of S, as n and d go to infinity. There is always an\\nangle-- you can actually write it explicitly. And it's an angle that\\ndepends on this ratio, gamma-- the asymptotic ratio of d/n. And so there's been a lot of\\nunderstanding how to correct, how to pay attention to this. This creates some biases that\\nwere sort of overlooked before. In particular, when\\nI do this, this is not the proportion\\nof explained variance, when n and d are similar. This is an estimated\\nnumber computed from S. This is computed from S. All\\nthese guys are computed from S. So those are\\nactually not exactly where you want them to be. And there's some nice work that\\nallows you to recalibrate what this ratio should be, how\\nthis ratio should be computed, so it's a better\\nrepresentative of what the proportion of explained\\nvariance actually is. So then, of course,\\nthere's the question of-- so that's when d/n\\ngoes to some constant. So the best case--\\nso that was '60s-- d is fixed and it's\\nmuch larger than d. And then random matrix theory\\ntells you, well, d and n are sort of the same\\norder of magnitude. When they go to infinity, the\\nratio goes to some constant. Think of it as being order 1. To be fair, if d is 100 times\\nlarger than n, it still works. And it depends on\\nwhat you think what the infinity is at this point. But I think the random matrix\\ntheory results are very useful. But then even in\\nthis case, I told you that the leading\\neigenvector of S is actually an angle of the\\nleading eigenvector of-- So what's happening is that-- so let's say that d/n\\ngoes to some gamma. And what I claim is\\nthat, if you look at-- so that's v1, that's the v1 of\\nS. And then there's the v1 of-- so this should be of size 1. So that's the v1 of sigma. Then those things are going\\nto have an angle, which is some function of gamma. It's complicated, but\\nthere's a function of gamma that you can see there. And there's some models. When gamma goes\\nto infinity, which means that d is now\\nmuch larger than n, this angle is 90\\ndegrees, which means that you're getting nothing. Yeah. AUDIENCE: If d is not\\non your lower plane, so like gamma is 0,\\nis there still angle? PHILIPPE RIGOLLET: No,\\nbut that's consistent-- the fact that it's\\nconsistent when-- so the angle is a function-- AUDIENCE: d is not a\\nconstant [INAUDIBLE]?? PHILIPPE RIGOLLET:\\nd is not a constant? So if d is little of n? Then gamma goes to 0 and\\nf of gamma goes to 0. So f of gamma is\\na function that-- so for example, if f of gamma-- this is the sine of the\\nangle, for example-- then it's a function that starts\\nat 0, and that goes like this. But as soon as gamma is\\npositive, it goes away from 0. So now when gamma\\ngoes to infinity, then this thing goes\\nto a right angle, which means I'm getting just junk. So this is not my\\nleading eigenvector. So how do you do this? Well, just like\\neverywhere in statistics, you have to just make\\nmore assumptions. You have to assume\\nthat you're not looking for the leading\\neigenvector or the direction that carries the most variance. But you're looking, maybe,\\nfor a special direction. And that's what\\nsparse PCA is doing. Sparse PCA is saying, I'm not\\nlooking for any direction new that carries the most variance. I'm only looking for a\\ndirection new that is sparse. Think of it, for example, as\\nhaving 10 non-zero coordinates. So that's a lot of\\ndirections still to look for. But once you do this,\\nthen you actually have not only--\\nthere's a few things that actually you\\nget from doing this. The first one is you\\nactually essentially replace d by k, which means\\nthat n now just-- I'm sorry, let's say S\\nnon-zero coefficients. You replace d by S,\\nwhich means that n only has to be much larger than S\\nfor this thing to actually work. Now, of course, you've\\nset your goal weaker. Your goal is not to\\nfind any direction, only a sparse direction. But there's something\\nvery valuable about sparse directions,\\nis that they actually are interpretable. When I found the v-- let's say that the v\\nthat I found before was 0.2, and then 0.9, and\\nthen 1.1 minus 3, et cetera. So that was the coordinates\\nof my leading eigenvector in the original\\ncoordinate system. What does it mean? Well, it means that if\\nI see a large number, that means that this\\nv is very close-- so that's my original\\ncoordinate system. Let's call it e1 and e2. So that's just 1,\\n0; and then 0, 1. Then clearly, from\\nthe coordinates of v, I can tell if my v is like\\nthis, or it's like this, or it's like this. Well, I mean, they should\\nall be of the same size. So I can tell if\\nit's here or here or here, depending\\non-- like here, that means I'm going\\nto see something where the Y-coordinate it much\\nlarger than the X-coordinate. Here, I'm going to see something\\nwhere the X-coordinate is much larger than the Y-coordinate. And here, I'm going\\nto see something where the X-coordinate\\nis about the same size of the Y-coordinate. So when things\\nstarts to be bigger, you're going to have\\nto make choices. What does it mean to be bigger-- when d is 100,000,\\nI mean, the sum of the squares of those\\nguys have to be equal to 1. So they're all\\nvery small numbers. And so it's hard for you to\\ntell which one is a big number and which ones is\\na small number. Why would you want to know this? Because it's\\nactually telling you that if v is very close to\\ne1, then that means that e1-- in the case of the\\ngene example, that would mean that e1 is the\\ngene that's very important. Maybe there's actually\\njust two genes that explain those two things. And those are the genes\\nthat have been picked up. There's two genes that I\\nencode geographic location, and that's it. And so it's very\\nimportant for you to be able to\\ninterpret what v means. Where it has large\\nvalues, it means that maybe it has large\\nvalues for e1, e2, and e3. And it means that it's a\\ncombination of e1, e2, and e3. And now, you can\\ninterpret, because you have only three variables to find. And so sparse PCA\\nbuilds that in. Sparse PCA says,\\nlisten, I'm going to want to have at most\\n10 non-zero coefficients. And the rest, I want to be 0. I want to be able to be a\\ncombination of at most 10 of my original variables. And now, I can do\\ninterpretation. So the problem\\nwith sparse PCA is that it becomes very\\ndifficult numerically to solve this problem. I can write it. So the problem is simply\\nmaximize the variance u transpose, say, Su\\nsubject to-- well, I wanted to have u2 equal to 1. So that's the original PCA. But now, I also\\nwant that the sum of the indicators of the\\nuj that are not equal to 0 is at most, say, 10. This constraint is\\nvery non-convex. So I can relax it\\nto a convex one like we did for\\nlinear aggression. But now, I've totally\\nmessed up with the fact that I could use linear\\nalgebra to solve this problem. And so now, you have to go\\nthrough much more complicated optimization techniques,\\nwhich are called semidefinite\\nprograms, which do not scale well in high dimensions. And so you have to do\\na bunch of tricks-- numerical tricks. But there are some packages\\nthat implements some heuristics or some other things-- iterative\\nthresholding, all sorts of various numerical\\ntricks that you can do. But the problem they are trying\\nto solve is exactly this. Among all directions that\\nI have norm 1, of course, because it's the direction\\nthat have at most, say, 10 non-zero coordinates, I want\\nto find the one that maximizes the empirical variance. Actually, let me let\\nme just so you this. I wanted to show\\nyou an output of PCA where people are actually\\ntrying to do directly-- maybe-- there you go. So right here, you\\nsee this is SPSS. That's a statistical software. And this is an output\\nthat was preprocessed by a professional-- not preprocessed,\\npost-processed. So that's something\\nwhere they read PCA. So what is the data? This is raw data\\nabout you ask doctors what they think of the\\nbehavior of a particular sales representative for\\npharmaceutical companies. So pharmaceutical\\ncompanies are trying to improve their sales force. And they're asking\\ndoctors how would they rate-- what do they value\\nabout their interaction with a sales representative. So basically, there's\\na bunch of questions. One offers credible point\\nof view on something trends, provides valuable\\nnetworking opportunities. This is one question. Rate this on a\\nscale from 1 to 5. That was the question. And they had a bunch\\nof questions like this. And then they asked 1,000\\ndoctors to make those ratings. And what they want--\\nso each doctor now is a vector of ratings. And they want to know if there's\\ndifferent groups of doctors, what do doctors respond to. If there's different\\ngroups, then maybe they know that they\\ncan actually address them separately, et cetera. And so to do that, of course,\\nthere's lots of questions. And so what you want is\\nto just first project into lower dimensions,\\nso you can actually visualize what's going on. And this is what\\nwas done for this. So these are the three\\nfirst principal component that came out. And even though we ordered\\nthe values of the lambdas, there's no reason why the\\nentries of v should be ordered. And if you look at\\nthe values of v here, they look like they're\\npretty much ordered. It starts at 0.784, and then\\nyou're at 0.3 around here. There's something that goes up\\nagain, and then you go down. Actually, it's marked in red\\nevery time it goes up again. And so now, what they\\ndid is they said, OK, I need to\\ninterpret those guys. I need to tell you what this is. If you tell me, we found\\nthe principal component that really discriminates\\nthe doctors in two groups, the drug company is\\ngoing to come back to you and say, OK, what is\\nthis characteristic? And you say, oh, it's\\nactually a linear combination of 40 characteristics. And they say, well, we\\ndon't need you to do that. I mean, it cannot be a linear\\ncombination of anything you didn't ask. And so for that,\\nfirst of all, there's a post-processing of PCA, which\\nsays, OK, once I actually, say, found three\\nprincipal components, that means that I found the\\ndimension three space on which I want to project my points. In this base, I can pick\\nany direction I want. So the first thing\\nis that you do some sort of local arrangements,\\nso that those things look like they are increasing\\nand then decreasing. So you just change, you\\nrotate your coordinate system in this three dimensional space\\nthat you've actually isolated. And so once you do\\nthis, the reason to do that is that\\nit sort of makes them big, sharp differences\\nbetween large and small values of the coordinates\\nof the thing you had. And why do you want this? Because now, you\\ncan say, well, I'm going to start looking at the\\nones that have large values. And what do they say? They say in-depth knowledge,\\nin-depth knowledge, in-depth knowledge,\\nknowledge about. This thing is clearly\\nsomething that actually characterizes\\nthe knowledge of my sales representative. And so that's something that\\ndoctors are sensitive to. That's something that\\nreally discriminates the doctors in a way. There's lots of variance\\nalong those things, or at least a lot of variance-- I mean, doctors are separate\\nin terms of their experience with respect to this. And so what they\\ndid is said, OK, all these guys, some of\\nthose they have large values, but I don't know how\\nto interpret them. And so I'm just going\\nto put the first block, and I'm going to call\\nit medical knowledge, because all those things are\\nknowledge about medical stuff. Then here, I didn't know\\nhow to interpret those guys. But those guys, there's a big\\nclump of large coordinates, and they're about respectful\\nof my time, listens, friendly but courteous. This is all about the\\nquality of interaction. So this block was actually\\ncalled quality of interaction. And then there\\nwas a third block, which you can tell starts to\\nbe spreading a little thin. There's just much less of them. But this thing was\\nactually called fair and critical opinion. And so now, you have three\\ndiscriminating directions. And you can actually\\ngive them a name. Wouldn't it be beautiful if\\nall the numbers in the gray box came non-zero and\\nall the other numbers came zero-- there\\nwas no ad hoc choice. I mean, this is probably\\nan afternoon of work to like scratch out\\nall these numbers and put all these\\ncolor codes, et cetera. Whereas, you could just have\\nsomething that tells you, OK, here are the non-zeros. If you can actually make a story\\naround why this group of thing actually makes sense, such\\nas it is medical knowledge, then good for you. Otherwise, you could\\njust say, I can't. And that's what sparse\\nPCA does for you. Sparse PCA outputs something\\nwhere all those numbers would be zero. And there would be exactly,\\nsay, 10 non-zero coordinates. And you can turn\\nthis knob off 10. You can make it 9. Depending on what\\nyour major is, maybe you can actually go\\non with 20 of them and have the ability to\\ntell the story about 20 different variables and how\\nthey fit in the same group. And depending on\\nhow you feel, it's easy to rerun the PCA\\ndepending on the value that you want here. And so you could actually\\njust come up with the one you prefer. And so that's the\\nsparse PCA thing which I'm trying to promote. I mean, this is not\\nsuper well-spread. It's a fairly new idea,\\nmaybe at most 10 years old. And it's not\\ncompletely well-spread in statistical packages. But that's clearly\\nwhat people are trying to emulate currently. Yes? AUDIENCE: So what\\nexactly does it mean that the doctors\\nhave a lot of variance in medical knowledge,\\nquality of interaction, and fair and critical opinion? Like, it was saying that\\nthese are like the main things that doctors vary on,\\nsome doctors care. Like we could sort of\\ncharacterize a doctor by, oh, he cares this much about\\nmedical knowledge, this much about the quality\\nof interaction, and this much about\\ncritical opinion. And that says most of the story\\nabout what this doctor wants from a drug representative? PHILIPPE RIGOLLET: Not really. I mean, OK, let's say\\nyou pick only one. So that means that you\\nwould take all your doctors, and you would have\\none direction, which is quality of interaction. And there would be just\\nspread out points here. So there are two\\nthings that can happen. The first one is that\\nthere's a clump here, and then there's a clump here. That still represents\\na lot of variance. And if this happens,\\nyou probably want to go back in\\nyour data and see were these people visited\\nby a different group than these people,\\nor maybe these people have a different specialty. I mean, you have to\\nlook back at your data and try to understand\\nwhy you would have different groups of people. And if it's like completely\\nevenly spread out, then all it's saying\\nis that, if you want to have a uniform\\nquality of interaction, you need to take\\nmeasures on this. You need to have this to\\nnot be discrimination. But I think really when it's\\nbecoming interesting it's not when it's complete spread out. It's when there's\\na big group here. And then there's\\nalmost no one here, and then there's\\na big group here. And then maybe there's\\nsomething you can do. And so those two things actually\\ngive you a lot of variance. So actually, maybe\\nI'll talk about this. Here, this is sort of a mixture. You have a mixture of\\ntwo different populations of doctors. And it turns out that\\nprincipal component analysis-- so a mixture is when you\\nhave different populations-- think of like two\\nGaussians that are just centered at two\\ndifferent points, and maybe they're\\nin high dimensions. And those are\\nclusters of people, and you want to be able to\\ndifferentiate those guys. If you're in very\\nhigh dimensions, it's going to be very\\ndifficult. But one of the first processing tools\\nthat people do is to do PCA. Because if you have one big\\ngroup here and one big group here, it means that\\nthere's a lot of variance along the direction that\\ngoes through the centers of those groups. And that's essentially\\nwhat happened here. You could think of this as being\\ntwo blobs in high dimensions. But you're really\\njust projecting them into one dimension. And this dimension, hopefully,\\ngoes through the center. And so as preprocessing--\\nso I'm going to stop here. But PCA is not just made\\nfor dimension reduction. It's used for\\nmixtures, for example. It's also used when you\\nhave graphical data. What is the idea of PCA? It just says, if you have a\\nmatrix that seems to have low rank-- meaning that there's a\\nlot of those lambda i's that are very small-- and then I see that\\nplus noise, then it's a good idea to\\ndo PCA on this thing. And in particular, people\\nuse that in networks a lot. So you take the adjacency\\nmatrix of a graph-- well, you sort of preprocess it\\na little bit, so it looks nice. And then if you have, for\\nexample, two communities in there, it should\\nlook like something that is low rank plus some noise. And low rank means that there's\\njust very few non-zero-- well, low rank means this. Low rank means that if\\nyou do the scree plot, you will see\\nsomething like this, which means that if you throw\\nout all the smaller ones, it should not really matter\\nin the overall structure. And so you can use all-- these techniques are used\\neverywhere these days, not just in PCA. So we call it PCA\\nas statisticians. But people call it the\\nspectral methods or SVD. So everyone--\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 11,\n        \"samples\": [\n          \"Data Structures\",\n          \"Linear Algebra\",\n          \"Diff. Eq.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"labels\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 0,\n        \"max\": 10,\n        \"num_unique_values\": 11,\n        \"samples\": [\n          5,\n          0,\n          9\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_org.label.value_counts().plot(kind='pie', figsize=(10,10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 844
        },
        "id": "7InqQqabyU8D",
        "outputId": "ee3a5afb-24fe-45e2-a9a3-d00dec2745f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: ylabel='count'>"
            ]
          },
          "metadata": {},
          "execution_count": 51
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x1000 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\", max_length=512)"
      ],
      "metadata": {
        "id": "FojBdRSbyaxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# problem_type=\"multi_label_classification\""
      ],
      "metadata": {
        "id": "UfMk8M36zsNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=NUM_LABELS, id2label=id2label, label2id=label2id)\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-rPfe-8NzYbm",
        "outputId": "c8dc9110-6d64-4234-8d92-f80e94185d9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=11, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SIZE= df_org.shape[0]\n",
        "\n",
        "train_texts= list(df_org.text[:SIZE//2])\n",
        "\n",
        "val_texts=   list(df_org.text[SIZE//2:(3*SIZE)//4 ])\n",
        "\n",
        "test_texts=  list(df_org.text[(3*SIZE)//4:])\n",
        "\n",
        "train_labels= list(df_org.labels[:SIZE//2])\n",
        "\n",
        "val_labels=   list(df_org.labels[SIZE//2:(3*SIZE)//4])\n",
        "\n",
        "test_labels=  list(df_org.labels[(3*SIZE)//4:])"
      ],
      "metadata": {
        "id": "poGhi6WHz5oj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_texts),len(test_texts), len(val_texts), len(train_labels), len(test_labels), len(val_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7tVKXq0qzvqk",
        "outputId": "5e4d61b7-cb0c-45fb-d022-07bbac19c568"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(429, 215, 215, 429, 215, 215)"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
        "val_encodings  = tokenizer(val_texts, truncation=True, padding=True)\n",
        "test_encodings = tokenizer(test_texts, truncation=True, padding=True)"
      ],
      "metadata": {
        "id": "OPC-sEHS2vp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# chck"
      ],
      "metadata": {
        "id": "wmgGAv7N3Wdx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DataLoader(Dataset):\n",
        "    \"\"\"\n",
        "    Custom Dataset class for handling tokenized text data and corresponding labels.\n",
        "    Inherits from torch.utils.data.Dataset.\n",
        "    \"\"\"\n",
        "    def __init__(self, encodings, labels):\n",
        "        \"\"\"\n",
        "        Initializes the DataLoader class with encodings and labels.\n",
        "\n",
        "        Args:\n",
        "            encodings (dict): A dictionary containing tokenized input text data\n",
        "                              (e.g., 'input_ids', 'token_type_ids', 'attention_mask').\n",
        "            labels (list): A list of integer labels for the input text data.\n",
        "        \"\"\"\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Returns a dictionary containing tokenized data and the corresponding label for a given index.\n",
        "\n",
        "        Args:\n",
        "            idx (int): The index of the data item to retrieve.\n",
        "\n",
        "        Returns:\n",
        "            item (dict): A dictionary containing the tokenized data and the corresponding label.\n",
        "        \"\"\"\n",
        "        # Retrieve tokenized data for the given index\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        # Add the label for the given index to the item dictionary\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Returns the number of data items in the dataset.\n",
        "\n",
        "        Returns:\n",
        "            (int): The number of data items in the dataset.\n",
        "        \"\"\"\n",
        "        return len(self.labels)"
      ],
      "metadata": {
        "id": "Fwel2vcO3I01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = DataLoader(train_encodings, train_labels)\n",
        "\n",
        "val_dataloader = DataLoader(val_encodings, val_labels)\n",
        "\n",
        "test_dataset = DataLoader(test_encodings, test_labels)"
      ],
      "metadata": {
        "id": "_YAJkxge3lTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support"
      ],
      "metadata": {
        "id": "tdxw8ZJA3ts5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(pred):\n",
        "    \"\"\"\n",
        "    Computes accuracy, F1, precision, and recall for a given set of predictions.\n",
        "\n",
        "    Args:\n",
        "        pred (obj): An object containing label_ids and predictions attributes.\n",
        "            - label_ids (array-like): A 1D array of true class labels.\n",
        "            - predictions (array-like): A 2D array where each row represents\n",
        "              an observation, and each column represents the probability of\n",
        "              that observation belonging to a certain class.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the following metrics:\n",
        "            - Accuracy (float): The proportion of correctly classified instances.\n",
        "            - F1 (float): The macro F1 score, which is the harmonic mean of precision\n",
        "              and recall. Macro averaging calculates the metric independently for\n",
        "              each class and then takes the average.\n",
        "            - Precision (float): The macro precision, which is the number of true\n",
        "              positives divided by the sum of true positives and false positives.\n",
        "            - Recall (float): The macro recall, which is the number of true positives\n",
        "              divided by the sum of true positives and false negatives.\n",
        "    \"\"\"\n",
        "    # Extract true labels from the input object\n",
        "    labels = pred.label_ids\n",
        "\n",
        "    # Obtain predicted class labels by finding the column index with the maximum probability\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "\n",
        "    # Compute macro precision, recall, and F1 score using sklearn's precision_recall_fscore_support function\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n",
        "\n",
        "    # Calculate the accuracy score using sklearn's accuracy_score function\n",
        "    acc = accuracy_score(labels, preds)\n",
        "\n",
        "    # Return the computed metrics as a dictionary\n",
        "    return {\n",
        "        'Accuracy': acc,\n",
        "        'F1': f1,\n",
        "        'Precision': precision,\n",
        "        'Recall': recall\n",
        "    }"
      ],
      "metadata": {
        "id": "sM4_V1JG3wVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    # The output directory where the model predictions and checkpoints will be written\n",
        "    output_dir='./Model_output',\n",
        "    do_train=True,\n",
        "    do_eval=True,\n",
        "    #  The number of epochs, defaults to 3.0\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=1,\n",
        "    # Number of steps used for a linear warmup\n",
        "    warmup_steps=100,\n",
        "    weight_decay=0.01,\n",
        "    logging_strategy='steps',\n",
        "   # TensorBoard log directory\n",
        "    logging_dir='./multi-class-logs',\n",
        "    logging_steps=50,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=50,\n",
        "    save_strategy=\"steps\",\n",
        "    fp16=True,\n",
        "    load_best_model_at_end=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_jlI7H_39Jf",
        "outputId": "b6f2a5e8-e139-40e5-eb6e-a126f877e1c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install accelerate -U"
      ],
      "metadata": {
        "id": "oxMn800J4IJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    # the pre-trained model that will be fine-tuned\n",
        "    model=model,\n",
        "     # training arguments that we defined above\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataloader,\n",
        "    eval_dataset=val_dataloader,\n",
        "    compute_metrics= compute_metrics\n",
        ")"
      ],
      "metadata": {
        "id": "yDEKXOaB4mw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "sj1EuFAN5RcO",
        "outputId": "7e4c42f2-9ed9-4834-ddba-369e0c5b327a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1287' max='1287' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1287/1287 03:54, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.398600</td>\n",
              "      <td>2.269145</td>\n",
              "      <td>0.134884</td>\n",
              "      <td>0.029324</td>\n",
              "      <td>0.102433</td>\n",
              "      <td>0.095571</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>2.359600</td>\n",
              "      <td>2.299405</td>\n",
              "      <td>0.148837</td>\n",
              "      <td>0.041491</td>\n",
              "      <td>0.027825</td>\n",
              "      <td>0.093240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>2.349700</td>\n",
              "      <td>2.280324</td>\n",
              "      <td>0.181395</td>\n",
              "      <td>0.027917</td>\n",
              "      <td>0.016490</td>\n",
              "      <td>0.090909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>2.325300</td>\n",
              "      <td>2.266997</td>\n",
              "      <td>0.181395</td>\n",
              "      <td>0.027917</td>\n",
              "      <td>0.016490</td>\n",
              "      <td>0.090909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>2.408600</td>\n",
              "      <td>2.270117</td>\n",
              "      <td>0.181395</td>\n",
              "      <td>0.027917</td>\n",
              "      <td>0.016490</td>\n",
              "      <td>0.090909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>2.377000</td>\n",
              "      <td>2.320530</td>\n",
              "      <td>0.102326</td>\n",
              "      <td>0.021146</td>\n",
              "      <td>0.024286</td>\n",
              "      <td>0.093583</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>2.299700</td>\n",
              "      <td>2.303979</td>\n",
              "      <td>0.181395</td>\n",
              "      <td>0.027917</td>\n",
              "      <td>0.016490</td>\n",
              "      <td>0.090909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>2.397800</td>\n",
              "      <td>2.314976</td>\n",
              "      <td>0.125581</td>\n",
              "      <td>0.020285</td>\n",
              "      <td>0.011416</td>\n",
              "      <td>0.090909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>2.332700</td>\n",
              "      <td>2.278511</td>\n",
              "      <td>0.181395</td>\n",
              "      <td>0.027917</td>\n",
              "      <td>0.016490</td>\n",
              "      <td>0.090909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>2.185300</td>\n",
              "      <td>2.284788</td>\n",
              "      <td>0.181395</td>\n",
              "      <td>0.027917</td>\n",
              "      <td>0.016490</td>\n",
              "      <td>0.090909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>2.452900</td>\n",
              "      <td>2.275541</td>\n",
              "      <td>0.181395</td>\n",
              "      <td>0.027917</td>\n",
              "      <td>0.016490</td>\n",
              "      <td>0.090909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>2.321300</td>\n",
              "      <td>2.287446</td>\n",
              "      <td>0.181395</td>\n",
              "      <td>0.027917</td>\n",
              "      <td>0.016490</td>\n",
              "      <td>0.090909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>2.335900</td>\n",
              "      <td>2.296825</td>\n",
              "      <td>0.125581</td>\n",
              "      <td>0.020285</td>\n",
              "      <td>0.011416</td>\n",
              "      <td>0.090909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>2.301200</td>\n",
              "      <td>2.269104</td>\n",
              "      <td>0.181395</td>\n",
              "      <td>0.027917</td>\n",
              "      <td>0.016490</td>\n",
              "      <td>0.090909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>2.383900</td>\n",
              "      <td>2.271112</td>\n",
              "      <td>0.181395</td>\n",
              "      <td>0.027917</td>\n",
              "      <td>0.016490</td>\n",
              "      <td>0.090909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>2.235300</td>\n",
              "      <td>2.269599</td>\n",
              "      <td>0.181395</td>\n",
              "      <td>0.027917</td>\n",
              "      <td>0.016490</td>\n",
              "      <td>0.090909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>2.363700</td>\n",
              "      <td>2.264580</td>\n",
              "      <td>0.181395</td>\n",
              "      <td>0.027917</td>\n",
              "      <td>0.016490</td>\n",
              "      <td>0.090909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>2.301000</td>\n",
              "      <td>2.259053</td>\n",
              "      <td>0.181395</td>\n",
              "      <td>0.027917</td>\n",
              "      <td>0.016490</td>\n",
              "      <td>0.090909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>950</td>\n",
              "      <td>2.278500</td>\n",
              "      <td>2.270649</td>\n",
              "      <td>0.181395</td>\n",
              "      <td>0.027917</td>\n",
              "      <td>0.016490</td>\n",
              "      <td>0.090909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>2.397600</td>\n",
              "      <td>2.263976</td>\n",
              "      <td>0.181395</td>\n",
              "      <td>0.027917</td>\n",
              "      <td>0.016490</td>\n",
              "      <td>0.090909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1050</td>\n",
              "      <td>2.158900</td>\n",
              "      <td>2.264621</td>\n",
              "      <td>0.181395</td>\n",
              "      <td>0.027917</td>\n",
              "      <td>0.016490</td>\n",
              "      <td>0.090909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>2.231200</td>\n",
              "      <td>2.262173</td>\n",
              "      <td>0.181395</td>\n",
              "      <td>0.027917</td>\n",
              "      <td>0.016490</td>\n",
              "      <td>0.090909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1150</td>\n",
              "      <td>2.301500</td>\n",
              "      <td>2.260093</td>\n",
              "      <td>0.181395</td>\n",
              "      <td>0.027917</td>\n",
              "      <td>0.016490</td>\n",
              "      <td>0.090909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>2.284400</td>\n",
              "      <td>2.259979</td>\n",
              "      <td>0.181395</td>\n",
              "      <td>0.027917</td>\n",
              "      <td>0.016490</td>\n",
              "      <td>0.090909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1250</td>\n",
              "      <td>2.433900</td>\n",
              "      <td>2.258185</td>\n",
              "      <td>0.181395</td>\n",
              "      <td>0.027917</td>\n",
              "      <td>0.016490</td>\n",
              "      <td>0.090909</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1287, training_loss=2.3352136145104896, metrics={'train_runtime': 234.1155, 'train_samples_per_second': 5.497, 'train_steps_per_second': 5.497, 'total_flos': 338651291556864.0, 'train_loss': 2.3352136145104896, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertForSequenceClassification, DistilBertTokenizerFast"
      ],
      "metadata": {
        "id": "sC6TbKWj54cf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(text):\n",
        "    \"\"\"\n",
        "    Predicts the class label for a given input text\n",
        "\n",
        "    Args:\n",
        "        text (str): The input text for which the class label needs to be predicted.\n",
        "\n",
        "    Returns:\n",
        "        probs (torch.Tensor): Class probabilities for the input text.\n",
        "        pred_label_idx (torch.Tensor): The index of the predicted class label.\n",
        "        pred_label (str): The predicted class label.\n",
        "    \"\"\"\n",
        "    # Tokenize the input text and move tensors to the GPU if available\n",
        "    inputs = tokenizer(text, padding=True, truncation=True, max_length=512, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    # Get model output (logits)\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "    probs = outputs[0].softmax(1)\n",
        "    \"\"\" Explanation outputs: The BERT model returns a tuple containing the output logits (and possibly other elements depending on the model configuration). In this case, the output logits are the first element in the tuple, which is why we access it using outputs[0].\n",
        "\n",
        "    outputs[0]: This is a tensor containing the raw output logits for each class. The shape of the tensor is (batch_size, num_classes) where batch_size is the number of input samples (in this case, 1, as we are predicting for a single input text) and num_classes is the number of target classes.\n",
        "\n",
        "    softmax(1): The softmax function is applied along dimension 1 (the class dimension) to convert the raw logits into class probabilities. Softmax normalizes the logits so that they sum to 1, making them interpretable as probabilities. \"\"\"\n",
        "\n",
        "    # Get the index of the class with the highest probability\n",
        "    # argmax() finds the index of the maximum value in the tensor along a specified dimension.\n",
        "    # By default, if no dimension is specified, it returns the index of the maximum value in the flattened tensor.\n",
        "    pred_label_idx = probs.argmax()\n",
        "\n",
        "    # Now map the predicted class index to the actual class label\n",
        "    # Since pred_label_idx is a tensor containing a single value (the predicted class index),\n",
        "    # the .item() method is used to extract the value as a scalar\n",
        "    pred_label = model.config.id2label[pred_label_idx.item()]\n",
        "\n",
        "    return probs, pred_label_idx, pred_label"
      ],
      "metadata": {
        "id": "c9wvn24n56W-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test with a an example text in Turkish\n",
        "text = \"Transpose of Orthogonal matrices are inverses.\"\n",
        "# \"Machine Learning itself is moving towards more and more automated\"\n",
        "predict(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3YJkq4Er6oUW",
        "outputId": "06532118-a8ff-4fd7-b30d-6b50174a5558"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[0.1854, 0.0204, 0.1022, 0.1484, 0.0414, 0.0651, 0.0661, 0.1282, 0.1067,\n",
              "          0.1153, 0.0207]], device='cuda:0', grad_fn=<SoftmaxBackward0>),\n",
              " tensor(0, device='cuda:0'),\n",
              " 'Linear Algebra')"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"Maths-Topics-text-classification-model\"\n",
        "trainer.save_model(model_path)\n",
        "tokenizer.save_pretrained(model_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4QY24-R671f",
        "outputId": "468c18dd-fc79-4649-d438-9d0e32c916bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Maths-Topics-text-classification-model/tokenizer_config.json',\n",
              " 'Maths-Topics-text-classification-model/special_tokens_map.json',\n",
              " 'Maths-Topics-text-classification-model/vocab.txt',\n",
              " 'Maths-Topics-text-classification-model/added_tokens.json',\n",
              " 'Maths-Topics-text-classification-model/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"Maths-Topics-text-classification-model\"\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(model_path)\n",
        "tokenizer= BertTokenizerFast.from_pretrained(model_path)\n",
        "nlp= pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "o5YYADBO7KBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp(\"Transpose of Orthogonal matrices are inverses\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yS56f8Jz7XAH",
        "outputId": "fbd56e80-fb07-463b-d786-c6c8f6c82adf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'label': 'Linear Algebra', 'score': 0.19579783082008362}]"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6DM4Sm7e7pT7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}